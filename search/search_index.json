{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Workflow Orchestrator","text":"<p>Production ready Workflow Orchestration Framework to manage product lifecycle and workflows. Easy to use, Built on top of FastAPI</p> <p> </p> <p> The Workflow Orchestrator is a project developed by SURF to facilitate the orchestration of services. Together with ESnet this project has been open-sourced in the commons conservancy to help facilitate collaboration. We invite all who are interested to take a look and to contribute!</p>"},{"location":"#orchestration","title":"Orchestration","text":"<p>When do you orchestrate and when do you automate? The answer is you probably need both. Automation helps you execute repetitive tasks reliably and easily. Orchestration adds a layer and allows you to add more intelligence to the tasks you need to automate and to have a complete audit log of changes.</p>"},{"location":"#orchestrate-transitive-verb","title":"Orchestrate* - Transitive Verb","text":"<p>/\u02c8\u00f4rk\u0259\u02ccstr\u0101t/ /\u02c8\u0254rk\u0259\u02ccstre\u026at/</p> <p>1: Arrange or score (music) for orchestral performance.   \u2018the song cycle was stunningly arranged and orchestrated\u2019</p> <p>2:  Arrange or direct the elements of (a situation) to produce a desired effect, especially surreptitiously.   \u2018the developers were able to orchestrate a favorable media campaign\u2019</p>"},{"location":"#project-goal","title":"Project Goal","text":"<p>This Workflow Orchestrator provides a framework through which you can manage service orchestration for your end-users. The framework helps and guides you, the person who needs to get things done, through the steps from automation to orchestration. With an easy to use set of API's and examples, you should be up and running and seeing results, before you completely understand all ins and outs of the project. The Workflow Orchestrator enables you to define products to which users can subscribe, and helps you intelligently manage the lifecycle, with the use of Creation, Modification, Termination and Validation workflows, of resources that you provide to your users. The Application extends a FastAPI application and therefore can make use of all the awesome features of FastAPI, pydantic and asyncio python.</p>"},{"location":"#what-does-a-workflow-look-like-it-must-be-pretty-complex","title":"What does a workflow look like? It must be pretty complex!!","text":"<p>Programming a new workflow should be really easy, and at its core it is. By defining workflows as Python functions, all you need to do is understand how to write basic python code, the framework will help take care of the rest.</p> <pre><code>@workflow(\"Name of the workflow\", initial_input_form=input_form_generator)\ndef workflow():\n    return (\n        init\n        &gt;&gt; arbitrary_step_func_1\n        &gt;&gt; arbitrary_step_func_2\n        &gt;&gt; arbitrary_step_func_3\n        &gt;&gt; done\n    )\n</code></pre>"},{"location":"#im-convinced-show-me-more","title":"I'm convinced! Show me more!","text":"<p>There are a number of options for getting started:</p> <ul> <li>First have a look at the demo orchestrator, where you can get a feel for creating subscriptions using workflows. It will take you to our demo environment, where you can see some of our examples in action.</li> <li>For those of you who would like to see it working, take a look at this repo and follow the README to setup a   docker-compose so you can experiment on your localhost.</li> <li>For those who are more adventurous, follow the guide on the next page to   start coding right away.</li> </ul>"},{"location":"architecture/framework/","title":"Orchestrator Framework","text":"<p>The Workflow Orchestrator program contains multiple components for both the frontend and backend, as shown below:</p> <p></p>"},{"location":"architecture/framework/#backend","title":"Backend","text":""},{"location":"architecture/framework/#orchestrator-core","title":"Orchestrator Core","text":"<p>The <code>orchestrator-core</code> component is an opensource backend component, which defines the ruleset for product modeling and workflows. The <code>orchestrator-core</code> is a mandatory component to have a functional workflow orchestrator application. This component is written in Python and makes use fo the Fastapi framework. The <code>orchestrator-core</code> repo can be found here. The <code>orchestrator-core</code> cannot be run standalone, as it contains no definition of any products and workflows.</p>"},{"location":"architecture/framework/#workflow-orchestrator","title":"Workflow Orchestrator","text":"<p>The <code>Workflow Orchestrator</code> is the custom implementation of the orchestrator backend. It is the application which defines all your products, workflows and tasks to create/modify/terminate/validate your product instances, the so-called <code>subscriptions</code>. All product modelling, tasks/wokflows and subscription details are stored in the <code>orchestrator-coredb</code> which is part of the orchestrator-core package. This custom implementation of the workflow orchestrator uses the <code>orchestrator-core</code> as sub-module.</p> <p>With the two backend components set up correctly, you'll have a running Workflow Orchestrator instance accessible via the API. Additionally, with minimal effort, you can have a fully functional frontend application running on top of it.</p>"},{"location":"architecture/framework/#example-orchestrator","title":"Example Orchestrator","text":"<p>An example of the <code>Workflow Orchestrator</code> using the <code>orchestrator-core</code> with some example products and workflow are available here.</p>"},{"location":"architecture/framework/#frontend","title":"Frontend","text":""},{"location":"architecture/framework/#workflow-orchestrator-ui-library","title":"Workflow Orchestrator UI library","text":"<p>The Workflow Orchestrator UI can also be split into 2 major components. A frontend library called the <code>components orchestrator-ui</code> library as available on npm. And the out-of-the-box <code>Workflow Orchestrator UI</code>, which uses the npm library in it's pages. The example of this frontend is the <code>example-orchestrator-ui</code> and can be found here and running instance can be found here. In most cases the example orchestrator is the best deployment model to start with, as is contains a fully functional userinterface, while you can focus your effort on developing products, workflows and tasks.</p>"},{"location":"architecture/framework/#more-advanced-ui-deployment-models","title":"More advanced UI deployment models","text":"<p>By tweaking the <code>example-orchestrator-ui</code> it is possible to easily add extra pages, cards on dashboard page, or change the rending of certain resource type. Examples of the possible changes is shown here . This will leverage the default architecture, like shown below:</p> <p></p> <p>Another approach could be to use individual components from the npm library and build your own application or integrate the components in an existing application.</p>"},{"location":"architecture/tldr/","title":"Architecture; TLDR","text":"<p>The architecture of how the orchestrator-core is setup can be split in two sections. The orchestration philosophy of how workflows are setup and run, and how the application can be used to define products that can be subscribed to by customers.</p>"},{"location":"architecture/tldr/#application-architecture","title":"Application architecture","text":"<p>The Application extends a FastAPI application and therefore can make use of all the awesome features of FastAPI, pydantic and asyncio python.</p>"},{"location":"architecture/tldr/#step-engine","title":"Step Engine","text":"<p>At its core the Orchestrator workflow engine will execute a list of functions in order and store the result of each function to the database. The Orchestrator is able to execute any list of functions that the user envisions so long as they return a dictionary and/or consume variables stored in keys under that dictionary.</p> <pre><code>@workflow(\"Name of the workflow\", initial_input_form=input_form_generator)\ndef workflow():\n    return (\n        init\n        &gt;&gt; arbitrary_step_func_1\n        &gt;&gt; arbitrary_step_func_2\n        &gt;&gt; arbitrary_step_func_3\n        &gt;&gt; done\n    )\n</code></pre> <p>The <code>@workflow</code> decorator converts what the function returns into a <code>StepList</code> which the engine executes sequentially. If and when the step functions raise an exception, the workflow will fail at that step and allow the user to retry.</p>"},{"location":"architecture/tldr/#products-and-subscriptions","title":"Products and Subscriptions","text":"<p>The second part of the orchestrator is a product database that allows a developer to define a collection of logically grouped resources, that when filled in create a Subscription, given to a customer. The description of a product is done in the <code>Product</code>, <code>FixedInput</code>, <code>ProductBlock</code> and <code>ResourceType</code> tables. When a workflow creates a subscription for a customer it creates instances of a <code>Product</code>, <code>ProductBlock</code> and <code>ResourceType</code> and stores them as <code>Subscriptions</code>, <code>SubscriptionInstances</code> and <code>`SubscriptionInstanceValues.</code></p> <p>It is therefore possible to have N number of Subscriptions to a single product. A workflow is typically executed to manipulate a Subscription and transition it from one lifecycle state to another (<code>Initial</code>, <code>Provisioning</code>, <code>Active</code>, <code>Terminated</code>).</p>"},{"location":"architecture/application/domainmodels/","title":"Domain Models - Why do we need them?","text":"<p>Domain Models are designed to help the developer manage complex subscription models and interact with the objects in a user-friendly way. Domain models leverage the Pydantic with some extra sauce to dynamically cast variables from the database where they are stored as a string to their correct type in Python at runtime.</p> <p>Domain Model benefits</p> <ul> <li>Strict MyPy typing and validation in models.</li> <li>Type Safe serialisation to and from the database</li> <li>Subscription lifecycle transition enforcement</li> <li>Hierarchy enforcement with domain models</li> <li>Customer Facing resources vs resource facing resources</li> </ul> <p>When implementing domain models it is possible to link all resources together as they are nodes in a graph through the relations defined in the domain models.</p>"},{"location":"architecture/application/domainmodels/#type-safety-during-serialisation","title":"Type Safety during serialisation","text":"<p>Logic errors that depend on type evaluations/comparisons are prevented by using domain models to serialise database objects. This has a number of benefits as it saves the user the effort of casting the database result to the correct type and allows the developer to be more <code>Type safe</code> whilst developing.</p>"},{"location":"architecture/application/domainmodels/#example","title":"Example","text":"<p>Example</p> <p>The main reason for developing domain models was to make sure bugs like this occurred less.</p>"},{"location":"architecture/application/domainmodels/#pre-domain-models","title":"Pre domain models","text":"<pre><code>&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; instance_value_from_db\n\"False\"\n&gt;&gt;&gt; if instance_value_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"True\"\n</code></pre>"},{"location":"architecture/application/domainmodels/#post-domain-models","title":"Post domain models","text":"<pre><code>&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; type(instance_value_from_db)\n&lt;class str&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model = SubscriptionModel.from_subscription(\"ID\")\n&gt;&gt;&gt; type(subscription_model.product_block.instance_from_db)\n&lt;class bool&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model.product_block.instance_from_db\nFalse\n&gt;&gt;&gt;\n&gt;&gt;&gt; if subscription_model.product_block.instance_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"False\"\n</code></pre>"},{"location":"architecture/application/domainmodels/#lifecycle-transitions","title":"Lifecycle transitions","text":"<p>When transitioning from <code>Initial</code> -&gt; <code>Provisioning</code> -&gt; <code>Active</code> -&gt; <code>Terminated</code> in the Subscription Lifecycle the domain model definitions make sure that all resource types and product blocks are assigned correctly. Typically the <code>Initial</code> status is less strict compared to the <code>Active</code> lifecycle. When assigning product blocks from other subscriptions as dependent on a product block from the subscription that is being modified, the domain-models respect Subscription boundaries and do not update variables and resources in the related Subscription product block.</p>"},{"location":"architecture/application/domainmodels/#enforcing-hierarchy","title":"Enforcing Hierarchy","text":"<p>When defining and modelling products it's often necessary to model resources that are in use by or dependent on other product blocks. A product block of a subscription can also be dependent on a product block from another subscription. This way a hierarchy of product blocks from all subscriptions can be build where the ownership of any product block is determined by the subscription it belongs to.</p>"},{"location":"architecture/application/domainmodels/#a-couple-of-examples-of-subscription-hierarchies","title":"A couple of Examples of subscription hierarchies","text":"<p>We will describe some practical examples to explain how you can deal with complex customers requirements, and how to layer subscriptions to represent a complex portfolio of network services.</p> <ol> <li> <p>Consider the relation between a Node and a Port: When you create Node and Port subscriptions. You should not be allowed to Terminate the Node subscriptions when the Port subscriptions are still being used by customers.</p> </li> <li> <p>Consider a scenario for networking with a layer 2 circuit, one needs at least two interfaces and VLAN configuration to create the circuit. The interfaces may be owned by different customers than the owner of the circuit. Typically we assign a subscription to a customer which contains the interface resource. That interface resource is then used again in the circuit subscription, as a resource.</p> </li> </ol>"},{"location":"architecture/application/domainmodels/#code-examples","title":"Code examples","text":""},{"location":"architecture/application/domainmodels/#product-block-model","title":"Product Block Model","text":"<p>Product block models are reusable Pydantic classes that enable the user to reuse product blocks in multiple Products. They are defined in lifecycle state and can be setup to be very restrictive or less restrictive. The orchestrator supports hierarchy in the way product block models reference each other. In other words, a product block model, may have a property that references one or more other product block models.</p> <p>Info</p> <p>The Product block model should be modeled as though it is a resource that can be re-used in multiple products. In networking the analogy would be: A physical interface may be used in a Layer 2 service and Layer 3 service It is not necessary to define two different physical interface types.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-inactive","title":"Product Block Model - Inactive","text":"<p><pre><code>class ServicePortBlockInactive(ProductBlockModel, product_block_name=\"Service Port\"):\n    \"\"\"Object model for a SN8 Service Port product block.\"\"\"\n\n    nso_service_id: Optional[UUID] = None\n    port_mode: Optional[PortMode] = None\n    lldp: Optional[bool] = None\n    ims_circuit_id: Optional[int] = None\n    auto_negotiation: Optional[bool] = None\n    node: Optional[NodeProductBlock] = None\n</code></pre> As you can see in this model we define it as an Inactive Class. As parameter we pass the name of the product_block in the database. In the second highlighted line you see a variable. This references a <code>resource_type</code> in the database, and annotates what type it should be at runtime. In the <code>Inactive</code> or <code>Initial</code> phase of the Subscription lifecycle we are least restrictive in annotating the properties; All fields/resource types are Optional.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-provisioning","title":"Product Block Model - Provisioning","text":"<p><pre><code>class ServicePortBlockProvisioning(\n    ServicePortBlockInactive , lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    \"\"\"Object model for a SN8 Service Port product block in active state.\"\"\"\n\n    nso_service_id: UUID\n    port_mode: PortMode\n    lldp: bool\n    ims_circuit_id: Optional[int] = None\n    auto_negotiation: Optional[bool] = None\n    node: NodeProductBlock\n</code></pre> In this stage whe have changed the way a Subscription domain model should look like in a certain Lifecycle state. You also see that the <code>resource_type</code> now no-longer is Optional. It must exist in this instantiation of the class. The model will raise a <code>ValidationError</code> upon <code>.save()</code> if typing is not filled in correctly.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-active","title":"Product Block Model - Active","text":"<pre><code>class ServicePortBlock(ServicePortBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    \"\"\"Object model for a SN8 Service Port product block in active state.\"\"\"\n\n    nso_service_id: UUID\n    port_mode: PortMode\n    lldp: bool\n    ims_circuit_id: int\n    auto_negotiation: Optional[bool] = None\n    node: NodeProductBlock\n</code></pre> <p>The Class is now defined in its most strict form, in other words in the Active lifecycle of a subscription, this product block model must have all resource_types filled in except for <code>auto_negotiation</code> to function correctly.</p> <p>Tip</p> <p>The stricter you are in defining your product block models the more you are able to leverage the built in validation of<code>Pydantic</code>.</p>"},{"location":"architecture/application/domainmodels/#product-model-aka-subscriptionmodel","title":"Product Model a.k.a SubscriptionModel","text":"<p>Product models are very similar to Product Block Models in that they adhere to the same principles as explained above. However the difference to Product Block models is that they create <code>Subscriptions</code> in the database. They must always have a reference to a customer and instead of containing other <code>ProductBlockModel</code> or <code>resource_types</code> they contain either <code>fixed_inputs</code> which basically describe fixed product attributes or other <code>ProductBlockModels.</code></p>"},{"location":"architecture/application/domainmodels/#product-model-inactive","title":"Product Model - Inactive","text":"<pre><code>class ServicePortInitial(\n    SubscriptionModel, is_base=True, lifecycle=[SubscriptionLifecycle.INITIAL, SubscriptionLifecycle.TERMINATED]\n):\n    domain: Domain\n    port_speed: PortSpeed\n\n    port: Optional[ServicePortBlockInactive] = None\n</code></pre> <p>In the above example you can observe the lifecycle definition as per the <code>ProductBlockModels</code>. Below that you see <code>fixed_inputs</code> These can be of any type, however if they are a <code>SubClass</code> of a <code>ProductBlockModel</code> the code will automatically create a database instance of that object.</p>"},{"location":"architecture/application/domainmodels/#product-model-provisioning-and-active","title":"Product Model -  Provisioning and Active","text":"<pre><code>class ServicePortProvisioning(\n    ServicePortInitial, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    domain: Domain\n    port_speed: PortSpeed\n    port: ServicePortBlockProvisioning\n\n\nclass ServicePort(ServicePortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    domain: Domain\n    port_speed: PortSpeed\n    port: ServicePortBlock\n</code></pre> <p>Again you can observe how the Product definition changes depending on the lifecycle. It annotates a different type to the <code>port</code> property in <code>SubscriptionLifecycle.ACTIVE</code> compared to <code>SubscriptionLifecycle.PROVISIONING</code>.</p>"},{"location":"architecture/application/domainmodels/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"architecture/application/domainmodels/#crossing-the-subscription-boundary","title":"Crossing the subscription boundary","text":"<p>As mentioned before an advanced use case would be to use <code>ProductBlockModels</code> from other Subscriptions.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; first_service_port = ServicePort.from_subscription(subscription_id=\"ID\")\n&gt;&gt;&gt; first_service_port.customer_id\n\"Y\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port = ServicePort.from_product(product_id=\"ID\", customer_id=\"ID\")\n&gt;&gt;&gt; second_service_port.port = first_service_port.port\n&gt;&gt;&gt; second_service_port.save()\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port.port.subscription == first_service_port.subscription\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port.port.subscription == second_service_port.subscription\nFalse\n</code></pre> <p>This is valid use of the domain models. The code will detect that <code>port</code> is part of <code>first_service_port</code> and respect ownership. It basically will treat it as a <code>read-only</code> property.</p>"},{"location":"architecture/application/domainmodels/#union-types","title":"Union types","text":"<p>There may also be a case where a user would like to define two different types to a <code>ProductBlockModel</code> property. This can be achieved by using the <code>Union</code> type decorator.</p> <p>Danger</p> <p>When using this method be sure as to declare the Most specific type first. This is how Pydantic attempts to cast types to the property. For more background as to why, read here</p> <pre><code>class ServicePort(ServicePortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    domain: Domain\n    port_speed: PortSpeed\n    port: Union[ServicePortBlock, DifferentServicePortBlock]\n</code></pre>"},{"location":"architecture/application/forms-frontend/","title":"Forms - from a frontend perspective","text":"<p>Orchestrator Core contains a module called Pydantic Forms. Pydantic Forms allows for configuration of input forms to collect user input needed for the execution of a workflow. The module contains a frontend part that displays the forms automatically and handles submission and showing validation errors. This documentation describes what happens on the frontend side of this process.</p>"},{"location":"architecture/application/forms-frontend/#initiating-a-workflow-from-frontend","title":"Initiating a workflow from frontend","text":"<p>A workflow can be initiated by doing a POST call to ''/processes/'' <p>The steps that happen to initiate a workflow on the frontend are:</p> <ul> <li>A <code>POST</code> request to <code>/processes/&lt;workflow_name&gt;</code> with an empty payload</li> <li>The backend determines what input values are missing and sends a response with http status code <code>510</code> and a payload containing a JSON6Schema definition describing the form to display. See Example JSON6Schema response</li> <li>The frontend uses the Uniforms library to parse the JSON response into a form to display</li> <li>The AutofieldLoader function is called for each of the form.properties in the JSON response. This function uses the properties <code>type</code> and <code>format</code> to determine what kind of field will be displayed.</li> </ul> <pre><code>In the example json response below one of the properties is\n...\n            \"customer_id\": {\n                \"default\": \"c9b5e717-0b11-e511-80d0-005056956c1a\",\n                \"format\": \"customerId\",\n                \"title\": \"Customer Id\",\n                \"type\": \"string\",\n                \"uniforms\": {\n                    \"disabled\": true,\n                    \"value\": \"c9b5e717-0b11-e511-80d0-005056956c1a\"\n                }\n            }\n...\n\nIn the autoFieldFunction this maps to a CustomerField.\n\nexport function autoFieldFunction(\n    props,\n    uniforms,\n) {\n    const { allowedValues, checkboxes, fieldType, field } = props;\n    const { format } = field;\n\n    switch (fieldType) {\n    ...\n        case String:\n            switch (format) {\n                ....\n                case 'customerId':\n                    return CustomerField;\n                ...\n            }\n    ...\n\nThe CustomerField is a React component that is provided by the Orchestrator Component Library.\nIt's passed the complete property object so it can use them to adjust it's behavior.\n</code></pre> <ul> <li>A <code>POST</code> with the form values is made to the same <code>/processes/&lt;workflow_name&gt;</code> endpoint</li> <li>The response status code can be:<ul> <li><code>400: Form invalid</code> Invalid values have been detected because the validators the backend runs have failed. An error message is shown.</li> <li><code>510: FormNotComplete</code> There is another step. This response contains another json response containing a form.</li> <li><code>201: Created</code> The workflow was initiated successfully. The response contains a workflow id and the user is redirected to the workflow detail page</li> </ul> </li> </ul> <p>Note. For forms that have multiple steps the user input for each step is accumulated in local frontend state and posted to <code>/processes/&lt;workflowname&gt;</code> on each step. The endpoint will receive all available user inputs on each step and determine what other user input it still needs or if it's ready to start the workflow.</p> <p>Note 2 The Orchestrator Component library contains fields that are marked as deprecated and live in a folder named <code>deprecated</code>. These contain field types that are very specific to workflows that are in use by SURF. There are plans to remove these from the general purpose components library.</p> <p>Note 3 There are plans to make it easier to extend this functionality to add custom field types and extend the switch statement in the autoFieldFunction to include these custom <code>types</code> or <code>formats</code></p>"},{"location":"architecture/application/forms-frontend/#backend-creating-a-workflow-that-generates-a-form-that-asks-for-user-input","title":"Backend: Creating a workflow that generates a form that asks for user input","text":"<p>Creating workflows is described in other parts of this documentation in more detail. The practical steps and those that are relevant to the frontend are these</p> <ul> <li>A mapping between a function and a <code>processes/&lt;workflow-name&gt;</code> endpoint is added to the <code>workflows/init.py</code> file</li> </ul> <pre><code>    `LazyWorkflowInstance(\"surf.workflows.core_link.create_core_link\", \"create_core_link\")`\n</code></pre> <p>This makes <code>POST</code> requests to <code>processes/create_core_link</code> call surf.core_lint.create_core_link with the <code>POST</code> payload</p> <ul> <li>The create_core_link function is decorated with the create_workflow decorator. It provides workflow orchestrator functionality.</li> </ul> <pre><code>@create_workflow(\"Create Core Link\", initial_input_form=initial_input_form_generator)\ndef create_core_link() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; step 1\n        ...\n        &gt;&gt; step last\n    )\n</code></pre> <ul> <li>If the POST request contains no user_input the value provided to initial_input_form is called, in this case the function initial_input_form_generator</li> </ul> <pre><code>def initial_input_form_generator(product_name: str) -&gt; FormGenerator:\n    user_input = yield step_1_form(product_name)\n\n    ....\n\n    user_input_ports = yield step_2_form(product_name, ..)\n\n    return (\n        ... result from step ...\n    )\n</code></pre> <ul> <li> <p>The functionality provided by the workflow orchestrator decorator makes every yield statement pass if the required user input data is passed in or return a response to the client with a <code>510</code> status code and a payload containing the definition for the form to display in JSON Schema 6 format</p> </li> <li> <p>An example of what the step_1_form function could look like.</p> </li> </ul> <pre><code>def step_1_form(product_name: str) -&gt; type[FormPage]:\n    class SpeedChoice(Choice):\n        _10000 = (\"10000\", \"10 Gbps\")\n        _40000 = (\"40000\", \"40 Gbps\")\n        _100000 = (\"100000\", \"100 Gbps\")\n        _400000 = (\"400000\", \"400 Gbps\")\n\n    class CreateCoreLinkSpeedForm(FormPage):\n        model_config = ConfigDict(title=product_name)\n\n        organisation: SurfnetOrganisation\n\n        label_corelink_settings: Label\n        divider_1: Divider\n\n        corelink_service_speed: SpeedChoice = SpeedChoice._400000\n        isis_metric: IsisMetric = 20\n\n    return CreateCoreLinkSpeedForm\n</code></pre> <p>The type specified for each property (eg divider_1: Divider) determines what <code>type</code> property it gets in the resulting JSON 6 Schema. There are a set number of property types that can be provided and that are automatically handled by the frontend by default. This is extendable. [TODO: Insert complete list of possible types]</p> <ul> <li>The response for a POST call without user input to /processes/create_core_link is"},{"location":"architecture/application/forms-frontend/#example-json6schema-response","title":"Example JSON6Schema response:","text":"<pre><code>{\n    \"type\": \"FormNotCompleteError\",\n    \"detail\": [not relevant]\n    \"traceback\": [not relevant]\n    \"form\": {\n        \"$defs\": {\n            \"SpeedChoice\": {\n                \"enum\": [\n                    \"10000\",\n                    \"40000\",\n                    \"100000\",\n                    \"400000\"\n                ],\n                \"options\": {\n                    \"10000\": \"10 Gbps\",\n                    \"100000\": \"100 Gbps\",\n                    \"40000\": \"40 Gbps\",\n                    \"400000\": \"400 Gbps\"\n                },\n                \"title\": \"SpeedChoice\",\n                \"type\": \"string\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"customer_id\": {\n                \"default\": \"c9b5e717-0b11-e511-80d0-005056956c1a\",\n                \"format\": \"customerId\",\n                \"title\": \"Customer Id\",\n                \"type\": \"string\",\n                \"uniforms\": {\n                    \"disabled\": true,\n                    \"value\": \"c9b5e717-0b11-e511-80d0-005056956c1a\"\n                }\n            },\n            \"label_corelink_settings\": {\n                \"anyOf\": [\n                    {\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"type\": \"null\"\n                    }\n                ],\n                \"default\": null,\n                \"format\": \"label\",\n                \"title\": \"Label Corelink Settings\",\n                \"type\": \"string\"\n            },\n            \"divider\": {\n                \"anyOf\": [\n                    {\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"type\": \"null\"\n                    }\n                ],\n                \"default\": null,\n                \"format\": \"divider\",\n                \"title\": \"Divider\",\n                \"type\": \"string\"\n            },\n            \"corelink_service_speed\": {\n                \"allOf\": [\n                    {\n                        \"$ref\": \"#/$defs/SpeedChoice\"\n                    }\n                ],\n                \"default\": \"400000\"\n            },\n            \"isis_metric\": {\n                \"default\": 20,\n                \"maximum\": 16777215,\n                \"minimum\": 1,\n                \"title\": \"Isis Metric\",\n                \"type\": \"integer\"\n            }\n        },\n        \"title\": \"SN8 Corelink\",\n        \"type\": \"object\"\n    },\n    \"title\": [not relevant],\n    \"status\": 510\n}\n</code></pre>"},{"location":"architecture/application/workflow/","title":"What is a workflow and how does it work?","text":"<p>The workflow engine is the core of the software, it has been created to execute a number of functions.</p> <ul> <li>Safely and reliable manipulate customer <code>Subscriptions</code> from one state to the next and maintain auditability.</li> <li>Create an API through which programmatically <code>Subscriptions</code> can be manipulated.</li> <li>Execute step functions in order and allow the retry of previously failed process-steps in an idempotent way.</li> <li>Atomically execute workflow functions.</li> </ul>"},{"location":"architecture/application/workflow/#best-practices","title":"Best Practices","text":"<p>The orchestrator will always attempt to be as robust as possible when executing workflow steps. However, it is always up to the developer to implement the best practices as well as they can.</p>"},{"location":"architecture/application/workflow/#safeguards-in-the-orchestrator","title":"Safeguards in the orchestrator;","text":"<ul> <li>Atomic Step Execution: Each step is treated as an atomic unit.   If a step fails, no partial changes are committed to the database.   Because of this, calling .commit() on the ORM within a step function is not allowed.</li> <li><code>insync</code> Subscription Requirement: By default, workflows can only run on subscriptions that are marked as <code>insync</code>, unless explicitly configured otherwise.   This prevents multiple workflows from manipulating the same subscription concurrently.   One of the first actions a workflow should perform is to mark the subscription as <code>out of sync</code> to avoid conflicts.</li> <li>Step Retry Behavior: Failed steps can be retried indefinitely. Each retry starts from the state of the last successfully completed step.</li> </ul>"},{"location":"architecture/application/workflow/#coding-gotchas","title":"Coding gotchas","text":"<ul> <li>The orchestrator is best suited to be used as a data manipulator, not as a data transporter.</li> <li>Use the State log as a log of work, not a log of data.</li> <li>If the data you enter in the state is corrupt or wrong, you might need a difficult database query to update the state to resolve the conflict.</li> <li>Always retrieve external data at the moment it's needed during a step, not earlier.   This increases the robustness of the step.</li> <li>Each step function should perform a single, clearly defined unit of work.   Theoretically you can execute the whole workflow in a single step, However this does not help with traceability and reliability.</li> </ul>"},{"location":"architecture/application/workflow/#workflows","title":"Workflows","text":"<p>explanation to create a workflow in code</p> <p>Workflows are composed of one or more steps, each representing a discrete unit of work in the subscription management process. Steps are executed sequentially by the workflow engine and are the fundamental building blocks of workflows.</p> <p>There are two high-level kinds of workflows:</p> <ul> <li>workflows<ul> <li>Defined for specific products.</li> <li>Perform operations like creating, modifying, or terminating subscriptions.</li> </ul> </li> <li>tasks<ul> <li>Not tied to a specific product and may not involve a subscription at all.</li> <li>Can be scheduled to run periodically or triggered manually.</li> <li>Useful for actions like cleanup jobs or triggering validations across multiple subscriptions.</li> <li>Examples can be found in <code>orchestrator.workflows.tasks</code>.</li> </ul> </li> </ul> <p>Workflows and tasks need to be registered in the database and initialized as a <code>LazyWorkflowInstance</code> to work, see registering workflows for more info.</p>"},{"location":"architecture/application/workflow/#subscription-workflow-types","title":"Subscription Workflow Types","text":"<p>Workflows are categorized based on the operations they perform on a subscription:</p> <ul> <li>Create (create_workflow)<ul> <li>The \"base\" workflow that initializes a new subscription for the product.</li> <li>Only one create workflow should exist per product.</li> </ul> </li> <li>Modify (modify_workflow)<ul> <li>Modify an existing subscription (e.g., updating parameters, migrating to another product).</li> <li>Multiple modify workflows can exist, each handling a specific type of modification.</li> </ul> </li> <li>Terminate (terminate_workflow)<ul> <li>Terminates the subscription and removes its data and references from external systems.</li> <li>External references should only be retained if they also hold historical records.</li> <li>Only one terminate workflow should exist per product.</li> </ul> </li> <li>Validate (validate_workflow)<ul> <li>Verifies that external systems are consistent with the orchestrator's subscription state.</li> <li>Only one validate workflow should exist per product.</li> </ul> </li> <li>Reconcile (reconcile_workflow)<ul> <li>Ensures that the orchestrator's subscription state is in sync with external systems.</li> <li>Only one reconcile workflow should exist per product.</li> </ul> </li> </ul>"},{"location":"architecture/application/workflow/#default-workflows","title":"Default Workflows","text":"<p>Registering a Default Workflow attaches a given workflow to all Products. To ensure this, modify the <code>DEFAULT_PRODUCT_WORKFLOWS</code> environment variable and add the workflow the database with a migration.</p> <p>By default, <code>DEFAULT_PRODUCT_WORKFLOWS</code> is set to <code>['modify_note']</code>.</p> <p>More about registering workflows can be found here.</p>"},{"location":"architecture/application/workflow/#tasks","title":"Tasks","text":"<p>Tasks are workflows that aren't associated with any subscriptions, and can be run by the Orchestrator's scheduler. Learn about tasks and scheduling here.</p>"},{"location":"architecture/application/workflow/#workflow-steps","title":"Workflow Steps","text":"<p>Workflows are composed of one or more steps, where each step is executed sequentially by the workflow engine and are the fundamental building blocks of workflows.</p>"},{"location":"architecture/application/workflow/#step-characteristics","title":"Step Characteristics","text":"<ul> <li>Atomicity: Each step is atomic, either it fully completes or has no effect. This ensures data consistency and reliable state transitions.</li> <li>Idempotency: Steps should be designed to be safely repeatable without causing unintended side effects.</li> <li>Traceability: By breaking workflows into fine-grained steps, the orchestrator maintains clear audit trails and simplifies error handling and retries.</li> </ul>"},{"location":"architecture/application/workflow/#types-of-steps","title":"Types of Steps","text":"<p>The orchestrator supports several kinds of steps to cover different use cases:</p> <ul> <li> <p><code>step</code> functional docs for step   Executes specific business logic or external API calls as part of the subscription process.</p> </li> <li> <p><code>retrystep</code> functional docs for retrystep   Similar to <code>step</code>, but designed for operations that may fail intermittently. These steps will automatically be retried periodically on failure.</p> </li> <li> <p><code>inputstep</code> functional docs for inputstep   Pauses the workflow to request and receive user input during execution.</p> </li> <li> <p><code>conditional</code> functional docs for conditional   Conditionally executes the step based on environment variables or process state.   If the condition evaluates to false, the step is skipped entirely.</p> </li> <li> <p><code>callback_step</code> functional docs for callback_step   Pauses workflow execution while waiting for a external event to complete.</p> </li> </ul> <p>For a practical example of how to define reusable workflow steps\u2014and how to leverage singledispatch for type-specific logic\u2014see: \ud83d\udc49 Reusable step functions and singledispatch usage</p>"},{"location":"architecture/application/workflow/#execution-parameters","title":"Execution parameters","text":"<p>You can fine-tune workflow execution behavior using a set of configuration parameters. The recommended location to define or override these is in <code>workflows/__init__.py</code>. Below are examples of key configuration options:</p> <ol> <li><code>WF_USABLE_MAP</code>: Define usable subscription lifecycles for workflows.</li> </ol> <p>By default, the associated workflow can only be run on a subscription with a lifecycle state set to <code>ACTIVE</code>. This behavior can be changed in the <code>WF_USABLE_MAP</code> data structure:</p> <p>note: Terminate workflows are by default, allowed to run on subscriptions in any lifecycle state unless explicitly restricted in this map.</p> <pre><code>from orchestrator.services.subscriptions import WF_USABLE_MAP\n\nWF_USABLE_MAP.update(\n    {\n        \"validate_node_enrollment\": [\"active\", \"provisioning\"],\n        \"provision_node_enrollment\": [\"active\", \"provisioning\"],\n        \"modify_node_enrollment\": [\"provisioning\"],\n    }\n)\n</code></pre> <p>Now validate and provision can be run on subscriptions in either <code>ACTIVE</code> or <code>PROVISIONING</code> states and modify can only be run on subscriptions in the <code>PROVISIONING</code> state.</p> <ol> <li><code>WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS</code>: Block modify workflows on subscriptions with unterminated <code>in_use_by</code> subscriptions</li> </ol> <p>By default, only terminate workflows are prohibited from running on subscriptions with unterminated <code>in_use_by</code> subscriptions. This behavior can be changed in the <code>WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS</code> data structure:</p> <pre><code>from orchestrator.services.subscriptions import WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS\n\nWF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS.update(\n    {\n        \"modify_node_enrollment\": True\n    }\n)\n</code></pre> <p>With this configuration, both terminate and modify will not run on subscriptions with unterminated <code>in_use_by</code> subscriptions.</p> <ol> <li><code>WF_USABLE_WHILE_OUT_OF_SYNC</code>: Allow specific workflows on out of sync subscriptions</li> </ol> <p>By default, only system workflows (tasks) are allowed to run on subscriptions that are not in sync. This behavior can be changed with the <code>WF_USABLE_WHILE_OUT_OF_SYNC</code> data structure:</p> <pre><code>from orchestrator.services.subscriptions import WF_USABLE_WHILE_OUT_OF_SYNC\n\nWF_USABLE_WHILE_OUT_OF_SYNC.extend(\n    [\n        \"modify_description\"\n    ]\n)\n</code></pre> <p>Now this particular modify workflow can be run on subscriptions that are not in sync.</p> <p>Danger</p> <p>It is potentially dangerous to run workflows on subscriptions that are not in sync. Only use this for small and specific usecases, such as editing a description that is only used within orchestrator.</p>"},{"location":"architecture/orchestration/orchestrator-ui/","title":"Workflow Orchestrator UI","text":"<p>The default Workflow Orchestrator UI app should offer sufficient functionality to start working with, and experiencing, the workflow orchestrator. For this an example-orchestrator-ui is available to start working with a ready to deploy example workflow orchestrator backend.</p> <p>At the same time the UI is developed with the concept in mind that any user of the workflow orchestrator can customize the UI to meet their own requirements. There are two possible ways to accomplish this:</p> <ul> <li>Overriding components</li> <li>Using components from the npm UI library</li> </ul>"},{"location":"architecture/orchestration/orchestrator-ui/#overriding-components","title":"Overriding components","text":"<p>The first solution is based on using the orchestrator-ui library in its full extend and just add/tweak components. Examples of this approach would be: - render certain resource type differently then the npm normally does - add menu items to the navigation - add summary cards to the dashboard page</p> <p>An example of a custom orchestrator-ui is shown below, which shows a custom summary card and additional menu items compared to the standard orchestrator-ui.</p>"},{"location":"architecture/orchestration/orchestrator-ui/#using-components-from-the-npm-ui-library","title":"Using components from the npm UI library","text":"<p>The second solution will probably require more work, but could be interesting to extend an existing application with orchestrator components.</p> <p>Both customization solutions rely on the npm package of the components libray published in npm. This package contains the pages and components that are meant to be used in an app that serves the frontend to a workflow orchestrator backend.</p> <p>To have a development setup where both the source code of the app and the source code of this package are available have a look at the Orchestrator UI library repository at the location packages/orchestrator-ui.</p>"},{"location":"architecture/orchestration/orchestrator-ui/#example-screenshots-of-orchestrator-ui","title":"Example screenshots of orchestrator-ui","text":""},{"location":"architecture/orchestration/orchestrator-ui/#standard-orchestrator-ui","title":"Standard orchestrator-ui","text":""},{"location":"architecture/orchestration/orchestrator-ui/#custom-orchestrator-ui","title":"Custom orchestrator-ui","text":"<ul> <li>showing additional summary card component (in-maintence corelink)</li> <li>additional menu items </li> </ul>"},{"location":"architecture/orchestration/orchestrator-ui/#env-variables","title":"Env variables","text":"<p>The env variables from .env.example are meant to work with an example workflow orchestrator backend. They can be changed as needed. The variable available are:</p> <pre><code>ENVIRONMENT_NAME=Development\nPROCESS_DETAIL_REFETCH_INTERVAL: Determines the interval in milliseconds the process detail page refreshes at. DEPRECATED. Number\nORCHESTRATOR_API_HOST: The base url of the workflow orchestrator engine. String\nORCHESTRATOR_API_PATH: The path to the api from the base url. String\nORCHESTRATOR_GRAPHQL_HOST: The base url to the graphql server. String\nORCHESTRATOR_GRAPHQL_PATH: The path to the graphql endpoint. String\nORCHESTRATOR_WEBSOCKET_URL: The url to the websocket server that emits cache invalidation events. String\nAUTH_ACTIVE: If authorization is active or not. Boolean\nUSE_WEBSOCKET: Establishes a websocket connection that allows the backend to send cachekey invalidation messages to let the frontend now something has changed on the backend. Boolean\nUSE_THEME_TOGGLE: Show a toggle that allows a user to switch from light to dark theme and back. Boolean\nSHOW_WORKFLOW_INFORMATION_LINK: Show a information icon on the workflow detail pages that allows for linking to an external documentation system. Boolean\nWORKFLOW_INFORMATION_LINK_URL: The url used to build the url that the information icon links to. The format of the link is: &lt;WORKFLOW_INFORMATION_LINK_URL&gt;&lt;WORKFLOW_NAME&gt;. The contents at this url should be maintained in the external documentation system.\n</code></pre>"},{"location":"architecture/orchestration/orchestrator-ui/#starting-a-workflow-from-the-orchestrator-ui-user-input-forms","title":"Starting a workflow from the orchestrator UI - User input forms","text":"<p>Creating a Workflow orchestrator workflow is explained here: Creating a workflow.</p> <p>Once a workflow is created it will - automatically - show up in the dropdown list that opens from '+ New Subscription'. When selected a request will be made to the /processes/ endpoint which will return a form definition describing the form that needs to be displayed to collect user input. The form definition is turned into a form automatically. <p>The example UI contains an example form that shows an example of a form definition and the form it renders at: http://localhost:3000/example-form</p>"},{"location":"architecture/orchestration/orchestrator-ui/#deployment-to-other-environments","title":"Deployment to other environments","text":"<p>For deployments to your own environment it's recommended to copy or fork the code of the Example orchestrator UI repository into your own code repository, add your customizations and configuration and deploy from there.</p>"},{"location":"architecture/orchestration/orchestrator-ui/#extensibility","title":"Extensibility:","text":"<p>The Orchestrator UI allows for a number of customizations:</p>"},{"location":"architecture/orchestration/orchestrator-ui/#adding-extra-pages","title":"Adding extra pages","text":"<p>The Orchestrator UI is based on NextJs and its pages router. Files that are added to the <code>pages</code> folder are automatically rendered at the url /. For example a file a the location <code>pages/cats.tsx</code> will cause the contents of the file to be rendered at the location http://localhost/cats. Please note the pages routes is deprecated by NextJS and might be replaced by the newer app router."},{"location":"architecture/orchestration/orchestrator-ui/#adding-menu-items","title":"Adding menu items","text":"<p>The file 'pages/_app.tsx' contains the rendering of 'WfoPageTemplate' component. This component takes a callback called overrideMenuItems which receives the current menu items and can later them</p> <pre><code>.... in _app.tsx ...\n&lt;WfoPageTemplate\n    overrideMenuItems={\n        (currentMenuItems) =&gt; {\n            return [\n                ...currentMenuItems,\n                {\n                    ... extra menu item\n                }\n            ]\n        }\n    }\n&gt;\n... see _app.tsx .. for an actual example\n</code></pre>"},{"location":"architecture/orchestration/orchestrator-ui/#adding-a-custom-logo","title":"Adding a custom logo","text":"<p>The file 'pages/_app.tsx' contains the rendering of 'WfoPageTemplate' component. This component takes a callback called getAppLogo. The React component that is returned by this function get's displayed as the apps logo.</p> <pre><code>.... in _app.tsx ...\n&lt;WfoPageTemplate\n    appLogo={() =&gt; {\n        return &lt;div&gt;LOGO&lt;/div&gt;\n    }}\n&gt;\n</code></pre>"},{"location":"architecture/orchestration/orchestrator-ui/#authorization-and-rbac","title":"Authorization and RBAC","text":"<p>The Orchestrator UI library provides a standard isAllowed handler that wraps certain components and can be used to wrap pages. The default behavior is to always return true allowing everything inside it to show. By providing a custom implementation you can implement your own rules for allowing or disallowing things.</p> <pre><code>...in _app.tsx...\n    &lt;WfoAuth isAllowedHandler={(routerPath: string, resource?: string) =&gt; {... custom rules ... }}&gt;\n        &lt;EuiProvider\n            colorMode={themeMode}\n            modify={defaultOrchestratorTheme}\n        &gt;\n        ....\n        &lt;/EuiProvider&gt;\n    &lt;/WfoAuth&gt;\n\n...\n</code></pre>"},{"location":"architecture/orchestration/orchestrator-ui/#overriding-fields-sections-and-start-page-content","title":"Overriding fields, sections and start page content","text":"<p>Field values are displayed as plain strings by default and the sections on a page have a set order. For some fields and sections you might want more control, this is made possible by supplying a orchestratorComponentOverride object to the StoreProvider in _app.tsx. Currently this is possible for the subscription detail and the start page only.</p> <p>The orchestratorComponentOverride has these options</p> <pre><code>...in _app.tsx...\n    &lt;StoreProvider\n        ...\n        orchestratorComponentOverride={\n            startPage?: {\n                summaryCardConfigurationOverride?:\n                    defaultItems: ReactElement[],\n                ) =&gt; ReactElement[];\n            };\n            subscriptionDetail?: {\n                valueOverrides?: Record&lt;string, (fieldValue: FieldValue) =&gt; ReactNode&gt;;\n                generalSectionConfigurationOverride?: (\n                    defaultSections: WfoSubscriptionDetailGeneralConfiguration[],\n                    subscriptionDetail: SubscriptionDetail,\n                ) =&gt; WfoSubscriptionDetailGeneralConfiguration[];\n            };\n        }\n        ...\n    &gt;\n</code></pre>"},{"location":"architecture/orchestration/orchestrator-ui/#available-functions","title":"Available functions","text":"<p>startpage: summaryCardConfigurationOverride: A function that gets the default items as input and should return a new list of items</p> <p>subscriptionsDetail: valueOverrides: A function that should supply a field name to function mapping where the function will be called with the field value when the field is rendered.</p> <p>subscriptionDetail: generalSectionConfigurationOverride: A function that receives the default sections and the subscription detail object and returns a new list of sections</p>"},{"location":"architecture/orchestration/orchestrator-ui/#component-library","title":"Component library","text":"<p>The repository that publishes the npm package that is used to supply the layout components is Orchestrator UI Component library Next to pages it exports page components that can be used to build custom pages.</p>"},{"location":"architecture/orchestration/orchestrator-ui/#theming","title":"Theming","text":""},{"location":"architecture/orchestration/orchestrator-ui/#customizing-the-theme","title":"Customizing the theme","text":"<p>The Workflow Orchestrator frontend ships with a default theme leveraging the theming mechanism of Elastic UI. This theme can be partially adjusted or completely overridden.</p> <p>As part of the boilerplate code, the <code>_app.tsx</code> file applies a <code>defaultOrchestratorTheme</code> object to the EuiProvider.</p> <pre><code>..._app.tsx...\n\n&lt;SeveralProviders&gt;\n    &lt;EuiProvider\n        colorMode={themeMode}\n        modify={defaultOrchestratorTheme}\n    &gt;\n        ...\n    &lt;/EuiProvider&gt;\n&lt;/SeveralProviders&gt;\n</code></pre> <p>The default defaultOrchestratorTheme object contains adjustments of the standard theme provided by Elastic UI and can be imported from the @orchestrator-ui/orchestrator-ui-components package.</p> <p>To make small adjustments, simply use defaultOrchestratorTheme as a base and override the desired properties:</p> <pre><code>// _app.tsx\n...\nimport { EuiThemeModifications } from '@elastic/eui';\nimport { defaultOrchestratorTheme } from '@orchestrator-ui/orchestrator-ui-components';\n...\n\nfunction CustomApp(...) {\n    ...\n    const myTheme: EuiThemeModifications = {\n        ...defaultOrchestratorTheme,\n        colors: {\n            DARK: {\n                primary: '#FF69B4',\n            },\n            LIGHT: {\n                primary: '#32CD32',\n            },\n        },\n    };\n\n    ...\n\n    return (\n        &lt;SeveralProviders&gt;\n            &lt;EuiProvider\n                colorMode={themeMode}\n                modify={myTheme}\n            &gt;\n                ...\n            &lt;/EuiProvider&gt;\n        &lt;/SeveralProviders&gt;\n    )\n}\n</code></pre> <p>The usage of defaultOrchestratorTheme is not required, a new <code>EuiThemeModifications</code> can also be made from scratch or using the helper tool on the EUI website.</p>"},{"location":"architecture/orchestration/orchestrator-ui/#color-mode","title":"Color Mode","text":"<p>The <code>color</code> property of the theme object contains a <code>DARK</code> and <code>LIGHT</code> object representing the color mode. The _app.tsx file contains a mechanism to switch and store the color mode. In any given component the <code>useOrchestratorTheme</code> hook can be used to get the current color mode. For more convenience, there is also the <code>isDarkThemeActive</code> boolean:</p> <pre><code>...\nimport { useOrchestratorTheme } from '@orchestrator-ui/orchestrator-ui-components';\n...\n\nconst WfoAnyComponent: FC&lt;WfoAnyComponentProps&gt; = (...) =&gt; {\n    const {\n        colorMode,          // type: EuiThemeColorModeStandard\n        isDarkThemeActive   // type: boolean\n    } = useOrchestratorTheme();\n\n    return(...);\n}\n</code></pre>"},{"location":"architecture/orchestration/philosophy/","title":"Philosophy","text":"<p>The Workflow Orchestrator is a framework of tools that help the developer create workflow to modify the lifecycle of subscribed products. It is un-opinionated in what it can orchestrate, but very opinionated in how. The Orchestrator is designed to run linear workflows that represent the business process of delivering a product. In comparison to other workflow engines like Camunda or Airflow we try to keep the options of the developer limited. In most cases the Workflow Orchestrator framework is flexible enough to handle the intelligence needed in the business process.</p>"},{"location":"architecture/orchestration/philosophy/#lightweight","title":"Lightweight","text":"<p>The core functionality of the framework is relatively simple:</p> <ul> <li>There is a simple step engine that executes python functions.</li> <li>Every step is designed to be atomic to make execution as safe as possible.</li> <li>When using the Workflow Orchestrator with the example-ui, it is possible to create highly dynamic forms in Python. The developer does not need to implement any code in the frontend to get started straight away.</li> <li>Furthermore we are working on an extensive set of tools to help bootstrap the development experience.</li> </ul>"},{"location":"architecture/product_modelling/context/","title":"Context","text":"<p>The models described further on assume an Ethernet network that consists of nodes where each node has physical ports. Network services have endpoints that connect to ports. The attributes that are specific to an endpoint are modelled as a service attach point. Examples of such attributes are the layer two label(s) used on that port or a point-to-point IP address. An inventory management system (IMS) is used to keep track of everything that is being deployed, and a network resource manager (NRM), such as NSO or Ansible, is used to provision the services on the network. All IP addresses and prefixes are stored in an IP address management (IPAM) tool.</p>"},{"location":"architecture/product_modelling/introduction/","title":"Introduction","text":"<p>Growing numbers of National Research and Education Networks (NREN) are interested in automating and orchestrating their network portfolio. However, individual NRENs may be at different levels of engagement, ranging from interested but with no concrete plans as yet, to fully automated and orchestrated. Of the many commercial and open-source tools that can be used, the NREN community\u2019s interest appears to be focused on Ansible and NSO for the automation part and on Workflow Orchestrator (WFO) for the orchestration part. Although the WFO is agnostic to the domain it is used in, this section describes, as an example that will be recognised by NREN, a set of network service products that are common to this community and can be used in combination with the Workflow Orchestrator.</p>"},{"location":"architecture/product_modelling/ip_static/","title":"IP static","text":"<p>The modelling of the IP static service is slightly more difficult. Luckily, we are again able to reuse existing product blocks and add or change attributes to meet our needs. First of all, a fixed input is used to distinguish between different types of IP services, in our case it is used to distinguish between static and BGP routing. The Ip_static_virtual_circuit product block reuses the L2_ptp_virtual_circuit product block and adds the ability to administer additional IP settings such as the use of multicast and whether a CERT filter is enabled or not. The list of service attach points is overridden, this time to reflect the fact that the IP static service only has one endpoint. The layer 3 service attach point extends the one at layer 2 and adds a list of customer prefixes, the IPv4/IPv6 MTU, and the IPv4/IPv6 point-to-point addresses used. For this example, we chose to bundle the IP settings in a separate product block to make it possible to be reused by other products, but we could also just have extended the Ip_static_virtual_circuit product block.</p> <p></p> <ul> <li>ip_routing_type: either Static or BGP, for this product set to Static</li> <li>customer_prefixes: list of IPAM ID\u2019s of the customer IP prefixes</li> <li>customer_ipv4_mtu: the customer IPv4 maximum transmission unit</li> <li>customer_ipv6_mtu: the customer IPv6 maximum transmission unit</li> <li>ptp_ipv4_ipam_id: the IPAM id of the IPv4 point-to-point prefix</li> <li>ptp_ipv6_ipam_id: the IPAM id of the IPv6 point-to-point prefix</li> <li>multicast: enable multicast</li> <li>cert_filter: enable CERT filter</li> </ul>"},{"location":"architecture/product_modelling/l2_point_to_point/","title":"L2 Point-to-Point","text":"<p>The Layer 2 point-to-point service is modelled using two product blocks. The l2_point_to_point product block holds the pointers to IMS and the NRM, the speed of the circuit, and whether the speed policer is enabled or not, as well as pointers to the two service attach points. The latter are modelled with the L2_service_attach_point product block and keep track of the port associated with that endpoint and, in the case where 802.1Q has to be enabled, the VLAN range used. The service can either be deployed protected or unprotected in the service provider network. This is administered with the fixed input protection_type.</p> <p></p> <ul> <li>protection_type: this service is either unprotected or protected</li> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>speed: the speed of the point-to-point service in Mbit/s</li> <li>speed_policer: enable the speed policer for this service</li> <li>sap: a constrained list of exactly two Layer2 service attach points</li> <li>vlan_range: range of Layer 2 labels to be used on this endpoint of the service</li> <li>port: link to the Port product block this service endpoint connects to</li> </ul>"},{"location":"architecture/product_modelling/l2_vpn/","title":"L2 VPN","text":"<p>2.4 L2 VPN</p> <p>The Layer 2 VPN service is much like the Layer 2 point-to-point service, which makes it possible to reuse existing product blocks, with a few differences such as the absence of fixed inputs. The L2_vpn_virtual_circuit product block inherits from the L2_ptp_virtual_circuit product block, and adds attributes to (dis)allow VLAN retagging and control over the BUM filter. And because a VPN can have one or more endpoints, unlike a point-to-point that has exactly two endpoints, the list of service attach points is overridden to reflect this.</p> <p></p> <ul> <li>bum_filter: enable broadcast, unknown unicast, and multicast (BUM) traffic filter</li> <li>vlan_retagging: allow VLAN retagging on endpoints</li> <li>sap: a constrained list of at least one Layer2 service attach point</li> </ul>"},{"location":"architecture/product_modelling/modelling/","title":"Modelling","text":"<p>There are several ways in which network service products can be modelled and split up into logical parts (product blocks). This may for example depend on the requirements from the stakeholders, the environment such products are used in, and/or personal taste. It is important to highlight that the modelling process aims to identify the main attributes necessary to describe a product and the relations between product blocks \u2013 it is not prescriptive in terms of design and implementation of a specific product. This approach helps to decouple the two topics and keep them in separate functional domains.</p> <p>Looking at different sets of network service product models we do see that they share a set of core attributes, regardless of which product or product block they belong to. This is not surprising, because the key attributes needed to actually provision a network service on the network are the same for all of these. This sections describes a set of network service product models and their attributes that can either be extended to meet the specific needs of the environment they will be used in, or can serve as a basis for a new constellation of product models. There is no intention to supply a complete set of products that covers all possible NREN network services, but it should be enough to help inspire thinking about (network) product modelling in your organisation.</p>"},{"location":"architecture/product_modelling/node/","title":"Node","text":"<p>The administration handoff in IMS will be different for every organisation. For this example, it is assumed that all administration that comes with the physical installation and first-time configuration of the network node in IMS is done manually by a NOC engineer. This makes the node product rather simple. The only product block that is defined holds pointers to all related information that is stored in the operations support systems (OSS). This includes of course a pointer to the information in IMS, and after the service has been deployed on the network, another pointer to the related information in the NRM. To keep track of all IP addresses and prefixes used across the network service product, the pointers to the IPv4 and IPv6 loopback addresses on the node are also stored.</p> <p></p> <ul> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>ipv4_ipam_id: ID of the node\u2019s iPv4 loopback address in IPAM</li> <li>ipv6_ipam_id: ID of the node\u2019s iPv6 loopback address in IPAM</li> </ul>"},{"location":"architecture/product_modelling/port/","title":"Port","text":"<p>Once a NOC engineer has physically installed a port in a node and added some basic administration to IMS, the port is marked as available and can be further configured through the port product. To distinguish between ports with different speeds (1Gbit/s, 10Gbit/s, etcetera), the fixed input speed is used, which also allows filtering available ports of the right speed. Besides pointers to the administration of the port in IMS and the NRM, configuration options including 802.1Q, Ethernet auto negotiation, and the use of LLDP are registered, as well as a reference to the Node the port is installed in.</p> <p></p> <ul> <li>speed: the speed of the physical interface on the node in Mbit/s</li> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>mode: the port is either untagged, tagged or a link member in an aggregate</li> <li>auto_negotiation: enable Ethernet auto negotiation</li> <li>lldp: enable the link layer discovery protocol</li> <li>node: link to the Node product block the port is residing on</li> </ul>"},{"location":"architecture/product_modelling/product_block_graph/","title":"Product Block Instance Graph","text":"<p>A subscription for a specific customer for a product that is deployed on the network is stored in the Workflow Orchestrator database. The notion of subscription ownership allows for fine-grained control over which customer is allowed to change what attribute. By correctly adding references from one product block to another, a graph of product block instances is generated that accurately reflects the relations between the snippets of network node configuration that are deployed to the network. The graph is automatically added to when a new subscription is created, allowing easy and intuitive navigation through all configuration data. Once every network service is modelled and provisioned to the network through the Workflow Orchestrator, every line of network node configuration can be linked to the corresponding subscription that holds the configuration parameters.</p> <p>The example below shows the product block instance graph for a L2 point-to-point and a L2 VPN service between three ports on three different nodes. The nodes are owned by the respective NREN\u2019s Network Operations Centre (NOC). University A has ports on nodes on two different locations, and uses a L2 point-to-point service to connect these locations. Research Institute B has one port of its own, and uses a L2 VPN service for their collaboration with the university. The business rules that describe the (optional) authorisation logic for connecting subscriptions from different customers to each other are coded in the Workflow Orchestrator workflows related to these products.</p> <p></p>"},{"location":"architecture/product_modelling/standards/","title":"Standards","text":"<p>There are many standards describing how network service products and their attributes can be modelled. Most of these are very detailed as they try to cover as many use cases as possible, which can prove overwhelming. Here we aim to do the opposite and only model the bare minimum. This makes it easier to see the relationship between the network service models, and how each model can be extended with attributes that are specific to the organisation that uses them.</p> <p>A common way of modelling products is to split the models into a customer-facing part that contains all the attributes that are significant to the customer, and a resource-facing part that extends that set of attributes with all the attributes that are needed to actually deploy a service on the network. We assume here that such a separation is being used, where the customer-facing part lives in the Workflow Orchestrator and the resource-facing part lives in a provisioning system such as NSO or Ansible.</p>"},{"location":"architecture/product_modelling/terminology/","title":"Terminology","text":"<p>The data and business rules of the products and product blocks are modelled in Workflow Orchestrator domain models. A product is a collection of one or more product blocks, and zero or more fixed inputs. Fixed inputs are customer-facing attributes that cannot be changed at will by a customer because they are constrained in some way, for example by a physical constraint such as the speed of a port or a financial constraint such as the maximum capacity of a service. Product blocks are collections of resource types (customer-facing attributes) that together describe a set of attributes that can be repeated one or more times within a product and can optionally point to other product blocks. A product block is a logical collection of resource types that taken together make reusable instances. They can be referenced many times from within other products and make it possible to build a logical topology of the network within the orchestrator database. A subscription is a product instantiation for a specific customer. See the rest of the Workflow Orchestrator documentation for more details.</p>"},{"location":"contributing/development/","title":"Setting up a development environment","text":"<p>To add features to the repository follow the following procedure to setup a working development environment.</p>"},{"location":"contributing/development/#installation","title":"Installation","text":"<p>Install the project and its dependencies to develop on the code.</p>"},{"location":"contributing/development/#step-1-install-uv","title":"Step 1 - install uv","text":"<p>Follow the installing uv instructions.</p>"},{"location":"contributing/development/#step-2-install-project-and-dependencies","title":"Step 2 - install project and dependencies","text":"<pre><code>uv sync --all-groups --all-extras\n</code></pre> <p>This creates a virtual environment at <code>.venv</code> with the latest dependencies and version of python. This command can be repeated to update the venv i.e. when switching to a branch with different dependencies, or after not having worked on the project for a while.</p> <p>You can either activate the venv with <code>source .venv/bin/activate</code> or prefix commands with <code>uv run &lt;command&gt;</code>. We'll use the latter form throughout this documentation.</p> <p>More details in About UV.</p>"},{"location":"contributing/development/#running-tests","title":"Running tests","text":"<p>Run the unit-test suite to verify a correct setup.</p>"},{"location":"contributing/development/#step-1-create-a-database","title":"Step 1 - Create a database","text":"<p>Setup a postgres database (see Getting Started).</p> <p>Create a database and user:</p> <pre><code>createuser -sP nwa\ncreatedb orchestrator-core-test -O nwa\n</code></pre> <p>Set the password to something simple, like <code>nwa</code>.</p>"},{"location":"contributing/development/#step-2-run-tests","title":"Step 2 - Run tests","text":"<p>Ensure the application can reach the database: <pre><code>export DATABASE_URI=postgresql://nwa:nwa@localhost:5432/orchestrator-core-test\n</code></pre></p> <pre><code>uv run pytest test/unit_tests\n</code></pre> <p>If you do not encounter any failures in the test, you should be able to develop features in the orchestrator-core.</p>"},{"location":"contributing/development/#adding-to-the-documentation","title":"Adding to the documentation","text":"<p>Documentation for the Orchestrator is written by using Mkdocs. To contribute to them follow the instructions above to <code>step 2</code>, you can then develop them locally by running:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>This should make the docs available on your local machine here: http://127.0.0.1:8000/orchestrator-core/</p>"},{"location":"contributing/development/#changing-the-core-database-schema","title":"Changing the Core database schema","text":"<p>When you would like to change the core database schema, first ensure you have the following: * An up-to-date development environment * A postgres instance and <code>orchestrator-core-test</code> database setup as described in Running tests * Venv activated with <code>source .venv/bin/activate</code> (or prefix alembic commands with <code>uv run</code>)</p> <p>Then execute the following steps to create and test your schema change.</p> <pre><code># Change to the migrations dir\ncd orchestrator/migrations\n\n# Run all current migrations\nalembic upgrade heads\n\n# add or change models in orchestrator/db/models.py\nvim orchestrator/db/models.py\n\n# Generate a migration for your change\nalembic revision --autogenerate -m \"Demonstrating schema changes\"\n\n# Output of the previous command mentions the generated migration file:\n#   Generating /.../2025-06-27_68d7ec0a554c_demonstrating_schema_changes.py ...  done\n# Please open and review it carefully.\n\n# Show the revision history and the current version of your database\nalembic history -i\n# 161918133bec -&gt; 68d7ec0a554c (schema) (head), Demonstrating schema changes.\n# 68d14db1b8da -&gt; 161918133bec (schema) (current), Add is_task to workflow.\n# fc5c993a4b4a -&gt; 68d14db1b8da (schema), Make workflow description mandatory.\n\n# Test the upgrade path of your migration\nalembic upgrade 68d7ec0a554c\n\n# Test the downgrade path of your migration\nalembic downgrade 161918133bec\n</code></pre>"},{"location":"contributing/development/#about-uv","title":"About UV","text":"<p>uv is a very fast replacement for pip and flit which also adds a lot of functionality. This section explains a few concepts relevant to know when developing on the orchestrator-core. For a full overview consult the uv documentation.</p>"},{"location":"contributing/development/#adding-and-removing-dependencies","title":"Adding and removing dependencies","text":"<p>Instead of manually editing <code>pyproject.toml</code> you can use <code>uv add</code>.</p> <p>For example, let's say we want to become an enterprise application :-)</p> <pre><code>uv add django  # adds \"django&gt;=5.2.3\" to pyproject.toml\n</code></pre> <p>Apart from updating <code>pyproject.toml</code> uv also installs the dependency (and subdependencies) in your environment.</p> <p>It is a uv default to specify the lower limit. You can also let uv set an upper limit or set it explicitly:</p> <pre><code>uv add django --bounds minor   # \"django&gt;=5.2.3,&lt;5.3.0\"\nuv add django --bounds major   # \"django&gt;=5.2.3,&lt;6.0.0\"\nuv add 'django&gt;=5.2.3,&lt;6.0.0'  # \"django&gt;=5.2.3,&lt;6.0.0\"\n</code></pre> <p>To remove the dependency you can run:</p> <pre><code>uv remove django\n</code></pre> <p>Note: The add/remove commands can be given the options, <code>--group &lt;groupname&gt;</code> or <code>--optional &lt;optionalname&gt;</code> to target a specific group or optional.</p>"},{"location":"contributing/development/#python-interpreter","title":"Python interpreter","text":"<p>uv will create the venv with the latest python version supported by orchestrator-core. If that version is not found on your machine, it is automatically downloaded. Check the output of <code>uv help python</code> how to change this behavior.</p>"},{"location":"contributing/development/#uv-sync","title":"uv sync","text":"<p>The <code>uv sync</code> syncs your environment (<code>.venv</code>) based on the options you give it and what's configured in <code>pyproject.toml</code>.</p> <p>A few examples of different options:</p> <pre><code># base dependencies\nuv sync --no-dev\n\n# base + development dependencies\nuv sync\n\n# base + development + celery dependencies\nuv sync --extra celery\n\n# base + development + mkdocs dependencies\nuv sync --group docs\n</code></pre> <p>uv removes anything not specified in the command or <code>pyproject.toml</code> to ensure a clean and reproducible environment.</p>"},{"location":"contributing/development/#upgrade-dependencies","title":"Upgrade dependencies","text":"<p>To upgrade all dependencies in the lockfile to the latest version (taking version constraints from <code>pyproject.toml</code> into account), run the sync command with <code>--upgrade</code>:</p> <pre><code>uv sync --all-groups --all-extras --upgrade\n</code></pre> <p>The github project has Renovate enabled which can create PRs for dependency upgrades.</p>"},{"location":"contributing/development/#what-is-uvlock-for","title":"What is uv.lock for?","text":"<p>The lockfile is used for development and testing of the orchestrator-core. It captures the exact set of dependencies used to create the local environment.</p> <p>Anytime you execute <code>uv run</code> or <code>uv sync</code> the lockfile is updated if necessary. These updates must be committed because the CI pipeline will fail otherwise. It can also be updated explicitly with <code>uv lock</code>.</p> <p>When installing orchestrator-core in another project the lockfile is no longer relevant. In that case only the broad requirements from <code>pyproject.toml</code> apply.</p> <p>More information on the uv website.</p>"},{"location":"contributing/development/#useful-settings","title":"Useful settings","text":""},{"location":"contributing/development/#sqlalchemy-logging","title":"SQLAlchemy logging","text":"<p>WFO uses SQLAlchemy for its ORM and DB connection management capabilities.</p> <p>To get information about which DB queries it is performing, adjust it's loglevel through this environment variable:</p> <pre><code>LOG_LEVEL_SQLALCHEMY_ENGINE=INFO\n</code></pre> <p>Set it to <code>DEBUG</code> for even more information.</p> <p>Both INFO and DEBUG generate a lot of logging! It is not recommended to use this in production.</p>"},{"location":"contributing/development/#graphql-query-statistics","title":"GraphQL query statistics","text":"<p>To get basic statistics per executed GraphQL Query, set the following environment variable:</p> <pre><code>ENABLE_GRAPHQL_STATS_EXTENSION=true\n</code></pre> <p>This will add the following details to the result of the query, as well as logging them.</p> <pre><code>{\n    \"data\": [],\n    \"extensions\": {\n        \"stats\": {\n            \"db_queries\": 7,\n            \"db_time\": 0.006837368011474609,\n            \"operation_time\": 0.11549711227416992\n        }\n    }\n}\n</code></pre> <p>The following should be noted: * this requires monitoring of executed SQLAlchemy cursor events, which may have some overhead * the number of queries can most likely be skewed by async FastAPI REST calls that take place, so it's recommended to use this in an isolated and controlled environment</p>"},{"location":"contributing/development/#graphql-query-profiling","title":"GraphQL query profiling","text":"<p>It is possible to profile GraphQL queries by enabling the PyInstrument extension:</p> <pre><code>ENABLE_GRAPHQL_PROFILING_EXTENSION=true\n</code></pre> <p>This will create a file <code>pyinstrument.html</code> in the repository root which shows which parts of the code are taking up most of the execution time.</p> <p>Note that you need to have the orchestrator-core's test dependencies installed.</p> <p>This has a lot of overhead and we advise you to not use this in production.</p>"},{"location":"contributing/guidelines/","title":"Contributing","text":"<p>The workflow orchestrator projects welcomes any contributions from any party. If you are interested in contributing or have questions about the project please contact the board: workfloworchestrator.board@commonsconservancy.org or feel free to raise an issue in the project. We will strive to reply to your enquiry A.S.A.P.</p>"},{"location":"contributing/guidelines/#documentation","title":"Documentation","text":"<p>We use MKDOCS as a documentation tool. Please create a PR if you have any additions or contributions to make. All docs can be written in MD or html. Full guidelines on how to set this up can be found here.</p>"},{"location":"contributing/guidelines/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>We use pre-commit hooks to ensure that the code is formatted correctly and that the tests pass. To install the pre-commit hooks, run the following command:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>To run the pre-commit hooks manually, run the following command:</p> <pre><code>uv run pre-commit run --all-files\n</code></pre>"},{"location":"contributing/guidelines/#orchestrator-release","title":"Orchestrator release","text":"<p>The <code>orchestrator-core</code> has no release schedule but is actively used and maintained by the workflow orchestrator group. Creating a new release is done by the developers of the project and the procedure is as follows.</p>"},{"location":"contributing/guidelines/#release-candidates","title":"Release candidates","text":"<p>When creating new features they can be released in so-called <code>pre-releases</code> on github. Depending on the feature type the developer will need to run <code>bumpversion (major|minor|patch)</code> and then <code>bumpversion build --allow-dirty</code> to create a new release candidate. This command will update the <code>.bumpversion.cfg</code> and the <code>orchestrator/__init__.py</code> files.</p> <p>The next step would be to \"Create a new release\" -&gt; \"Fill in the tag and check the box, create tag upon release\" and use the checkbox \"pre-release.\"</p> <p>The code will be pushed to pypi.</p>"},{"location":"contributing/guidelines/#stable-releases","title":"Stable releases","text":"<p>Stable releases follow the same procedure as described above and can be either created from a release candidate by removing the <code>-rc</code> string from the <code>.bumpversion.cfg</code> and the <code>orchestrator/__init__.py</code> files. After that a new release can be created and the <code>Autogenerate changelog</code> option may be used.</p>"},{"location":"contributing/testing/","title":"Writing unit tests","text":"<p>Notes and lessons learned about writing unit tests for workflows.</p> <p>Point the first, there are <code>test</code> and <code>test_esnet</code> file hierarchies. The latter is a clone of the former with all of the SN specific stuff torn out. Any of our stuff should go in <code>test_esnet</code> but in some cases will still reference code in the <code>test</code> hierarchy. This is covered later.</p>"},{"location":"contributing/testing/#domain-model-tests","title":"Domain model tests","text":"<p>There is a test for the circuit transition domain models here:</p> <pre><code>orchestrator/test_esnet/unit_tests/domain/product_types/test_cts.py\n</code></pre> <p>These are relatively straightforward. There are basic imports which include the domain models being tested:</p> <pre><code>from server.db import Product, db\nfrom server.domain.product_blocks.cts import CircuitTransitionBlockInactive\nfrom server.domain.product_types.cts import CircuitTransition, CircuitTransitionInactive\nfrom server.types import SubscriptionLifecycle\n</code></pre> <p>It pulls in the inactive version of the product block, plus the default and inactive product type models. Then just write functions. They seemed to have defined two kinds of test (you can put them in the same module).</p>"},{"location":"contributing/testing/#new","title":"New","text":"<p>This test just creates a new model in the function. Super easy, instantiate the inactive version of the model and test that the default fields are properly defined.</p>"},{"location":"contributing/testing/#save-and-load","title":"Save and load","text":"<p>This is commented and a bit more complex. In the <code>conftest.py</code> file at the root of the test directory a <code>pytest</code> fixture is defined that creates the model and saves it in the db.</p> <p>Then the test function loads the version from the db, checks the contents, makes some changes, save it, and load it up again.</p> <p>Also pretty simple but you mostly need to know where the fixtures get defined.</p>"},{"location":"contributing/testing/#workflow-tests","title":"Workflow tests","text":"<p>There is a unit test for the ESnet circuit transition workflow here:</p> <pre><code>orchestrator/test_esnet/unit_tests/workflows/cts/test_create_cts.py\n</code></pre> <p>It's mostly complete and is liberally commented.</p>"},{"location":"contributing/testing/#fundamental-imports","title":"Fundamental imports","text":"<p>It uses the <code>pytest</code> framework, and some custom orchestrator code. So we need to pull in some imports for the framework and functions to run the workflow in the test. Finally, the function gets a decorator:</p> <pre><code>import uuid\nimport pytest\n\nfrom server.db import Product, Subscription\nfrom test.unit_tests.workflows import (\n    assert_complete,\n    assert_failed,\n    extract_error,\n    extract_state,\n    run_workflow,\n    assert_suspended,\n    resume_workflow,\n)\n\n\n@pytest.mark.workflow\ndef test_create_cts(responses):\n    pass\n</code></pre>"},{"location":"contributing/testing/#general-flow","title":"General flow","text":"<p>To start it off, just define the initial input content in a data structure and feed it to a function that starts the workflow:</p> <pre><code>initial_state = [\n    {\"product\": str(product.product_id)},\n    {\n        \"customer_id\": ESNET_ORG_UUID,\n        \"esnet5_circuit_id\": \"2\",\n        \"esnet6_circuit_id\": \"2\",\n        \"snow_ticket_assignee\": \"mgoode\",\n        \"noc_due_date\": \"2020-07-02 07:00:00\",\n    },\n]\n\nresult, process, step_log = run_workflow(\"create_circuit_transition\", initial_state)\nassert_suspended(result)\nstate = extract_state(result)\n</code></pre> <p>In this example it's a workflow suspends several times so the <code>assert_suspended</code> function is called. If the workflow doesn't have anything like that (ie: it's just one step) you can just let it go and call <code>assert_complete</code>. In the above example, you can pause, and examine the state object to make sure the contents are what is expected.</p> <p>To resume a multi-step/suspended workflow, you do this:</p> <pre><code>confirm_complete_prework = {\n    \"outbound_shipper\": \"fedex\",\n    \"return_shipper\": \"fedex\",\n    \"generate_shipping_details\": \"ACCEPTED\",\n    \"provider_receiving_ticket\": \"23432\",\n    \"provider_remote_hands_ticket\": \"345345\",\n    \"confirm_colo_and_ports\": \"ACCEPTED\",\n    \"complete_mops_info\": \"ACCEPTED\",\n    \"create_pmc_notification\": \"pmc notify\",\n    \"reserve_traffic_generator\": \"ACCEPTED\",\n}\n\nresult, step_log = resume_workflow(process, step_log, confirm_complete_prework)\n</code></pre> <p>One doesn't need to update the main state object, just create a fresh data structure of the new data and call <code>resume_workflow</code> - the new data will be added to the state object. Lather, rinse and repeat until the workflow is complete.</p>"},{"location":"contributing/testing/#http-mocking","title":"HTTP mocking","text":"<p>One non-obvious facet of the test framework is that it forces one to mock any HTTP calls going to an external service. This is defined by a fixture in the <code>conftest.py</code> file - this is compatible with the http lib we are using.</p> <p>Consider a test function prototype:</p> <pre><code>@pytest.mark.workflow\ndef test_create_cts(responses):\n    product = Product.query.filter(Product.name == \"Circuit Transition Service\").one()\n</code></pre> <p>That responses arg being passed in is the aforementioned fixture. This is then passed into your mock classes:</p> <pre><code>esdb = EsdbMocks(responses)\noauth = OAuthMocks(responses)\nsnow = SnowMocks(responses)\n</code></pre> <p><code>Product.name</code> needs to match the first argument of the <code>@create_workflow</code> decorator and needs to be defined as a product in the database or that call will fail.</p>"},{"location":"contributing/testing/#defining-a-mock","title":"Defining a mock","text":"<p>Here is the constructor and a single method from a mock file:</p> <pre><code>class OAuthMocks:\n    def __init__(self, responses):\n        self.responses = responses\n\n    def post_token(self):\n        response = r\"\"\"{\n  \"access_token\":\"MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3\",\n  \"token_type\":\"bearer\",\n  \"expires_in\":3600,\n  \"refresh_token\":\"IwOGYzYTlmM2YxOTQ5MGE3YmNmMDFkNTVk\",\n  \"scope\":\"create\"\n}\"\"\"\n        response = json.loads(response)\n\n        self.responses.add(\n            \"POST\",\n            f\"/oauth_token.do\",\n            body=json.dumps(response),\n            content_type=\"application/json\",\n        )\n</code></pre> <p>Pretty basic - define what the return payload looks like. One defines an HTTP verb, URI and the like.</p> <p>Also, if you're mocking a call that contains a query string make sure to include the</p> <pre><code>match_querystring = (True,)\n</code></pre> <p>flag to <code>responses.add()</code> or you'll go insane trying to figure out why it didn't register properly.</p> <p>Not going to lie, this part can get kind of tedious depending on the amount of calls you need to mock.</p>"},{"location":"contributing/testing/#registering-with-the-fixture-the-non-obvious-bit","title":"Registering with the fixture (the non-obvious bit)","text":"<p>The way this works is that rather than mimicking the name of the method being mocked, it does a look up using a two-tuple of the verb and the uri. And it needs to be registered with the fixture or else the lookup won't work. So back in the test function, one needs to do this before initiating the workflow:</p> <pre><code>oauth = OAuthMocks(responses)\n...\ntoken = oauth.post_token()\n</code></pre> <p>Even though you haven't run the workflow yet, and you won't use the return value, doing that registers the verb/uri pair with the fixture. Then going forward when the code executes and there is an HTTP call to that verb/uri pair, the contents of that method will be returned (payload, headers, etc).</p> <p>And if you try to cheat, the fixture will stop you. Any un-mocked HTTP call will raise an exception.</p>"},{"location":"contributing/testing/#running-the-tests","title":"Running the tests","text":"<p>The tests need to be run inside the container. First, to enable \"live\" updating, add this to the <code>volumes</code> stanza of the docker compose file:</p> <pre><code>      - ../test:/usr/src/app/test\n</code></pre> <p>Then shell into the container:</p> <pre><code>    docker exec -it backend /bin/bash\n</code></pre> <p>And run the test:</p> <pre><code>root@d30f71ee1afe:/usr/src/app# pytest -s test_esnet/unit_tests/workflows/cts/test_create_cts.py\n</code></pre> <p>The <code>-s</code> flag to <code>pytest</code> is needed if you want to see your print statements. Otherwise <code>pytest</code> will cheerfully eat them.</p>"},{"location":"contributing/testing/#gotchas-and-etc","title":"Gotchas and etc","text":""},{"location":"contributing/testing/#executing-multiple-tasks","title":"Executing multiple tasks","text":"<p>The <code>test_esnet</code> tree is a clone of the SN <code>test</code> tree with all of the SN specific stuff removed. Some tests may still reference code in the <code>test</code> tree - utility testing code for example:</p> <pre><code>from test.unit_tests.workflows import (\n    assert_complete,\n    assert_failed,\n    extract_error,\n    extract_state,\n    run_workflow,\n    assert_suspended,\n    resume_workflow,\n)\n</code></pre> <p>That's by design - those things are core orchestrator code so it stays put.</p> <p>At some point we might want to crib off of SN code or modify it (like some of the mocking code for example) - if so, go ahead and move it into our tree. The goal of this is to have the <code>test_esnet</code> tree be pretty lean and just have our stuff in it. That way we can also just run the entire tree w/out worrying about their stuff.</p>"},{"location":"contributing/testing/#test-db","title":"Test DB","text":"<p>See <code>.env.example</code> on how to set the URI for the database the testing framework uses. The original default was to use your \"production\" local db which had the super helpful side effect of trashing your orchestrator state.</p>"},{"location":"contributing/testing/#initial-state","title":"Initial state","text":"<p>The initial state for the form input is defined in a pretty straightforward way - at least for create workflows:</p> <pre><code>initial_state = [\n    {\"product\": str(product.product_id)},\n    {\n        \"customer_id\": ESNET_ORG_UUID,\n        \"esnet5_circuit_id\": \"2\",\n        \"esnet6_circuit_id\": \"2\",\n        \"snow_ticket_assignee\": \"mgoode\",\n        \"noc_due_date\": \"2020-07-02 07:00:00\",\n    },\n]\n\nresult, process, step_log = run_workflow(\"create_circuit_transition\", initial_state)\n</code></pre> <p>But there seems to be a gotcha when defining initial state for a terminate / etc workflow that modifies existing subscriptions:</p> <pre><code># Yes, the initial state is a list of two identical dicts.\n# Why? I don't know. But I do know if you don't do this an\n# maddening form incomplete validation error will happen. -mmg\ninitial_state = [\n    {\"subscription_id\": nes_subscription2},\n    {\n        \"subscription_id\": nes_subscription2,\n    },\n]\n\nresult, process, step_log = run_workflow(\"terminate_node_enrollment\", initial_state)\n</code></pre> <p>So if one gets a vague form validation error when doing this, it might be something alone these lines.</p>"},{"location":"contributing/testing/#insync-true","title":"insync = True","text":"<p>When defining a fixture in <code>conftest.py</code> to make an entry in the testing DB for a subscription that a unit test might consume, make sure to mark the subscription object <code>.insync = True</code>. Otherwise the unit test will fail thinking that it is attached to an active process.</p>"},{"location":"getting-started/agentic/","title":"LLM-enabled Orchestrator Core","text":"<p>Enhance the Orchestrator's functionality by enabling LLM features, which unlock both the search and agent modules for natural language interaction.</p>"},{"location":"getting-started/agentic/#features","title":"Features","text":""},{"location":"getting-started/agentic/#search-module","title":"Search Module","text":"<ul> <li>Enables semantic and structured search across subscriptions, products, processes, and workflows</li> <li>Utilizes vector embeddings for powerful search capabilities</li> <li>Works with or without LLM integration</li> </ul>"},{"location":"getting-started/agentic/#agent-module","title":"Agent Module","text":"<ul> <li>Enables natural language interaction with the Orchestrator</li> <li>Supports complex queries and operations through conversational interfaces</li> </ul> <p>Experimental Features</p> <p>The features and APIs described in this section are under active development and may change.</p> <p>Note: While these features are experimental, they unlock significant potential for advanced use cases.</p>"},{"location":"getting-started/agentic/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/agentic/#prerequisites","title":"Prerequisites","text":"<ul> <li>PostgreSQL with <code>pgvector</code> extension</li> <li>Python 3.9+</li> <li>OpenAI API key (or compatible LLM provider)</li> <li>(Optional) Configured UI for LLM integration</li> </ul>"},{"location":"getting-started/agentic/#1-installation","title":"1. Installation","text":"<p>Create and activate a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <p>Install the package with LLM dependencies:</p> <pre><code>pip install \"orchestrator-core[agent,search]\"\n</code></pre>"},{"location":"getting-started/agentic/#2-database-setup","title":"2. Database Setup","text":""},{"location":"getting-started/agentic/#using-docker-recommended","title":"Using Docker (Recommended)","text":"<pre><code>docker run --name orch-db -e POSTGRES_PASSWORD=yourpassword -p 5432:5432 -d pgvector/pgvector:pg17\ndocker exec -it orch-db createdb -U postgres orchestrator-core\n</code></pre>"},{"location":"getting-started/agentic/#manual-setup","title":"Manual Setup","text":"<ol> <li>Install PostgreSQL with pgvector</li> <li>Create a database and user:    <pre><code>CREATE USER nwa WITH PASSWORD 'yourpassword';\nCREATE DATABASE orchestrator-core OWNER nwa;\n</code></pre></li> </ol>"},{"location":"getting-started/agentic/#3-configuration","title":"3. Configuration","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>DATABASE_URI=postgresql://nwa:yourpassword@localhost:5432/orchestrator-core\nOPENAI_API_KEY=your_openai_api_key\nAGENT_MODEL=gpt-4o-mini\nSEARCH_ENABLED=true\nAGENT_ENABLED=true\n</code></pre>"},{"location":"getting-started/agentic/#4-application-setup","title":"4. Application Setup","text":"<p>Create a <code>main.py</code> file. This provides the CLI entrypoint to your Orchestrator.</p> <pre><code>import typer\nfrom nwastdlib.logging import initialise_logging\nfrom orchestrator import app_settings\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.db import init_database\nfrom orchestrator.log_config import LOGGER_OVERRIDES\n\ndef init_cli_app() -&gt; typer.Typer:\n    initialise_logging(LOGGER_OVERRIDES)\n    init_database(app_settings)\n    return core_cli()\n\nif __name__ == \"__main__\":\n    init_cli_app()\n</code></pre> <p>Create a <code>wsgi.py</code> file. This will be used to run the Orchestrator API.</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.settings import app_settings\nfrom orchestrator.llm_settings import llm_settings\n\napp = OrchestratorCore(\n    base_settings=app_settings,\n    llm_settings=llm_settings,\n    llm_model=llm_settings.AGENT_MODEL,\n    agent_tools=[]\n)\n</code></pre>"},{"location":"getting-started/agentic/#5-database-migrations","title":"5. Database Migrations","text":"<pre><code>export $(grep -v '^#' .env | xargs)\npython main.py db init\npython main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/agentic/#6-start-the-application","title":"6. Start the Application","text":"<pre><code>uvicorn --reload --host 0.0.0.0 --port 8080 wsgi:app\n</code></pre>"},{"location":"getting-started/agentic/#7-index-your-data","title":"7. Index Your Data","text":"<p>API Costs</p> <p>This step makes API calls to your LLM provider and may incur costs.</p> <pre><code># Index all available data\npython main.py index all\n\n# Or index specific types\npython main.py index subscriptions\npython main.py index products\npython main.py index processes\npython main.py index workflows\n</code></pre>"},{"location":"getting-started/agentic/#using-the-api","title":"Using the API","text":"<p>Once running, you can access: - API Documentation: http://localhost:8080/api/docs - ReDoc: http://localhost:8080/api/redoc</p>"},{"location":"getting-started/agentic/#next-steps","title":"Next Steps","text":"<ul> <li>Create a product</li> <li>Create a workflow for a product</li> <li>Generate products and workflows</li> <li>Lookup the reference documentation</li> </ul>"},{"location":"getting-started/base/","title":"Base application","text":"<p>By following these steps you can start a bare orchestrator application that can be used to run workflows. This app runs as a standalone API with workflows loaded that can be run in the background. Similar to a Framework like FastAPI, Flask and Django, you install the core library, initialise it with configuration and run. The orchestrator-core contains:</p> <ul> <li>API</li> <li>Workflow engine</li> <li>Database</li> </ul> <p>Note</p> <p>The Orchestrator-core is designed to be installed and extended just like a FastAPI or Flask application. For more information about how this works read the Reference documentation.</p>"},{"location":"getting-started/base/#step-1-install-the-package","title":"Step 1 - Install the package:","text":"<p>Create a virtualenv and install the core.</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install orchestrator-core\n</code></pre>"},{"location":"getting-started/base/#step-2-setup-the-database","title":"Step 2 - Setup the database:","text":"<p>Create a postgres database:</p> <pre><code>createuser -sP nwa\ncreatedb orchestrator-core -O nwa\n</code></pre> <p>Choose a password and remember it for later steps.</p> <p>As an example, you can run these docker commands in separate shells to start a temporary postgres instance:</p> <pre><code>docker run --rm --name temp-orch-db -e POSTGRES_PASSWORD=rootpassword -p 5432:5432 pgvector/pgvector:pg17\n\ndocker exec -it temp-orch-db su - postgres -c 'createuser -sP nwa &amp;&amp; createdb orchestrator-core -O nwa'\n</code></pre>"},{"location":"getting-started/base/#step-3-create-the-mainpy-and-wsgipy","title":"Step 3 - Create the main.py and wsgi.py:","text":"<p>Create a <code>main.py</code> file. This provides the CLI entrypoint to your Orchestrator.</p> <pre><code>import typer\nfrom nwastdlib.logging import initialise_logging\nfrom orchestrator import app_settings\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.db import init_database\nfrom orchestrator.log_config import LOGGER_OVERRIDES\n\ndef init_cli_app() -&gt; typer.Typer:\n    initialise_logging(LOGGER_OVERRIDES)\n    init_database(app_settings)\n    return core_cli()\n\nif __name__ == \"__main__\":\n    init_cli_app()\n</code></pre> <p>Create a <code>wsgi.py</code> file. This will be used to run the Orchestrator API.</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.settings import app_settings\n\napp = OrchestratorCore(base_settings=app_settings)\n</code></pre>"},{"location":"getting-started/base/#step-4-run-the-database-migrations","title":"Step 4 - Run the database migrations:","text":"<p>Initialize the migration environment and database tables.</p> <pre><code>export DATABASE_URI=postgresql://nwa:PASSWORD_FROM_STEP_2@localhost:5432/orchestrator-core\n\npython main.py db init\npython main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/base/#step-5-run-the-app","title":"Step 5 - Run the app","text":"<pre><code>export DATABASE_URI=postgresql://nwa:PASSWORD_FROM_STEP_2@localhost:5432/orchestrator-core\nexport OAUTH2_ACTIVE=False\n\nuvicorn --reload --host 127.0.0.1 --port 8080 wsgi:app\n</code></pre>"},{"location":"getting-started/base/#step-6-profit","title":"Step 6 - Profit","text":"<p>Visit the ReDoc or OpenAPI to view and interact with the API.</p>"},{"location":"getting-started/base/#next","title":"Next:","text":"<ul> <li>Create a product.</li> <li>Create a workflow for a product.</li> <li>Generate products and workflows</li> </ul>"},{"location":"getting-started/docker/","title":"Docker development","text":"<p>As well as developing within a regular python environment it is also possible to develop with a docker environment. This method clones our example-orchestrator repo and kickstarts the development from this mono-repo setup.</p> <p>Note</p> <p>This method of developing is meant for beginners who would like to have a very opinionated version of the orchestrator that already has some pre-built integrations.</p>"},{"location":"getting-started/docker/#shipped-inside-this-repo","title":"Shipped inside this repo","text":"<p>This repo contains a <code>docker-compose</code> that builds the following applications:</p> <ul> <li>Orchestrator-core</li> <li>Orchestrator-ui</li> <li>Postgres</li> <li>Redis</li> <li>NetBox</li> <li>GraphQL Federation</li> </ul> <p>Furthermore the repository also contains a lot of example code for some of the example products that have been implemented. If you would like to quickly get to know the application please follow the README.md to find out how the docker setup works.</p>"},{"location":"getting-started/orchestration-ui/","title":"The Orchestrator web interface","text":"<p>As part of the Workflow Orchestrator setup it's possible to run a web interface. An example of how to setup the web interface is provided in the Example orchestrator UI repository. It works 'out-of-the-box' with a standard Workflow orchestrator engine and can be expanded with extra fields and pages and customized with your branding. It shows an example local development setup.</p>"},{"location":"getting-started/orchestration-ui/#short-overview","title":"Short overview","text":"<p>The Orchestrator UI is based on the NextJS framework which in turn is based on React. It uses the Orchestrator UI component library NPM package to provide most of the functionality. The Example orchestrator UI repository shows how to provide configuration and customization.</p>"},{"location":"getting-started/orchestration-ui/#prerequisites","title":"Prerequisites:","text":"<p>Before running an installation of the Workflow Orchestrator UI make sure to have these installed</p> <pre><code>- NodeJS, version &gt; 21\n- npm\n- A git client\n</code></pre>"},{"location":"getting-started/orchestration-ui/#installation","title":"Installation","text":"<p>Clone the example UI repository:</p> <pre><code>mkdir orchestrator-ui\ncd orchestrator-ui\ngit clone https://github.com/workfloworchestrator/example-orchestrator-ui .\n</code></pre> <p>Install the npm packages:</p> <pre><code>npm i\n</code></pre> <p>Set the correct env variables. To run the UI against the dockerized example orchestrator setup, it's recommended to use the <code>orchestrator-ui.env</code> env-file.</p> <pre><code>wget -O .env https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator/refs/heads/master/docker/orchestrator-ui/orchestrator-ui.env\n</code></pre> <p>Run the application:</p> <pre><code>npm run dev\n</code></pre> <p>The Orchestrator UI now runs on http://localhost:3000</p>"},{"location":"getting-started/prepare-source-folder/","title":"Prepare source folder","text":""},{"location":"getting-started/prepare-source-folder/#folder-layout","title":"Folder layout","text":"<p>The suggested folder layout of your own orchestrator implementation is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 migrations\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 schema\n\u251c\u2500\u2500 products\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 product_blocks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 product_types\n\u251c\u2500\u2500 translations\n\u2514\u2500\u2500 workflows\n</code></pre> <ul> <li>skip to creating a workflow</li> </ul> <p>Some of the orchestrator-core functionality relies on the source folder being a valid Git repository.</p>"},{"location":"getting-started/prepare-source-folder/#migrations-folder-and-local-alembic-head","title":"Migrations folder and local Alembic head","text":"<p>The <code>migrations</code> folder is created when running <code>python main.py db init</code>. This folder is used to store the local Alembic database migrations. The orchestrator-core package also contains Alembic database migrations, the Alembic head in the core is labeled <code>schema</code>. The local Alembic head should be labeled <code>data</code>.</p> <p>When you already have product(s), workflow(s) or product template(s) defined, the local Alembic head can be created by running either <code>python main.py db migrate-domain-models</code>, <code>python main.py db migrate-workflows</code>, or <code>python main.py generate migration</code>, these commands will detect a missing <code>data</code> head and will add one automatically.  Please refer to the command-line documentation for more information on these commands.</p> <p>Or a handcrafted local migration can be added to the <code>migrations/versions/schema</code> folder using the following as template:</p> <pre><code>\"\"\"Create data head.\n\nRevision ID: 9cbc348cc1dc\nRevises:\nCreate Date: 2023-10-19T10:51:44.368621\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"9cbc348cc1dc\"\ndown_revision = None\nbranch_labels = (\"data\",)\ndepends_on = \"da5c9f4cce1c\"\n\n\ndef upgrade() -&gt; None:\n    pass\n\n\ndef downgrade() -&gt; None:\n    pass\n</code></pre> <p>The <code>depends_on</code> version ID should point to the current <code>schema</code> head.</p> <p>To check the correct configuration of the local Alembic head, run the follwoing command:</p> <pre><code>python main.py db heads\n</code></pre> <p>The output should be similar to this (except for the version ID's):</p> <pre><code>27571992ebfb (data) (head)\nda5c9f4cce1c (schema) (effective head)\n</code></pre> <p>Or have a look at the Alembic migration version history with:</p> <pre><code>python main.py db history\n</code></pre> <p>That will display somthing like this:</p> <pre><code>&lt;base&gt; (da5c9f4cce1c) -&gt; 9cbc348cc1dc (data), Create data head.\n165303a20fb1 -&gt; da5c9f4cce1c (schema) (effective head), Add subscription metadata to fulltext search index.\na09ac125ea73 -&gt; 165303a20fb1 (schema), customer_id to VARCHAR.\nb1970225392d -&gt; a09ac125ea73 (schema), Add throttling to refresh_subscriptions_view trigger.\ne05bb1967eff -&gt; b1970225392d (schema), Add subscription metadata workflow.\nbed6bc0b197a -&gt; e05bb1967eff (schema), Add Subscriptions search view.\n19cdd3ab86f6 -&gt; bed6bc0b197a (schema), rename_parent_and_child_block_relations.\n6896a54e9483 -&gt; 19cdd3ab86f6 (schema), fix_parse_websearch.\n3c8b9185c221 -&gt; 6896a54e9483 (schema), Add product_block_relations.\n3323bcb934e7 -&gt; 3c8b9185c221 (schema), Add task_validate_products.\na76b9185b334 -&gt; 3323bcb934e7 (schema), Fix tsv triggers.\nc112305b07d3 -&gt; a76b9185b334 (schema), Add generic workflows to core.\n&lt;base&gt; -&gt; c112305b07d3 (schema), Initial schema migration.\n</code></pre>"},{"location":"getting-started/prepare-source-folder/#products-workflows-and-translations-folders","title":"Products, workflows and translations folders","text":"<p>The <code>products</code>, <code>workflows</code> and <code>translations</code> folders are either created by hand, or when the <code>python main.py generate --help</code> commands are used in combination with product templates, will be created automatically.</p>"},{"location":"getting-started/versions/","title":"Prerequisites","text":"<p>The orchestrator backend has the following requirements</p>"},{"location":"getting-started/versions/#backend","title":"Backend","text":"<p>For the backend you need the following packages:</p> <ul> <li>Python &gt;= 3.11</li> <li>Postgres &gt;= 15</li> </ul>"},{"location":"getting-started/versions/#optional-dependencies","title":"Optional dependencies","text":"<ul> <li>Redis</li> <li>Docker</li> </ul>"},{"location":"getting-started/workflows/","title":"Workflows","text":""},{"location":"getting-started/workflows/#creating-a-workflow","title":"Creating a workflow","text":"<p>A workflow is the combination of:</p> <ul> <li>An initial input form \u2014 used to collect input from the user.</li> <li>A sequence of workflow steps \u2014 defining the logic to be executed.</li> </ul> <p>For a more detailed explanation, see \ud83d\udc49 Detailed explanation of workflows</p> <p>There are specialized decorators for each workflow type that execute \"default\" steps before and after the steps from your workflow. It is recommended to use these decorators because they ensure correct functioning of the Orchestrator.</p> <ul> <li>create_workflow</li> <li>modify_workflow</li> <li>terminate_workflow</li> <li>validate_workflow</li> </ul> <p>Under the hood they all use a workflow decorator which can be used for tasks that don't fit any of the types above.</p> <p>The decorated function must return a chain of steps using the <code>&gt;&gt;</code> operator to define their execution order.</p>"},{"location":"getting-started/workflows/#minimal-create-workflow-example","title":"Minimal create workflow example","text":"<pre><code>from orchestrator.workflows.utils import create_workflow\nfrom orchestrator.workflow import StepList, begin\n\n\n@create_workflow(\n    \"Create product subscription\",\n    initial_input_form=initial_input_form_generator\n)\ndef create_product_subscription() -&gt; StepList:\n    return begin &gt;&gt; create_subscription\n</code></pre> <p>In this example:</p> <ul> <li>The workflow is named \"Create product subscription\".</li> <li>The input form is defined by <code>initial_input_form_generator</code>.</li> <li>The workflow engine will execute the steps inside <code>create_workflow</code> before returned steps,    <code>create_subscription</code>, and steps inside <code>create_workflow</code> after returned steps.</li> </ul> <p>Each step should be defined using the <code>@step</code> decorator and can access and update the shared subscription model.</p>"},{"location":"getting-started/workflows/#how-workflow-steps-work","title":"How workflow steps work","text":"<p>Information between workflow steps is passed using <code>State</code>, which is nothing more than a collection of key/value pairs. In Python the state is represented by a <code>Dict</code>, with string keys and arbitrary values. Between steps the <code>State</code> is serialized to JSON and stored in the database.</p> <p>The <code>@step</code> decorator converts a function into a workflow step. Arguments to the step function are automatically filled using matching keys from the <code>State</code>. The function must return a dictionary of new or updated key-value pairs, which are merged into the <code>State</code> and passed to the next step. The serialization and deserialization between JSON and the indicated Python types are done automatically. A minimal workflow step looks as follows:</p> <pre><code>@step(\"Create subscription\")\ndef create_subscription(\n    product: UUID,\n    user_input: str,\n) -&gt; State:\n    subscription = build_subscription(product, user_input)\n    return {\"subscription\": subscription}\n</code></pre> <p>In this step:</p> <ul> <li><code>product</code> and <code>user_input</code> are populated from the <code>State</code>.</li> <li>The return value includes a new key <code>subscription</code>, which will be available to the next step in the workflow.</li> </ul> <p>Every workflow starts with the builtin step <code>init</code> and ends with the builtin step <code>done</code>,  with an arbitrary list of other builtin steps or custom steps in between. the workflow type decorators have these included and can use <code>begin &gt;&gt; your_step</code>.</p> <p>Domain models as parameters are subject to special processing. With the previous step, the <code>subscription</code> is available in the state, which for the next step, can be used directly with the Subscription model type, for example:</p> <pre><code>@step(\"Add subscription to external system\")\ndef add_subscription_to_external_system(\n    subscription: MySubscriptionModel,\n) -&gt; State:\n    payload = subscription.my_block\n    response = add_to_external_system(payload)\n    return {\"response\": response}\n</code></pre> <p>For <code>@modify_workflow</code>, <code>@validate_workflow</code> and <code>@terminate_workflow</code> the <code>subscription</code> is directly usable from the first step.</p> <p>Information about all usable step decorators can be found on the architecture page on workflows.</p>"},{"location":"getting-started/workflows/#register-workflows","title":"Register workflows","text":"<p>To make workflows available in the orchestrator, they must be registered in two stages:</p> <ol> <li>In code \u2014 by defining them as workflow functions and registering them via <code>LazyWorkflowInstance</code>.</li> <li>In the database \u2014 by mapping them to the corresponding <code>product_type</code> using a migration.<ul> <li>workflows don't need to necessarily be added to a product_type, doing this will only make them available as tasks not meant to be ran by a subscription.</li> </ul> </li> </ol> <p>We\u2019ll start with the code registration, followed by options for generating the database migration.</p>"},{"location":"getting-started/workflows/#step-1-register-workflow-functions-in-code","title":"Step 1: Register workflow functions in code","text":"<p>Workflow functions must be registered by creating a <code>LazyWorkflowInstance</code>, which maps a workflow function to the Python module where it's defined.</p> <p>Example \u2014 registering the <code>create_user_group</code> workflow:</p> <pre><code>from orchestrator.workflows import LazyWorkflowInstance\n\nLazyWorkflowInstance(\"workflows.user_group.create_user_group\", \"create_user_group\")\n</code></pre> <p>To ensure the workflows are discovered at runtime:</p> <ul> <li>Add all <code>LazyWorkflowInstance(...)</code> calls to <code>workflows/__init__.py</code>.</li> <li>Add <code>import workflows</code> to <code>main.py</code> so they are registered during app startup.</li> </ul> <p>Example</p> <p>For inspiration look at an example implementation of the lazy workflow instances</p>"},{"location":"getting-started/workflows/#step-2-register-workflows-in-the-database","title":"Step 2: Register workflows in the database","text":"<p>After registering workflows in code, you need to add them to the database by mapping them to their <code>product_type</code>. There are three ways to do this:</p> <ul> <li>Migrate workflows generator script</li> <li>Copy the example workflows migration</li> <li>Manual</li> </ul>"},{"location":"getting-started/workflows/#migrate-workflows-generator-script","title":"Migrate workflows generator script","text":"<p>Similar to <code>db migrate-domain-models</code>, the orchestrator command line interface offers the <code>db migrate-workflows</code> command that walks you through a menu to create a database migration file based on the difference between the registered workflows in the code and the database.</p> <p>Start with the following command:</p> <pre><code>python main.py db migrate-workflows \"add User and UserGroup workflows\"\n</code></pre> <p>Navigate through the menu to add the six workflows to the corresponding <code>User</code> or <code>UserGroup</code> product type. After confirming a migration file will be added to <code>migrations/versions/schema</code>.</p> <p>The migration can be run with:</p> <pre><code>python main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/workflows/#copy-the-example-workflows-migration","title":"Copy the example workflows migration","text":"<p>You can copy a predefined migration file from the example repository:</p> <pre><code>(\n  cd migrations/versions/schema\n  curl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/2022-11-12_8040c515d356_add_user_and_usergroup_workflows.py\n)\n</code></pre> <p>Update it to your own workflow and update the database with:</p> <pre><code>python main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/workflows/#manual","title":"Manual","text":"<p>Create a new empty database migration with the following command:</p> <pre><code>python main.py db revision --head data --message \"add User and UserGroup workflows\"\n</code></pre> <p>This will create an empty database migration in the folder <code>migrations/versions/schema</code>. For the migration we will make use of the migration helper functions <code>create_workflow</code> and <code>delete_workflow</code> that both expect a <code>Dict</code> that describes the workflow registration to be added or deleted from the database.</p> <p>To add all User and UserGroup workflows in bulk a list of <code>Dict</code> is created, for only the UserGroup create workflow the list looks like this:</p> <pre><code>from orchestrator.targets import Target\n\nnew_workflows = [\n    {\n        \"name\": \"create_user_group\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create user group\",\n        \"product_type\": \"UserGroup\",\n    },\n]\n</code></pre> <p>This registers the workflow function <code>create_user_group</code> as a create workflow for the <code>UserGroup</code> product.</p> <p>Add a list of <code>Dict</code>s describing the create, modify and terminate workflows for both the <code>UserGroup</code> and <code>User</code> products to the migration that was created above.</p> <p>The migration <code>upgrade</code> and <code>downgrade</code> functions will just loop through the list:</p> <pre><code>from orchestrator.migrations.helpers import create_workflow, delete_workflow\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n</code></pre> <p>Run the migration with the following command:</p> <pre><code>python main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/workflows/#more-workflow-examples","title":"More workflow examples","text":""},{"location":"getting-started/workflows/#validate","title":"Validate","text":"<p>Validate workflows run integrity checks on an existing subscription. Checking the state of associated data in an external system for example. The validate migration parameters look something like this:</p> <pre><code>new_workflows = [\n    {\n        \"name\": \"validate_node_enrollment\",\n        \"target\": Target.VALIDATE,\n        \"description\": \"Validate Node Enrollment before production\",\n        \"product_type\": \"Node\",\n        \"is_task\": True,\n    },\n]\n</code></pre> <p>This workflow uses <code>Target.VALIDATE</code>, which explicitly distinguishes it from system tasks that use <code>Target.SYSTEM</code>. While both are marked with <code>is_task=True</code> and treated as tasks, they serve different purposes:</p> <ul> <li><code>SYSTEM</code> workflows are typically used for background processing and internal orchestration.</li> <li><code>VALIDATE</code> workflows are used to confirm that a subscription is still correct and consistent, verifying that external systems are still in sync with it.</li> </ul> <p>Validate workflow steps generally raise an <code>AssertionError</code> when a condition fails. If all checks pass, they return a simple success marker (e.g., \"OK\") to the workflow state.</p> <pre><code>@step(\"Check NSO\")\ndef check_nso(subscription: NodeEnrollment, node_name: str) -&gt; State:\n    device = get_device(device_name=node_name)\n\n    if device is None:\n        raise AssertionError(f\"Device not found in NSO\")\n    return {\"check_nso\": \"OK\"}\n</code></pre>"},{"location":"getting-started/workflows/#modify","title":"Modify","text":"<p>The <code>Modify</code> workflow is similar to a <code>Validate</code> workflow, but uses different migration parameters appropriate to its <code>Target.MODIFY</code> context.</p> <pre><code>new_workflows = [\n    {\n        \"name\": \"modify_node_enrollment\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Modify Node Enrollment\",\n        \"product_type\": \"Node\",\n    },\n]\n</code></pre> <p>This type of workflow applies changes to an existing subscription. If necessary, it can also update the subscription\u2019s lifecycle state at the end of the process. For example, suppose a <code>CREATE</code> workflow initially sets the subscription to the <code>PROVISIONING</code> state. A follow-up <code>Modify</code> workflow might transition it to production and set the lifecycle state to <code>ACTIVE</code>:</p> <pre><code>@step(\"Activate Subscription\")\ndef update_subscription_and_description(subscription: NodeEnrollmentProvisioning, node_name: str) -&gt; State:\n    subscription = NodeEnrollment.from_other_lifecycle(subscription)\n    subscription.description = f\"Node {node_name} Production\"\n\n    return {\"subscription\": subscription}\n</code></pre> <p>These also have the <code>subscription</code> passed in in the initial step as outlined above.</p>"},{"location":"getting-started/workflows/#terminate","title":"Terminate","text":"<p>A Terminate workflow is used to cleanly remove a subscription and undo any changes made during its lifecycle.</p> <p>The migration params are as one would suspect:</p> <p><pre><code>new_workflows = [\n    {\n        \"name\": \"terminate_node_enrollment\",\n        \"target\": Target.TERMINATE,\n        \"description\": \"Terminate Node Enrollment subscription\",\n        \"product_type\": \"Node\",\n    },\n]\n</code></pre> Here, the <code>target</code>, <code>name</code>, and <code>description</code> follow standard naming conventions for <code>terminate</code> workflows.</p> <p>The first step of a terminate workflow can be used to store identifiers in the state, for example:</p> <pre><code>@step(\"Load relevant subscription information\")\ndef load_subscription_info(subscription: NodeEnrollment) -&gt; FormGenerator:\n    node = get_detailed_node(subscription.ne.esdb_node_id)\n    return {\"subscription\": subscription, \"node_name\": node.get(\"name\")}\n</code></pre> <p>This approach ensures that the workflow has all the necessary context to safely tear down the subscription and associated resources.</p>"},{"location":"guides/pause-and-resume/","title":"Pausing the Orchestrator","text":"<p>This document explains the different Orchestrator Engine states and outlines the various methods to pause and resume the Orchestrator engine.</p>"},{"location":"guides/pause-and-resume/#orchestrator-engine-states","title":"Orchestrator Engine States","text":"<p>The Orchestrator engine operates in three distinct states that control workflow execution and system behavior.</p> Status Description Workflow Behavior Transitions <code>RUNNING</code> Normal operational state New workflows can start, existing workflows with status <code>RUNNING</code> continue execution Can transition to <code>PAUSING</code> <code>PAUSING</code> Transitional state during shutdown No new workflows accepted, existing workflows are being gracefully stopped Automatically transitions to <code>PAUSED</code> when complete <code>PAUSED</code> Fully stopped state No workflow activity, all processes stopped Can transition back to <code>RUNNING</code> \u00e5 ## Pause and Resume the Orchestrator There are several ways to pause (and resume) the Orchestrator:"},{"location":"guides/pause-and-resume/#1-using-the-api","title":"1. Using the API","text":""},{"location":"guides/pause-and-resume/#pause-orchestrator","title":"Pause Orchestrator","text":"<p>You can send a <code>PUT</code> request to the <code>/api/settings/status</code> endpoint with the <code>global_lock</code> parameter  set to <code>true</code> to pause the Orchestrator. This will stop all running workflows and prevent new workflows from starting.</p> <p>Via CLI: <pre><code>curl -X PUT http://localhost:8080/api/settings/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"global_lock\": true}'\n</code></pre></p> <p>Using Python: <pre><code>import requests\n\nresponse = requests.put(\n    \"http://localhost:8080/api/settings/status\",\n    json={\"global_lock\": True}\n)\nstatus = response.json()\n</code></pre></p> <p>Note</p> <p>The Orchestrator Engine State should be <code>RUNNING</code> before pausing via above API call.</p>"},{"location":"guides/pause-and-resume/#resume-orchestrator","title":"Resume Orchestrator","text":"<p>You can send a <code>PUT</code> request to the <code>/api/settings/status</code> endpoint with the <code>global_lock</code> parameter  set to <code>false</code> to resume the Orchestrator. This will allow new workflows to start and existing  workflows to continue execution.</p> <pre><code>curl -X PUT http://localhost:8080/api/settings/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"global_lock\": false}'\n</code></pre> <p>Note</p> <p>The Orchestrator Engine State should be <code>PAUSED</code> before resuming via above API call.</p>"},{"location":"guides/pause-and-resume/#api-docs","title":"API Docs","text":"<p>You can also pause and resume the Orchestrator via the interactive Swagger UI API docs.</p>"},{"location":"guides/pause-and-resume/#2-using-the-ui","title":"2. Using the UI","text":"<p>If you have access to the WFO UI (e.g. when running the <code>example-orchestrator</code> or when running both the <code>orchestrator-core</code> and <code>orchestrator-ui</code>), you can pause the Orchestrator from there.</p> <ol> <li>Navigate to the \"Settings\" page in the left sidebar.</li> <li>Click the \"Pause workflow engine\" button.</li> </ol>"},{"location":"guides/scaling/","title":"Scaling the Orchestrator","text":"<p>By default the Orchestrator is capable to handle a reasonable amount of workflows and tasks. For a larger and more distributed workload we introduced the Celery library.</p> <p>This document describes the two modes in which an Orchestrator instance can run and what you need to configure:</p> <ol> <li>running tasks and workflows in a threadpool (default)</li> <li>running the Orchestrator with a number of workflow workers</li> </ol>"},{"location":"guides/scaling/#running-workflows-or-tasks-within-a-threadpool","title":"Running workflows or tasks within a threadpool","text":"<p>This is the default configuration. Workflows and tasks are both scheduled by the same threadpool with equal priority. If you need to have tasks with a lower priority, you can for example use a scheduler and run them during a quiet period.</p> <p>In AppSettings you will notice the default <code>\"threadpool\"</code>, which can be updated to <code>\"celery\"</code> directly or overridden via the <code>EXECUTOR</code> environment variable.</p> <pre><code>class AppSettings(BaseSettings):\n    # fields omitted\n\n    EXECUTOR: str = \"threadpool\"\n\n    # fields omitted\n</code></pre>"},{"location":"guides/scaling/#running-workflows-or-tasks-using-a-worker","title":"Running workflows or tasks using a worker","text":"<p>When the orchestrator-core's process executor is specified as <code>\"celery\"</code>, the FastAPI application registers Celery-specific task functions, and <code>start_process</code> and <code>resume_process</code> now defer to the Celery task queue.</p> <p>For those new to Celery, we recommend the Celery introduction.</p> <p>When using Celery, the Orchestrator is split into two parts:</p> <ol> <li>orchestrator-api</li> <li>orchestrator-worker</li> </ol> <p>The orchestrator-api functionality is now limited to handling REST requests and delegating them (via one or more queues) to the orchestrator-worker. The workflows are executed in the orchestrator-worker.</p> <p>The orchestrator-worker has additional dependencies which can be installed with the <code>celery</code> dependency group:</p> <pre><code>pip install orchestrator-core[celery]\n</code></pre> <p>Celery's task queue enables features like nightly validations by providing a task queue and workers to execute workflows that are all started in parallel, which would crash a single-threaded orchestrator-core.</p> <p>The application flow looks like this when <code>EXECUTOR = \"celery\"</code> and websockets are enabled:</p> <ul> <li>FastAPI application validates form input and places a task on Celery queue (<code>tasks.new_workflow</code>)<ul> <li>If websockets are enabled, a connection should exist already between the client and backend.</li> </ul> </li> <li>FastAPI application begins watching Redis pubsub channel for process updates from Celery.</li> <li>Celery worker picks up task from queue and begins executing.</li> <li>On each step completion, it publishes state information to the Redis pubsub channel.</li> <li>FastAPI application grabs this information and publishes it to the client websocket connection.</li> </ul> <p>By default, Redis is used for the Celery broker and backend, but these can be overridden.</p>"},{"location":"guides/scaling/#invoking-celery","title":"Invoking Celery","text":"<p>A Celery worker must start by calling your worker module instead of <code>main.py</code>, like so:</p> <pre><code>celery -A your_orch.celery_worker worker -E -l INFO -Q new_tasks,resume_tasks,new_workflows,resume_workflows\n</code></pre> <ul> <li><code>-A</code> points to this module where the worker class is defined</li> <li><code>-E</code> sends task-related events (capturable and monitorable)</li> <li><code>-l</code> is the short flag for <code>--loglevel</code></li> <li><code>-Q</code> specifies the queues which the worker should watch for new tasks</li> </ul>"},{"location":"guides/scaling/#queues","title":"Queues","text":"<p>Tasks and workflows are submitted on different queues:</p> <ul> <li><code>tasks</code>: starting or resuming tasks</li> <li><code>workflows</code>: starting or resuming workflows</li> </ul> <p>This allows for independent scaling of workers that handle low priority tasks and high priority workflows simply by letting the workers listen to different queues. For example, a user starting a CREATE workflow expects timely resolution, and shouldn't have to wait for a scheduled validation to complete in order to start their workflow.</p> <p><code>\"orchestrator.services.tasks\"</code> is the namespace in orchestrator-core where the Celery tasks (i.e. Celery jobs, not Orchestrator tasks) can be found. At the moment, 4 Celery tasks are defined as constants in <code>services/tasks.py</code>:</p> <ol> <li><code>tasks.new_task</code>: start a new task (delivered on the Task queue)</li> <li><code>tasks.new_workflow</code>: start a new workflow (delivered on the Workflow queue)</li> <li><code>tasks.resume_task</code>: resume an existing task (delivered on the Task queue)</li> <li><code>tasks.resume_workflow</code>: resume an existing workflow (delivered on the Workflow queue)</li> </ol> <p>To handle the Tasks and Workflows queues independently, use the <code>-Q</code> option described above. That is, kick off one worker with</p> <pre><code>celery -A your_orch.celery_worker worker -E -l INFO -Q new_tasks,resume_tasks\n</code></pre> <p>and the other with</p> <pre><code>celery -A your_orch.celery_worker worker -E -l INFO -Q new_workflows,resume_workflows\n</code></pre> <p>The queues are defined in the Celery config in <code>services/tasks.py</code>:</p> <pre><code>celery.conf.task_routes = {\n    NEW_TASK: {\"queue\": \"new_tasks\"},\n    NEW_WORKFLOW: {\"queue\": \"new_workflows\"},\n    RESUME_TASK: {\"queue\": \"resume_tasks\"},\n    RESUME_WORKFLOW: {\"queue\": \"resume_workflows\"},\n}\n</code></pre> <p>If you decide to override the queue names in this configuration, you must also update the names accordingly after the <code>-Q</code> flag.</p>"},{"location":"guides/scaling/#worker-count","title":"Worker count","text":"<p>How many workers one needs for each queue depends on the number of subscriptions they have, what resources (mostly RAM) they have available, and how demanding their workflows/tasks are on external systems.</p> <p>Currently, SURF recommends 1 worker per queue by default. You can then scale those up after observing which queues experience the most contention for your workflows.</p>"},{"location":"guides/scaling/#implementing-the-worker","title":"Implementing the worker","text":"<p>The orchestrator-core needs to know what workflows a user has defined. After creating workflows, you should have registered them. For the default threadpool executor, these are exposed to the application by importing them in <code>main.py</code> to ensure the registration calls are made. When using the Celery executor, you'll need to do this again for the worker instance(s) to run those registrations.</p> <p>Below is an example implementation of a Celery worker with Websocket support, which can be updated to your project's needs.</p> <pre><code>\"\"\"This module contains functions and classes necessary for celery worker processes.\n\nThe application flow looks like this when EXECUTOR = \"celery\" (and websockets are enabled):\n\n- FastAPI application validates form input, and places a task on celery queue (create new process).\n  - If websockets are enabled, a connection should exist already b/t the client and backend.\n- FastAPI application begins watching Redis pubsub channel for process updates from celery.\n- Celery worker picks up task from queue and begins executing.\n- On each step completion, it publishes state information to Redis pubsub channel.\n- FastAPI application grabs this information and publishes it to the client websocket connection.\n\"\"\"\n\nfrom structlog import get_logger\nfrom uuid import UUID\n\nfrom celery import Celery\nfrom celery.signals import worker_shutting_down\nfrom nwastdlib.debugging import start_debugger\nfrom orchestrator.db import init_database\nfrom orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\nfrom orchestrator.types import BroadcastFunc\nfrom orchestrator.websocket import broadcast_process_update_to_websocket, init_websocket_manager\nfrom orchestrator.websocket.websocket_manager import WebSocketManager\nfrom orchestrator.workflows import ALL_WORKFLOWS\n\n# Substitute your_orch with your org's Orchestrator instance.\n# class AppSettings(OrchSettings):\n#     ...\n#\n# app_settings = AppSettings()\nfrom your_orch.settings import app_settings\n\n\nlogger = get_logger(__name__)\n\n\ndef process_broadcast_fn(process_id: UUID) -&gt; None:\n    # Catch all exceptions as broadcasting failure is noncritical to workflow completion\n    try:\n        broadcast_process_update_to_websocket(process_id)\n    except Exception as e:\n        logger.exception(e)\n\n\nclass OrchestratorWorker(Celery):\n    websocket_manager: WebSocketManager\n    process_broadcast_fn: BroadcastFunc\n\n    def on_init(self) -&gt; None:\n        # Depending on how you gate your debug settings, you can do something like this:\n        # if app_settings.DEBUG:\n        #     start_debugger()\n\n        init_database(app_settings)\n\n        # Prepare the wrapped_websocket_manager\n        # Note: cannot prepare the redis connections here as broadcasting is async\n        self.websocket_manager = init_websocket_manager(app_settings)\n        self.process_broadcast_fn = process_broadcast_fn\n\n        # Load the product and workflow modules to register them with the application\n        import your_orch.products\n        import your_orch.workflows\n\n\n    def close(self) -&gt; None:\n        super().close()\n\n\ncelery = OrchestratorWorker(\n    f\"{app_settings.SERVICE_NAME}-worker\", broker=str(app_settings.CACHE_URI), include=[\"orchestrator.services.tasks\"]\n)\n\nif app_settings.TESTING:\n    celery.conf.update(backend=str(app_settings.CACHE_URI), task_ignore_result=False)\nelse:\n    celery.conf.update(task_ignore_result=True)\n\ncelery.conf.update(\n    result_expires=3600,\n    worker_prefetch_multiplier=1,\n    worker_send_task_event=True,\n    task_send_sent_event=True,\n)\n</code></pre> <p>Create a file with the above, for example <code>my_orchestrator/celery_client.py</code>.</p> <p>Next, update your <code>main.py</code> and <code>wsgi.py</code> to include the following imports:</p> <pre><code>from orchestrator.services.tasks import initialise_celery\nfrom my_orchestrator.celery_client import celery\n</code></pre> <p>And finally, ensure both files include <code>initialise_celery(celery)</code> in the initialization of the CLI or API app.</p>"},{"location":"guides/scaling/#redis","title":"Redis","text":"<p>As you can see in the code above, we are using Redis as a broker. You can of course replace this by RabbitMQ or another broker of your choice. See the Celery documentation for more details.</p>"},{"location":"guides/scaling/#running-locally","title":"Running locally","text":"<p>If you want to test your application locally you have to start both the orchestrator-api and one or more workers. For example:</p> <p>Start the orchestrator API with Celery as the executor:</p> <pre><code>EXECUTOR=\"celery\" uvicorn --reload --host 127.0.0.1 --port 8080 wsgi:app\n</code></pre> <p>Start a single worker that listens both on the <code>tasks</code> and <code>workflows</code> queue (indicated by the <code>-Q</code> flag):</p> <pre><code>celery -A surf.tasks  worker --loglevel=info -Q new_tasks,resume_tasks,new_workflows,resume_workflows\n</code></pre> <p>Notice that <code>-A surf.tasks</code> indicates the module that contains your <code>celery.Celery</code> instance.</p>"},{"location":"guides/scaling/#celery-workflowtask-flow","title":"Celery workflow/task flow","text":"<p>This diagram shows the current flow of how we execute a workflow or task with celery. It's created to show the reason why a workflow/task can get stuck on <code>CREATED</code> or <code>RESUMED</code> and what we've done to fix it. All step statuses are shown in UPPERCASE for clarity.</p> <p></p>"},{"location":"guides/tasks/","title":"Scheduling tasks in the Orchestrator","text":"<p>This document covers the moving parts needed to schedule jobs in the orchestrator.</p> <p>In short, the scheduler is simply an additional process that adds Tasks to the queue. A Task is just a workflow that isn't tied to a specific product. Tasks are created in the same way as workflows, but with the <code>\"system\"</code> target, i.e.</p> <pre><code>@workflow(\"Some task\", target=Target.SYSTEM)\ndef some_task() -&gt; StepList:\n    return init &gt;&gt; foo &gt;&gt; done\n</code></pre> <p>Such a workflow will be flagged as a task in the database, and will not have a relation defined connecting it to a specific product.</p> <p>Note that <code>@workflow</code> is a lower-level call than, say, <code>@create_workflow</code>. So instead of <code>return begin &gt;&gt; foo</code>, we need to use <code>return init &gt;&gt; foo &gt;&gt; done</code> to instantiate a <code>StepList</code>.</p>"},{"location":"guides/tasks/#the-task-file","title":"The task file","text":"<p>Let's step through a more complete example. Four things need to happen to register a task:</p> <ol> <li>Defining the task via <code>@workflow</code></li> <li>Registering the task via <code>LazyWorkflowInstance</code> in your workflows module</li> <li>Writing or generating a migration file</li> <li>Adding a translation for the frontend (necessary for the task to show in the UI)</li> </ol>"},{"location":"guides/tasks/#task-code","title":"Task code","text":"<p>Here is a very bare-bones task file:</p> <pre><code># workflows/tasks/nightly_sync.py\n\nimport structlog\nimport time\n\nfrom orchestrator.targets import Target\nfrom orchestrator.types import State\nfrom orchestrator.workflow import StepList, done, init, step, workflow\n\nlogger = structlog.get_logger(__name__)\n\n\n@step(\"NSO calls\")\ndef nso_calls() -&gt; State:\n    logger.info(\"Start NSO calls\", ran_at=time.time())\n    time.sleep(5)  # Do stuff\n    logger.info(\"NSO calls finished\", done_at=time.time())\n\n\n@workflow(\"Nightly sync\", target=Target.SYSTEM)\ndef task_sync_from() -&gt; StepList:\n    return init &gt;&gt; nso_calls &gt;&gt; done\n</code></pre> <p>Again, the task is basically a workflow with <code>target=Target.SYSTEM</code>.</p> <p>And like a workflow, it will need to be registered in your workflows module:</p> <pre><code># workflows/__init__.py\n\n# Tasks\nLazyWorkflowInstance(\".tasks.nightly_sync\", \"task_sync_from\")\n</code></pre>"},{"location":"guides/tasks/#the-task-migration","title":"The task migration","text":"<p>Like other workflows, a task needs to be registered in the database in addition to being defined in the code. However, instead of <code>create_workflow</code>, simply use the <code>create_task</code> helper instead.</p> <pre><code>from orchestrator.migrations.helpers import create_task, delete_workflow\n\nnew_tasks = [\n    {\n        \"name\": \"task_sync_from\",\n        \"description\": \"Nightly validate and NSO sync\",\n    }\n]\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    for task in new_tasks:\n        create_task(conn, task)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for task in new_tasks:\n        delete_workflow(conn, task[\"name\"])\n</code></pre>"},{"location":"guides/tasks/#running-the-task-in-the-ui","title":"Running the task in the UI","text":"<p>After the migration is applied, the new task will surface in the UI under the Tasks tab. It can be manually executed that way. Even if the task does not have any form input, an entry will still need to be made in <code>translations/en-GB.json</code>.</p> <pre><code>// translations/en-GB.json\n{\n  ...\n  \"workflows\": {\n    ...\n    \"task_sync_from\": \"Verify and NSO sync\",\n  }\n}\n</code></pre>"},{"location":"guides/tasks/#the-schedule-file","title":"The schedule file","text":"<p>Warning</p> <p>As of v4.7.0 this is deprecated, and it will be removed in v5.0.0. Please use the new scheduling system below.</p> <p>The schedule file is essentially the crontab associated with the task. Continuing with our previous example:</p> <pre><code># schedules/nightly_sync.py\n\nfrom orchestrator.schedules.scheduler import scheduler\nfrom orchestrator.services.processes import start_process\n\n\n# previously `scheduler()` which is now deprecated\n@scheduler.scheduled_job(id=\"nightly-sync\", name=\"Nightly sync\", trigger=\"cron\", hour=1)\ndef run_nightly_sync() -&gt; None:\n    start_process(\"task_sync_from\")\n</code></pre> <p>This schedule will start the <code>task_sync_from</code> task every day at 01:00.</p> <p>There are multiple triggers that can be used (trigger docs):</p> <ul> <li>IntervalTrigger: use when you want to run the task at fixed intervals of time.</li> <li>CronTrigger: use when you want to run the task periodically at certain time(s) of day.</li> <li>DateTrigger: use when you want to run the task just once at a certain point of time.</li> <li>CalendarIntervalTrigger: use when you want to run the task on calendar-based intervals, at a specific time of day.</li> <li>AndTrigger: use when you want to combine multiple triggers so the task only runs when all of them would fire at the same time.</li> <li>OrTrigger: use when you want to combine multiple triggers so the task runs when any one of them would fire.</li> </ul> <p>For detailed configuration options, see the APScheduler scheduling docs.</p> <p>The scheduler automatically loads any schedules that are imported before the scheduler starts. To keep things organized and consistent (similar to how workflows are handled), it\u2019s recommended to place your schedules in a <code>/schedules/__init__.py</code>.</p> <p><code>ALL_SCHEDULERS</code> (Backwards Compatibility) In previous versions, schedules needed to be explicitly listed in an ALL_SCHEDULERS variable. This is no longer required, but ALL_SCHEDULERS is still supported for backwards compatibility.</p>"},{"location":"guides/tasks/#the-schedule-api","title":"The schedule API","text":"<p>Info</p> <p>In v4.4.0 we switched from schedule package to apscheduler to allow schedules to be stored in the DB and retrieve schedule tasks from the API. The apscheduler library has its own decorator to schedule tasks: <code>@scheduler.scheduled_job()</code> (from <code>orchestrator.schedules.scheduler</code>). We therefore deprecated the old <code>@schedule</code> decorator (from <code>orchestrator.schedules.scheduling</code>) and made it forwards compatible.</p> <p>In v4.7.0 we deprecated <code>@scheduler.scheduled_job()</code> provided by apscheduler in favor of a more dynamic API based system described below. Although we no longer support the <code>@scheduler.scheduled_job()</code> decorator, it is still available because it is part of apscheduler. Therefore, we do NOT recommend using it for new schedules. Because you will miss a Linker Table join between schedules and workflows/tasks.</p> <p>Consult the v4.7 upgrade guide for more details.</p> <p>Schedules can be created, updated, and deleted via the REST API, and retrieved via the already existing GraphQL API. It will become possible to manage schedules through the UI (development ticket), but you may also use the API directly to automate configuration of your schedules.</p> <p>Example POST</p> <p>To create a schedule, you can now simply run a <code>POST</code> request to the <code>/api/schedules</code> endpoint with a JSON body containing the schedule details. An example body to create a nightly sync schedule would look like this:</p> <pre><code>{\n  \"name\": \"Nightly Product Validation\",\n  \"workflow_name\": \"validate_products\",\n  \"workflow_id\": \"e96cc6bb-9494-4ac1-a572-050988487ee1\",\n  \"trigger\": \"interval\",\n  \"trigger_kwargs\": {\n    \"hours\": 12\n  }\n}\n</code></pre> <p>Respectively, you can update or delete schedules via <code>PUT</code> and <code>DELETE</code> requests to the same endpoint.</p> <p>Example PUT</p> <p>With <code>PUT</code> you can only update the <code>name</code>, <code>trigger</code>, and <code>trigger_kwargs</code> of an existing schedule. For example, to update the above schedule to run every 24 hours instead of every 12 hours <pre><code>{\n  \"schedule_id\": \"c1b6e5e3-d9f0-48f2-bc65-3c9c33fcf561\",\n  \"name\": \"Updated Nightly Cleanup\",\n  \"trigger\": \"interval\",\n  \"trigger_kwargs\": {\n    \"hours\": 24\n  }\n}\n</code></pre></p> <p>Example DELETE</p> <p>To delete a schedule, you only need to provide the <code>schedule_id</code> in the <code>DELETE</code> call <pre><code>{\n  \"workflow_id\": \"b67d4ca7-19fb-4b83-a022-34c6322fb5f1\",\n  \"schedule_id\": \"1fe43a96-b0f4-4c89-b9b7-87db14bbd8d3\"\n}\n</code></pre></p> <p>There are multiple triggers that can be used (trigger docs):</p> <ul> <li>IntervalTrigger: use when you want to run the task at fixed intervals of time.</li> <li>CronTrigger: use when you want to run the task periodically at certain time(s) of day.</li> <li>DateTrigger: use when you want to run the task just once at a certain point of time.</li> <li>CalendarIntervalTrigger: use when you want to run the task on calendar-based intervals, at a specific time of day.</li> <li>AndTrigger: use when you want to combine multiple triggers so the task only runs when all of them would fire at the same time.</li> <li>OrTrigger: use when you want to combine multiple triggers so the task runs when any one of them would fire.</li> </ul> <p>For detailed configuration options, see the APScheduler scheduling docs.</p> <p>The scheduler automatically loads any schedules that are imported before the scheduler starts.</p> <p>Info</p> <p>In previous versions, schedules needed to be explicitly added in the <code>ALL_SCHEDULERS</code> variable. This is no longer required; <code>ALL_SCHEDULERS</code> is deprecated as of orchestrator-core v4.7.0 and will be removed in v5.0.0. Follow-up ticket to remove deprecated code: #1276</p>"},{"location":"guides/tasks/#the-scheduler","title":"The scheduler","text":"<p>The scheduler is invoked via <code>python main.py scheduler</code>. Try <code>--help</code> or review the CLI docs to learn more.</p>"},{"location":"guides/tasks/#initial-schedules","title":"Initial schedules","text":"<p>From version orchestrator-core &gt;= <code>4.7.0</code>, the scheduler uses the database to store schedules instead of hard-coded schedule files. Previous versions (orchestrator-core &lt; <code>4.7.0</code> had hard-coded schedules. These can be ported to the new system by creating them via the API or CLI. Run the following CLI command to import previously existing orchestrator-core schedules and change them if needed via the API.</p> <pre><code>python main.py scheduler load-initial-schedule\n</code></pre> <p>Remember, that if you do not explicitly import these, they will not be available to the scheduler.</p>"},{"location":"guides/tasks/#manually-executing-tasks","title":"Manually executing tasks","text":"<p>When doing development, it is possible to manually make the scheduler run your task even if your Orchestrator instance is not in \"scheduler mode.\"</p> <p>Shell into your running instance and run the following:</p> <pre><code>docker exec -it backend /bin/bash\npython main.py scheduler force \"c1b6e5e3-d9f0-48f2-bc65-3c9c33fcf561\"\n</code></pre> <p>...where <code>c1b6e5e3-d9f0-48f2-bc65-3c9c33fcf561</code> is the job id of the schedule you want to run. The job id can be found via the GraphQL API or directly in the database.</p>"},{"location":"guides/tasks/#starting-the-scheduler","title":"Starting the scheduler","text":"<p>The scheduler runs as a separate process - it isn't just a feature in the backend that gets toggled on. In short, the scheduler is started by calling <code>python main.py scheduler run</code>. The scheduler will then run the jobs as they have been scheduled in the schedule files - and they will also be available to be run manually on an ad hoc basis in the UI.</p> <p>When running Orchestrator in Docker, you can run the scheduler in its own container, or you can fork it from the main process of your backend in a pinch.</p> <p>The first option can be accomplished by re-using your Orchestrator image with a new entrypoint and command: <pre><code># Dockerfile for scheduler image\nFROM your-orchestrator\nENTRYPOINT [\"python\", \"main.py\"]\nCMD [\"scheduler\", \"run\"]\n</code></pre></p> <p>For the second option: suppose you start your app with a script, <code>bin/server</code>, that handles your migrations, kicks off uvicorn, etc. You can then replace your backend's Docker entrypoint with a script like this, <code>bin/wrapper</code>:</p> <pre><code>#!/bin/sh\n# bin/wrapper\n\n# Start the scheduler.\npython main.py scheduler run &amp;\nstatus=$?\nif [ $status -ne 0 ]; then\n  echo \"Failed to start scheduler: $status\"\n  exit $status\nfi\n\n# Start the server backend process in the background.\n/usr/src/app/bin/server\nstatus=$?\nif [ $status -ne 0 ]; then\n  echo \"Failed to start backend: $status\"\n  exit $status\nfi\n</code></pre>"},{"location":"guides/product-modeling/backfilling/","title":"Backfilling Existing Subscriptions","text":"<p>When updating a product block that already exists in your orchestrator, it could be the case that new attributes are added or removed. When removing resource types from a <code>ProductBlock</code>, the <code>migrate-domain-models</code> command is able to pick up on this change, and generates a migration that removes these resource types from your database completely.</p> <p>However, when adding a new resource type to a <code>ProductBlock</code>, pre-existing product instances in the subscription database are not backfilled. For this, another SQL transaction must be added to the generated migration file.</p>"},{"location":"guides/product-modeling/backfilling/#generating-a-database-migration","title":"Generating a Database Migration","text":"<p>After the new resource type is added to the product block, the generated migration file should already contain at least the following two transactions:</p> <pre><code>conn.execute(sa.text(\"\"\"\nINSERT INTO resource_types (resource_type, description) VALUES ('site_contains_optical_equipment', 'Whether a site contains optical equipment') RETURNING resource_types.resource_type_id\n\"\"\"))\n\nconn.execute(sa.text(\"\"\"\nINSERT INTO product_block_resource_types (product_block_id, resource_type_id) VALUES ((SELECT product_blocks.product_block_id FROM product_blocks WHERE product_blocks.name IN ('SiteBlock')), (SELECT resource_types.resource_type_id FROM resource_types WHERE resource_types.resource_type IN ('site_contains_optical_equipment')))\n\"\"\"))\n</code></pre> <p>Note that this will correctly add the new resource type to the database, but is missing a backfill for a default value. In the case of this example, a new resource type <code>site_contains_optical_equipment</code> is added. Assume that all subscriptions that already exist in the database already are sites that contain optical equipment. It would therefore make sense to backfill the value <code>True</code> into all subscriptions that already exist.</p>"},{"location":"guides/product-modeling/backfilling/#adapting-the-generated-database-migration","title":"Adapting the Generated Database Migration","text":"<p>To implement this backfilling mechanism, only one SQL statement needs to be added to the migration, given below.</p> <pre><code>conn.execute(sa.text(\"\"\"\nWITH rt_id AS (SELECT resource_type_id FROM resource_types WHERE resource_type = 'site_contains_optical_equipment') INSERT INTO subscription_instance_values (subscription_instance_id, resource_type_id, value) SELECT subscription_instance_id, rt_id.resource_type_id, 'True' FROM rt_id, subscription_instances WHERE product_block_id = (SELECT product_block_id FROM product_blocks WHERE name = 'SiteBlock');\n\"\"\"))\n</code></pre> <p>Adding this statement at the end of the <code>upgrade()</code> method in the generated migration file, will set the value of <code>site_contains_optical_equipment</code> in all existing Site subscriptions to <code>True</code>. A more formatted version of this SQL statement is given here. To adapt this example to your needs, update the resource type name, default value, and name of the product block where this resource type is added.</p> <pre><code>WITH rt_id AS (\n    SELECT\n        resource_type_id\n    FROM\n        resource_types\n    WHERE\n        -- The name of your new resource type\n        resource_type = 'site_contains_optical_equipment'\n)\nINSERT INTO\n    subscription_instance_values (\n        subscription_instance_id,\n        resource_type_id,\n        value\n    )\nSELECT\n    subscription_instance_id,\n    rt_id.resource_type_id,\n    'True'  -- The new value that is backfilled\nFROM\n    rt_id,\n    subscription_instances\nWHERE\n    product_block_id = (\n        SELECT\n            product_block_id\n        FROM\n            product_blocks\n        WHERE\n            -- The name of the product block where this value is backfilled\n            name = 'SiteBlock'\n    );\n</code></pre> <p>The <code>downgrade()</code> method of the generated migration does not need any modification to work. It simply removes the resource type from the subscription database altogether.</p>"},{"location":"guides/product-modeling/imports/","title":"Importing Existing Subscriptions","text":"<p>When adding new products to Workflow Orchestrator, these products can rely on existing estate in the network that first needs to be added to the service database. This page describes a procedure that can be used to have a separate creation and import workflow for one product.</p> <p>As a requirement, each product type must have exactly one workflow where <code>target=Target.CREATE</code>. It is therefore not possible to have for example two workflows <code>create_node</code> and <code>import_node</code>. It is possible however, to have a separate product type, that relies on the same product block.</p>"},{"location":"guides/product-modeling/imports/#using-separate-product-types","title":"Using Separate Product Types","text":"<p>In the code example below, an example is given for a <code>Node</code> product type that needs to be imported into Orchestrator.</p> <pre><code>class Node(NodeProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    \"\"\"A node that is currently active.\"\"\"\n    node: NodeBlock\n\nclass ImportedNode(ImportedNodeInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    \"\"\"An imported node that is currently active.\"\"\"\n    node: NodeBlock\n</code></pre> <p>In this example, both types contain the same product block, which is required for this approach to import existing products. The creation workflow for a <code>Node</code> will remain <code>create_node</code>, which is most likely a workflow that takes user input from a form page, and then interacts with external systems to provision a new node.</p> <p>For the <code>ImportedNode</code>, the creation workflow is <code>create_imported_node</code>. This workflow can be designed to still take user input from a form page, but could also be provided programmatically by either a CLI command or API endpoint that is called. Data sources can include external API resources, CSV- or YAML files, etc. If desired, interaction with all external provisioning systems can be skipped, resulting in a creation workflow that could be as simple as follows.</p> <pre><code>from orchestrator.workflow import StepList, begin\nfrom orchestrator.workflows.steps import store_process_subscription\nfrom orchestrator.workflows.utils import create_workflow\n\n@create_workflow(\"Create imported Node\")\ndef create_imported_node() -&gt; StepList:\n    \"\"\"Workflow to import a Node without provisioning it.\"\"\"\n    return (\n        begin\n        &gt;&gt; create_subscription\n        &gt;&gt; store_process_subscription()\n        &gt;&gt; initialize_subscription\n    )\n</code></pre>"},{"location":"guides/product-modeling/imports/#importing-products","title":"Importing Products","text":"<p>With the <code>ImportedNode</code> part of the service database, we need a modification workflow to take the imported product to a <code>Node</code> subscription. This is done using a modification workflow <code>import_node</code> that is added to the <code>ImportedNode</code> product.</p> <p>This workflow is another place where external provisioning could take place, but it could also be a straightforward workflow alike the example given earlier. For the modification of importing the <code>Node</code> product, the following serves as an example.</p> <pre><code>from orchestrator.workflow import StepList, begin, step\nfrom orchestrator.workflows.utils import modify_workflow\n\n@step(\"Create new Node subscription\")\ndef import_node_subscription(subscription_id: UUIDstr) -&gt; State:\n    \"\"\"Take an ImportedNode subscription, and turn it into a Node subscription.\"\"\"\n    imported_node = ImportedNode.from_subscription(subscription_id)\n    new_product_id = get_product_id_by_name(ProductName.NODE)\n    new_subscription = Node.from_other_product(imported_node, new_product_id)\n\n    return {\"subscription\": new_subscription}\n\n\n@modify_workflow(\"Import Node\", target=Target.MODIFY)\ndef import_node() -&gt; StepList:\n    \"\"\"Modify into a Node subscription to complete the import.\"\"\"\n    return begin &gt;&gt; import_node_subscription\n</code></pre> <p>In this workflow, the existing <code>ImportedNode</code> subscription is modified into a <code>Node</code> subscription, and is stored in the service database. Now, the import has been completed, and the imported variety of the product has been replaced with a \"regular\" <code>Node</code> subscription.</p> <p>In short, the procedure is visualized in the following flowchart.</p> <pre><code>graph LR\n  A[CSV file with existing nodes] --&gt;|create_imported_node| B[ImportedNode];\n  B --&gt;|import_node| C[Node];</code></pre> <p>With <code>create_imported_node</code> as the creation workflow, and <code>import_node</code> as a modification workflow of the <code>ImportedNode</code> product.</p>"},{"location":"guides/upgrading/2.0/","title":"2.0 Migration Guide","text":"<p>In this document we'll help you migrate your orchestrator application from orchestrator-core 1.3 to 2.0.</p>"},{"location":"guides/upgrading/2.0/#about-20","title":"About 2.0","text":"<p>In this release we migrate the orchestrator-core from Pydantic v1 to Pydantic v2. This major change took a lot of effort (PR) as Pydantic v2 is quite different.</p> <p>There were quite a few breaking changes that we had to deal with. We have adapted orchestrator-core to mitigate the impact of these breaking changes. Your orchestrator application may need to be changed as well, we will guide you in this process. Several changes have been automated through a migration script - more about this later.</p>"},{"location":"guides/upgrading/2.0/#steps","title":"Steps","text":"<p>Make sure you have upgraded to the latest orchestrator-core 1.3.x release before starting.</p> <ul> <li>Create a new branch in your own orchestrator</li> <li>Create and activate a virtualenv with python 3.11 (recommended), 3.10 or 3.9 and install your dependencies</li> <li>Run <code>pip install -U orchestrator-core~=2.0.0</code> to get the latest 2.0 core version and upgrade all other dependencies<ul> <li>Add this to your pinned dependencies</li> </ul> </li> <li>Run <code>pip install bump-pydantic</code> to install a migration tool made by the Pydantic maintainers<ul> <li>No need to add this to your dependencies</li> </ul> </li> <li>Run <code>bump-pydantic .</code><ul> <li>It is a bit fragile and not very fast on large codebases. You can run it against specific subdirectories to work   around problematic files or slow performance</li> <li>The tool will leave <code># TODO[pydantic]</code> comments on pydantic 1.x code that it cannot automatically rewrite to   pydantic 2.x</li> </ul> </li> <li>Run <code>python -m orchestrator.devtools.scripts.migrate_20 &lt;dir&gt;</code> and point <code>&lt;dir&gt;</code> to your orchestrator code and tests<ul> <li>This migration script will perform a number of rewrites on your code for orchestrator-core 2.0</li> </ul> </li> </ul> <p>Then continue with the following sections.</p>"},{"location":"guides/upgrading/2.0/#pydantic-v2-changes","title":"Pydantic v2 Changes","text":"<p>First and foremost, the full Pydantic v2 migration guide is located here https://docs.pydantic.dev/2.5/migration.</p> <p>We recommend to read through it quickly before continuing with this guide.</p> <p>Next we'll give you some pointers of what to change based on our experience of migrating the SURF orchestrator.</p>"},{"location":"guides/upgrading/2.0/#change-renamed-basemodel-functions","title":"Change renamed BaseModel functions","text":"<p>Replace these deprecated function calls on any Pydantic Models (i.e. Subscriptions, ProductBlocks).</p> Pydantic V1 Pydantic V2 __fields__ model_fields __validators__ __pydantic_validator__ construct() model_construct() copy() model_copy() dict() model_dump() json_schema() model_json_schema() json() model_dump_json() parse_obj() model_validate() update_forward_refs() model_rebuild() <p>Full overview: https://docs.pydantic.dev/2.5/migration/#changes-to-pydanticbasemodel</p>"},{"location":"guides/upgrading/2.0/#replace-constrained-types-to-python-annotated","title":"Replace constrained types to Python Annotated[ ]","text":"<p>Most constrained types have been removed from Pydantic v2. We will show constrained int as an example.</p> <p>The symbols in the after example can be imported with: <pre><code>from typing import Annotated\nfrom annotated_types import Ge, Le, Len, MinLen, MaxLen, doc\n</code></pre></p> <p>conint</p> <p>Before:</p> <pre><code>ipv4_prefixlen: conint(ge=30, le=31)\n</code></pre> <p>After: <pre><code>ipv4_prefixlen: Annotated[int, Ge(30), Le(31)]\n</code></pre></p> <p>ConstrainedInt</p> <p>Before: <pre><code>class NumberOfPeerings(ConstrainedInt):\n    \"\"\"Number of peerings.\"\"\"\n\n    ge = 1\n    le = MAX_NUMBER_OF_PEERINGS\n</code></pre></p> <p>After: <pre><code>NumberOfPeerings = Annotated[\n    int, Ge(1), Le(MAX_NUMBER_OF_PEERINGS), doc(\"Number of peerings.\")\n]\n</code></pre></p>"},{"location":"guides/upgrading/2.0/#orchestrator-core-20-changes","title":"orchestrator-core 2.0 changes","text":"<p>The following breaking changes have been made:</p> <ul> <li>Removed <code>SubscriptionInstanceList</code> -&gt; see SubscriptionInstanceList removed (covered by migration-script)</li> <li>Removed <code>@serializable_property</code> -&gt; use <code>@computed_field</code> instead (covered by migration-script)</li> <li>Removed <code>DomainModel.get_properties</code> -&gt; use <code>BaseModel.model_computed_fields()</code> instead</li> <li>Removed <code>build_extendend_domain_model()</code> -&gt; use <code>build_extended_domain_model()</code> instead</li> <li>(pydantic-forms) Removed class <code>UniqueConstrainedList</code> -&gt; see UniqueConstrainedList</li> <li>(pydantic-forms) Removed class <code>ChoiceList</code> -&gt; use <code>choice_list()</code> instead</li> <li>(pydantic-forms) Removed class <code>ContactPersonList</code> -&gt; use <code>contact_person_list()</code> instead</li> <li>(pydantic-forms) Moved <code>ReadOnlyField</code> import (covered by migration-script)</li> <li>(pydantic-forms) Changed <code>ReadOnlyField</code> from a Field to an Annotated Literal -&gt; change your code from <code>field: int = ReadOnlyField(123)</code> to <code>field: ReadOnlyField(123)</code></li> </ul>"},{"location":"guides/upgrading/2.0/#subscriptioninstancelist-removed","title":"SubscriptionInstanceList removed","text":"<p>There are 2 different usecases of <code>SubscriptionInstanceList</code> that can be changed as follows.</p> <p>Note: this change is covered by the migration script.</p> <p>Note: you can no longer instantiate an annotated list type.</p> <p>I.e. before you could write <code>mylist = ListMax2()</code> but this is no longer valid. Instead, write <code>mylist: ListMax2 = []</code>.</p> <p>1. Generic List</p> <pre><code>class ListMax2(SubscriptionInstanceList[SI]):\n    max_items = 2\n</code></pre> <p>becomes</p> <pre><code>ListMax2 = Annotated[list[SI], Len(max_length=2)]\n</code></pre> <p>Which can then be used in a pydantic model, i.e. a product block, with a type subscription</p> <pre><code>class MainProductBlockInactive(ProductBlockModel):\n    values: ListMax2[SubProductBlockInactive]\n</code></pre> <p>When inheriting from this model you can change the type of the list, however a type: ignore comment is needed to silence errors from type checkers like mypy.</p> <pre><code>class MainProductBlockProvisioning(MyModel):\n    values: ListMax2[SubProductBlockProvisioning]  # type: ignore\n</code></pre> <p>2. Typed List</p> <pre><code>class ListMax2Numbers(SubscriptionInstanceList[int]):\n    max_items = 2\n</code></pre> <p>Becomes:</p> <pre><code>ListMax2Numbers = Annotated[list[int], Len(max_length=2)]\n</code></pre> <p>Which can then be used in a pydantic model without a type subscription</p> <pre><code>class MyModel(BaseModel):\n    values: ListMax2Numbers\n</code></pre>"},{"location":"guides/upgrading/2.0/#uniqueconstrainedlist","title":"UniqueConstrainedList","text":"<p>Class type has been removed. Replace with one of * <code>unique_conlist(T)</code> can be imported from pydantic-forms * <code>Annotated[list[T], AfterValidator(validate_unique_list)]</code> * <code>Annotated[set, ...]</code></p> <p>Before:</p> <pre><code>class ListOfTwo(UniqueConstrainedList[T]):\n    min_items = 2\n    max_items = 2\n</code></pre> <p>After:</p> <pre><code>ListOfTwo = Annotated[list[T], AfterValidator(validate_unique_list), Len(2, 2)]\n</code></pre>"},{"location":"guides/upgrading/2.0/#recommendations-and-examples","title":"Recommendations and examples","text":"<p>Set validators on the annotated type rather than on a FormPage/Model</p> <p>For example, instead of:</p> <pre><code>AddedServicePorts = conlist(\n    BgpServicePort, min_items=0, max_items=6 - len(current_service_ports)\n)\n\n\nclass ModifySN8IPForm(FormPage):\n    added_service_ports: AddedServicePorts\n\n    _validate_single_vlan: classmethod = validator(\n        \"added_service_ports\", allow_reuse=True\n    )(validate_single_vlan)\n    _validate_unique_vlans: classmethod = validator(\n        \"added_service_ports\", allow_reuse=True\n    )(validate_service_ports_unique_vlans)\n</code></pre> <p>Write:</p> <pre><code>AddedServicePorts = Annotated[\n    list[BgpServicePort],\n    Len(0, 6 - len(current_service_ports)),\n    AfterValidator(validate_single_vlan),\n    AfterValidator(validate_service_ports_unique_vlans),\n]\n\n\nclass ModifySN8IPForm(FormPage):\n    added_service_ports: AddedServicePorts\n</code></pre> <p>This encapsulates validation logic to the type, making it easier to reuse in other places.</p> <p>This also makes it possible to extend types, for example we could write the following to perform an extra validation on top of the existing ones.</p> <pre><code>AddedServicePorts_Extra = Annotated[AddedServicePorts, AfterValidator(validate_extra)]\n</code></pre> <p>(mypy) Define a TypeAlias for Choice fields</p> <p>A form using a <code>Choice</code> with dynamic values requires a <code># type: ignore</code> on the FormPage field to prevent mypy errors.</p> <p>Example in core v1:</p> <pre><code>from orchestrator.forms.validators import Choice\nfrom orchestrator.forms import FormPage\nfrom orchestrator.types import FormGenerator, SubscriptionLifecycle, UUIDstr\n\n\ndef node_selector(enum: str = \"NodesEnum\") -&gt; Choice:\n    node_subscriptions = subscriptions_by_product_type(\n        \"Node\", [SubscriptionLifecycle.ACTIVE]\n    )\n    nodes = {\n        str(subscription.subscription_id): subscription.description\n        for subscription in sorted(\n            node_subscriptions, key=lambda node: node.description\n        )\n    }\n    return Choice(enum, zip(nodes.keys(), nodes.items()))  # type:ignore\n\n\ndef initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class SelectNode(FormPage):\n        class Config:\n            title = f\"{product_name} - select node\"\n\n        node_subscription_id: node_selector()  # type:ignore # noqa: F821\n\n    select_node = yield SelectNode\n</code></pre> <p>The same code migrated to core v2, and with a <code>TypeAlias</code> to prevent the <code># type: ignore</code> on the FormPage field.</p> <pre><code>from pydantic import ConfigDict\n\nfrom orchestrator.types import SubscriptionLifecycle, UUIDstr\nfrom pydantic_forms.core import FormPage\nfrom pydantic_forms.validators import Choice\nfrom pydantic_forms.types import FormGenerator\n\n\ndef node_selector(enum: str = \"NodesEnum\") -&gt; type[Choice]:\n    node_subscriptions = subscriptions_by_product_type(\n        \"Node\", [SubscriptionLifecycle.ACTIVE]\n    )\n    nodes = {\n        str(subscription.subscription_id): subscription.description\n        for subscription in sorted(\n            node_subscriptions, key=lambda node: node.description\n        )\n    }\n    return Choice(enum, zip(nodes.keys(), nodes.items()))  # type:ignore\n\n\ndef initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    NodeChoice: TypeAlias = cast(type[Choice], node_selector())  # noqa: F821\n\n    class SelectNode(FormPage):\n        model_config = ConfigDict(title=f\"{product_name} - select node\")\n\n        node_subscription_id: NodeChoice\n\n    select_node = yield SelectNode\n</code></pre>"},{"location":"guides/upgrading/2.0/#what-now","title":"What now?","text":"<p>After following all of these tips your IDE may still show plenty of errors in your orchestrator.</p> <p>The best thing to do next is try to start your orchestrator and see which errors are critical to actually running it.</p> <p>After that, you can start running your testsuite. Pytest has (among many other things) the helpful option <code>--stepwise</code> that runs all your tests until the first failure. Rerunning the command will resume running tests from the last failed test. This will make it more manageable to start fixing the errors.</p> <p>Once that is done, your pytest or orchestrator output may still contain many warnings to look into regarding deprecated changes.</p> <p>If you are not sure how to proceed, don't hesitate to reach out through Slack or a Github Discussion.</p>"},{"location":"guides/upgrading/3.0/","title":"3.0 Migration Guide","text":"<p>In this document we describe the steps that should be taken to migrate from <code>orchestrator-core</code> v2 to v3.</p>"},{"location":"guides/upgrading/3.0/#about-30","title":"About 3.0","text":"<p>In this release, deprecated import statements from the <code>orchestrator.types</code> module are removed, that now come from <code>pydantic-forms.types</code> instead. These will have to be updated in your implementation of the orchestrator as well.</p>"},{"location":"guides/upgrading/3.0/#steps","title":"Steps","text":"<p>To update the import statements you may have in your implementation of Workflow Orchestrator, we offer a migration script that can be run as follows: <code>python -m orchestrator.devtools.scripts.migrate_30 &lt;dir&gt;</code> where <code>&lt;dir&gt;</code> points to your orchestrator implementation.</p>"},{"location":"guides/upgrading/4.0/","title":"4.0 Migration Guide","text":"<p>In this document we describe the steps that should be taken to migrate from <code>orchestrator-core</code> v3 to v4.</p>"},{"location":"guides/upgrading/4.0/#about-40","title":"About 4.0","text":""},{"location":"guides/upgrading/4.0/#removed-caching-of-domain-models","title":"Removed caching of domain models","text":"<p>In this release we have removed the caching of domain models. Domain models will always be loaded from the database.</p>"},{"location":"guides/upgrading/4.0/#added-targetvalidate","title":"Added Target.VALIDATE","text":"<p>In this release, a new workflow target, <code>VALIDATE</code>, has been added for validation workflows. Previously, the <code>SYSTEM</code> target was used for validation workflows, which implied that they were expected to run in a system context. However, this was not appropriate for all validation workflows. To address this, the new <code>VALIDATE</code> target has been introduced specifically for validation workflows. The <code>SYSTEM</code> target is now reserved exclusively for system workflows.</p> <p>The change of the <code>SYSTEM</code> target to <code>VALIDATE</code> is a breaking change, as it will break any workflows that are using the <code>SYSTEM</code> target for validation workflows. You will need to update your workflows to use the <code>VALIDATE</code> target instead.</p> <p>In the Steps section below we describe how to update your workflows to use the new <code>VALIDATE</code> target.</p>"},{"location":"guides/upgrading/4.0/#steps","title":"Steps","text":"<p>Follow these steps to upgrade:</p>"},{"location":"guides/upgrading/4.0/#caching-domain-models","title":"Caching domain models","text":"<p>To use 4.0.0, all workflows must have run to completion. The <code>cache_domain_models</code> step no longer is part of the codebase therefore <code>in flight</code> workflows will fail.</p>"},{"location":"guides/upgrading/4.0/#upgrading-to-use-the-validate-target","title":"Upgrading to use the VALIDATE target","text":"<p>After running the migration <code>(2025-05-08_cdf8758831d4_add_is_task_to_workflow.py)</code>, the workflow table should look like this:</p> <pre><code>| workflow_id                          | name                       | target | is_task | description                                                             | created_at                        | deleted_at |\n|--------------------------------------|----------------------------|--------|---------|-------------------------------------------------------------------------|-----------------------------------|------------|\n| ded79954-f16e-422b-a204-7770a59757e8 | modify_note                | MODIFY | FALSE   | Modify Note                                                             | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| ca6a76ff-dd4e-4f66-9fb0-cee1878f0d20 | task_clean_up_tasks        | SYSTEM | FALSE   | Clean up old tasks                                                      | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| 40058c3d-0c95-47f4-a75f-93719299c5be | task_resume_workflows      | SYSTEM | FALSE   | Resume all workflows that are stuck on tasks with the status 'waiting'  | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| 33b5520e-85d4-4ca1-8713-d26f7de5b7a5 | task_validate_products     | SYSTEM | FALSE   | Validate products                                                       | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| 94d4889e-6bb6-4724-a9d2-f21696fe6f43 | task_validate_product_type | SYSTEM | FALSE   | Validate all subscriptions of Product Type                              | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| 0c4f3b8d-2a1e-4b5f-9a7c-6d8e0f1b2c3d | validate_some_thing        | SYSTEM | FALSE   | Validate The thing                                                      | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n| f4b0a2c1-5d3e-4c8f-9b6d-7a2e5f3b8c4e | validate_another_thing     | SYSTEM | FALSE   | Validate Another thing                                                  | 2025-05-01 09:57:28.033504 +00:00 | &lt;null&gt;     |\n</code></pre> <p>The <code>target</code> in this table is no longer valid for the <code>validate_some_thing</code> and <code>validate_another_thing</code> workflows. You will need to update the <code>target</code> to <code>VALIDATE</code> for these workflows. You will also need to update the is_task column to <code>TRUE</code> for all targets that are <code>SYSTEM</code> or <code>VALIDATE</code>. This is because the <code>is_task</code> column is used to determine if a workflow is a task or not. If the <code>is_task</code> column is set to <code>FALSE</code>, the workflow will not be run as a task. Tasks are <code>SYSTEM</code> or <code>VALIDATE</code> workflows that are run in the context of a system.</p> <p>Example on how to update the <code>target</code> and <code>is_task</code> for all workflows that start with <code>validate_</code>:</p> <pre><code>UPDATE workflows\nSET target = 'VALIDATE', is_task = TRUE\nWHERE name LIKE 'validate_%';\n</code></pre> <p>Example on how to update the <code>target</code> and <code>is_task</code> for all workflows that are <code>SYSTEM</code> or <code>VALIDATE</code>:</p> <pre><code>UPDATE workflows\nSET is_task = TRUE\nWHERE target IN ('SYSTEM', 'VALIDATE');\n</code></pre> <p>This will update the <code>target</code> for all workflows that are <code>SYSTEM</code> or <code>VALIDATE</code> and set the <code>is_task</code> column to <code>TRUE</code>.</p> <p>This is a breaking change, so you will need to test your workflows after making this change to ensure that they are working as expected.</p>"},{"location":"guides/upgrading/4.7/","title":"v4.7","text":"<p>from orchestrator import begin</p>"},{"location":"guides/upgrading/4.7/#47-upgrade-guide","title":"4.7 Upgrade Guide","text":"<p>In this document we describe important changes to be aware of when upgrading from <code>orchestrator-core</code> v4.x to v4.7.</p> <p>If you are not yet using v4.x, please consult the 4.0 upgrade guide.</p>"},{"location":"guides/upgrading/4.7/#about-47","title":"About 4.7","text":"<p>This release introduces a deprecation and a breaking change to a beta feature:</p> <ol> <li>Scheduling tasks via decorator is deprecated in favor of using the API</li> <li>Workflow authorization callbacks (Beta) change from <code>def</code> to <code>async def</code></li> </ol>"},{"location":"guides/upgrading/4.7/#scheduling-tasks-via-decorator-is-deprecated-in-favor-of-using-the-api","title":"Scheduling tasks via decorator is deprecated in favor of using the API","text":"<p>In v4.4.0 we introduced the apscheduler library with a persistent jobstore. The decorator @scheduler.scheduled_job(...) allowed to schedule tasks from code.</p> <p>In v4.7.0 this has evolved to scheduling tasks via the Scheduler API. Since tasks scheduled from code cannot be related to their task definition in the DB, they cannot be managed through the API. Therefore, using the decorator to schedule tasks is deprecated. This release contains a DB migration which ensures you can update from any v4.x version to v4.7.0.</p> <p>Please check which of the 3 scenarios applies for you and which action is required.</p>"},{"location":"guides/upgrading/4.7/#1-i-use-the-scheduler-with-only-the-default-schedules","title":"1. I use the scheduler with only the default schedules","text":"<p>Short answer</p> <p>If you want to keep using the default schedules, run <code>python main.py scheduler load-initial-schedule</code> from the CLI where you normally run your scheduler.</p> <p>Otherwise, no action is required.</p> <p>Long answer</p> <p>As of v4.6.5, the default core schedules that are scheduled with the decorator were:</p> <ul> <li>clean-tasks: Runs the task <code>task_clean_up_tasks</code> every 6 hours</li> <li>resume-workflows: Runs the task <code>task_resume_workflows</code> task every hour</li> <li>validate-products: Runs the task <code>task_validate_products</code> every night (only if no previous run is incomplete)</li> <li>subscriptions-validator: Runs a validate workflow for each active subscription</li> </ul> <p>In v4.7.0 the only remaining schedule is <code>validate-products</code>. (this will be removed in #1250)</p> <p>The other 3 schedules have been removed. When you upgrade from anywhere between v4.4.0 - v4.6.x then the DB will still contain the scheduled jobs. You can remove them from the <code>apscheduler_jobs</code> table manually, or simply wait for the scheduler to clean them up, which will produce these 3 messages:</p> <pre><code>[error    ] Unable to restore job \"resume-workflows\" -- removing it [apscheduler.jobstores.default]\n[error    ] Unable to restore job \"clean-tasks\" -- removing it [apscheduler.jobstores.default]\n[error    ] Unable to restore job \"subscriptions-validator\" -- removing it [apscheduler.jobstores.default]\n</code></pre> <p>If you upgrade from an older v4.x version, this does not apply.</p> <p>If you want to recreate these 3 schedules by using the API, we have provided a command to do so:</p> <pre><code>python main.py scheduler load-initial-schedule\n</code></pre> <p>You can also choose to only schedule a specific task, or change when they run. For now this can only be done through the API; in the future this will be possible from the UI. (orchestrator-ui-library#2215)</p> <p>For more information: Scheduler API.</p>"},{"location":"guides/upgrading/4.7/#2-i-use-the-scheduler-with-both-the-default-schedules-and-my-own-schedules","title":"2. I use the scheduler with both the default schedules AND my own schedules","text":"<p>Short answer</p> <p>Check the actions for scenario 1.</p> <p>No further action is required as your own schedules will continue to work.</p> <p>Long answer</p> <p>Please check scenario 1 regarding the default schedules.</p> <p>For your own schedules, you may remove their definitions in code and schedule them using the [Schedule API]. However, this is not yet a requirement as the decorator-scheduled jobs can be used alongside API-scheduled jobs. This will only be a requirement for orchestrator-core v5.0.0.</p> <p>You can determine how jobs have been scheduled by running the show-schedule CLI command and inspecting the <code>source</code> column. For example, an excerpt of the SURF schedule:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 id                                   \u2503 name                      \u2503 source    \u2503 next run time             \u2503 trigger           \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 6db5c270-8239-455c-97e4-14a6a865c68d \u2502 Task Resume Workflows     \u2502 API       \u2502 2025-12-11 11:07:42+00:00 \u2502 interval[1:00:00] \u2502\n\u2502 import-crm-to-ims                    \u2502 Import CRM data to IMS    \u2502 decorator \u2502 2025-12-11 12:00:00+00:00 \u2502 cron              \u2502\n\u2502 70c0b5ce-88f5-48fb-ba5f-d19422628bef \u2502 Task Clean Up Tasks       \u2502 API       \u2502 2025-12-11 16:07:42+00:00 \u2502 interval[6:00:00] \u2502\n\u2502 import-customers-from-crm            \u2502 Import customers from CRM \u2502 decorator \u2502 2025-12-11 19:00:00+00:00 \u2502 cron              \u2502\n</code></pre>"},{"location":"guides/upgrading/4.7/#3-i-do-not-use-the-scheduler","title":"3. I do not use the scheduler","text":"<p>No action required!</p>"},{"location":"guides/upgrading/4.7/#workflow-authorization-callbacks-change-from-def-to-async-def","title":"Workflow authorization callbacks change from <code>def</code> to <code>async def</code>","text":"<p>In v4.7.0 the workflow authorization callbacks change from synchronous to asynchronous. This was done for consistency with API authorization callback functions which are also async. Additionally, it is always possible to run a sync function in an async context, but the other way around is much harder.</p> <p>While this is a breaking change, we decided to add it in this minor release since workflow authorization is currently still a beta feature.</p>"},{"location":"guides/upgrading/4.7/#example","title":"Example","text":"<p>Given a synchronous callback, like this:</p> <pre><code>from oauth2_lib.fastapi import OIDCUserModel\nfrom orchestrator.workflow import workflow, StepList, begin\n\ndef authorize_workflow_access(user: OIDCUserModel) -&gt; bool:\n    ...\n\n@workflow(\"My workflow\", authorize_callback=authorize_workflow_access)\ndef my_workflow() -&gt; StepList:\n    return begin &gt;&gt; ...\n</code></pre> <p>The required change is to make the callback look like this:</p> <pre><code>async def authorize_workflow_access(user: OIDCUserModel) -&gt; bool:\n    ...\n</code></pre> <p>Info</p> <p>Important: check your callback for synchronous I/O calls. These block the eventloop, which has a negative impact on performance of the API.</p> <p>For example, if your callback performs a <code>requests.get()</code> call or a sqlalchemy query, this will block the eventloop. You can resolve this either using an asynchronous alternative that can be <code>await</code>ed, or by using to_thread to run the synchronous call in a thread. Running in a thread is not as optimal as truly async code, but it's still a lot better than blocking the loop.</p> <p>For more information see Workflow authorization callbacks.</p>"},{"location":"reference-docs/api/","title":"API Documentation","text":"<p>The WFO has an API located at <code>/api/</code> and has browsable UI docs at <code>/api/redoc</code> OpenAPI spec available to download directly from your WFO instance. You can also view the swagger docs here, however, note that this is pulled directly from the example orchestrator demo site and might not exactly line up with this version of the documentation, so pay attention to the version shown below. You can also view these on the functioning demo WFO instance here!</p> <p></p>"},{"location":"reference-docs/auth-backend-and-frontend/","title":"Authentication and authorization","text":"<p>The <code>Orchestrator-Core</code> application incorporates a robust security framework, utilizing OpenID Connect (OIDC) for authentication and Open Policy Agent (OPA) for authorization. This flexible system ensures secure access, allowing you to tailor the authorization components to best fit your application's specific requirements.</p> <p>WFO can be run with or without authentication. With authentication turned on authorization logic can be provided that uses - for example - user privileges to allow further access to resources. Authentication is configured using ENV variables. The frontend and backend have their own set of ENV variables and logic to be implemented to run auth(n/z).</p> <p>Note: With authentication enabled on the backend the frontend has to have authentication enabled as well. When the frontend has authentication enabled it is possible to run a backend without authentication. Please note the limitations of frontend authentication and authorization mentioned in a note under frontend authentication.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#definitions","title":"Definitions","text":"<p>A frontend application refers to a web frontend based on the frontend example ui repository: frontend repo A backend application refers to an application build using the orchestrator core as a base: backend repo</p>"},{"location":"reference-docs/auth-backend-and-frontend/#without-authentication","title":"Without authentication","text":"<p>Without authentication WFO allows all users access to all resources.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#backend","title":"Backend","text":"<p><code>OAUTH2_ACTIVE=false</code></p>"},{"location":"reference-docs/auth-backend-and-frontend/#frontend","title":"Frontend:","text":"<p><code>OAUTH2_ACTIVE=false</code></p>"},{"location":"reference-docs/auth-backend-and-frontend/#with-authentication","title":"With Authentication","text":"<p>WFO provides authentication based on an OIDC provider. The OIDC provider is presumed to be configured and to provide</p> <ul> <li>An authentication endpoint</li> <li>A tenant</li> <li>A client id</li> <li>A client secret</li> </ul>"},{"location":"reference-docs/auth-backend-and-frontend/#frontend_1","title":"Frontend","text":"<p>The WFO frontend uses NextAuth to handle authentication. Authentication configuration can be found in page/api/auth/[...nextauth].ts</p> <p>ENV variables These variables need to be set for authentication to work on the frontend.</p> <pre><code># Auth variables\nOAUTH2_ACTIVE=true\nOAUTH2_CLIENT_ID=\"orchestrator-client\" // The oidc client id as configured in the OIDC provider\nOAUTH2_CLIENT_SECRET=[SECRET] // The oidc client secret id as configured in the OIDC provider\n\nNEXTAUTH_PROVIDER_ID=\"keycloak\" // String identifying the OIDC provider\nNEXTAUTH_PROVIDER_NAME=\"Keycloak\" // The name of the OIDC provider. Keycloak uses this name to display in the login screen\nNEXTAUTH_AUTHORIZATION_SCOPE_OVERRIDE=\"openid profile\" // Optional override of the scopes that are asked permission for from the OIDC provider\n\n# Required by the Nextauth middleware\nNEXTAUTH_URL=[DOMAIN]/api/auth // The path to the [...nextauth].js file\nNEXTAUTH_SECRET=[SECRET] // Used by NextAuth to encrypt the JWT token\n</code></pre> <p>With authentication turned on and these variables provided the frontend application will redirect unauthorized users to the login screen provided by the OIDC provider to request their credentials and return them to the page they tried to visit.</p> <p>Note: It's possible to add additional oidc providers including some that are provided by the NextAuth library like Google, Apple and others. See NextAuthProviders for more information.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#authorization","title":"Authorization","text":"<p>Authorization on the frontend can be used to determine if a page, action or navigation item is shown to a user. For this it uses an <code>isAllowedHandler</code> function can be passed into the WfoAuth component that wraps the page in <code>_app.tsx</code></p> <pre><code>...\n    &lt;WfoAuth isAllowedHandler={..custom function..}&gt;\n    ...\n    &lt;/WfoAuth&gt;\n...\n</code></pre> <p>The signature of the function should be <code>(routerPath: string, resource?: string) =&gt; boolean;</code>. The function is called on with the <code>routerpath</code> value and the <code>resource</code>. This is the list of events the function is called on is:</p> <pre><code>export enum PolicyResource {\n    NAVIGATION_METADATA = '/orchestrator/metadata/', // called when determining if the metadata menuitem should be shown\n    NAVIGATION_SETTINGS = '/orchestrator/settings/', // called when determining if the settings menuitem should be shown\n    NAVIGATION_SUBSCRIPTIONS = '/orchestrator/subscriptions/', // called when determining if the subscriptions should be shown\n    NAVIGATION_TASKS = '/orchestrator/tasks/', // called when determining if the tasks menuitem should be shown\n    NAVIGATION_WORKFLOWS = '/orchestrator/processes/', // called when determining if the processes menuitem should be shown\n    PROCESS_ABORT = '/orchestrator/processes/abort/', // called when determining if the button to trigger a process abort should be shown\n    PROCESS_DELETE = '/orchestrator/processes/delete/', // called when determining if the button to trigger a process delete button should be shown\n    PROCESS_DETAILS = '/orchestrator/processes/details/', // called when determining if the process detail page should be displayed\n    PROCESS_RELATED_SUBSCRIPTIONS = '/orchestrator/subscriptions/view/from-process', // called when determining if the related subscriptions for a subscription should be shown\n    PROCESS_RETRY = '/orchestrator/processes/retry/', // called when determining if the button to trigger a process retry should be shown\n    PROCESS_USER_INPUT = '/orchestrator/processes/user-input/', // called when determining if th\n    SUBSCRIPTION_CREATE = '/orchestrator/processes/create/process/menu', // called when determining if create if actions that trigger a create workflow should be displayed\n    SUBSCRIPTION_MODIFY = '/orchestrator/subscriptions/modify/', // called when determining if create if actions that trigger a modify workflow should be displayed\n    SUBSCRIPTION_TERMINATE = '/orchestrator/subscriptions/terminate/', // called when determining if create if actions that trigger a terminate workflow should be displayed\n    SUBSCRIPTION_VALIDATE = '/orchestrator/subscriptions/validate/', // called when determining if create if actions that trigger a validate task should be displayed\n    TASKS_CREATE = '/orchestrator/processes/create/task', // called when determining if create if actions that trigger a task should be displayed\n    TASKS_RETRY_ALL = '/orchestrator/processes/all-tasks/retry', // called when determining if create if actions that trigger retry all tasks task should be displayed\n    SETTINGS_FLUSH_CACHE = '/orchestrator/settings/flush-cache', // called when determining if a button to flush cache should be displayed\n    SET_IN_SYNC = '/orchestrator/subscriptions/set-in-sync', // called when determining if a button to set a subscription in sync should be displayed\n}\n</code></pre> <p>Note: Components that are hidden for unauthorized users are still part of the frontend application, authorization just makes sure unauthorized users are not presented with actions they are not allowed to take. The calls these actions make can still be made through curl calls for example. Additional authorization needs to be implemented on these calls on the backend.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#backend_1","title":"Backend","text":"<p>ENV variables These variables need to be set for authentication to work on the backend</p> <pre><code>...\n# OIDC settings\nOAUTH2_ACTIVE: bool = True\nOAUTH2_AUTHORIZATION_ACTIVE: bool = True\nOAUTH2_RESOURCE_SERVER_ID: str = \"\"\nOAUTH2_RESOURCE_SERVER_SECRET: str = \"\"\nOAUTH2_TOKEN_URL: str = \"\"\nOIDC_BASE_URL: str = \"\"\nOIDC_CONF_URL: str = \"\"\n\n# OPtional OPA settings\nOPA_URL: str = \"\"\n</code></pre> <p>With the variables provided, requests to endpoints will return 403 error codes for users that are not logged in and 401 error codes for users that are not authorized to do a call.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#customization","title":"Customization","text":"<p><code>AuthManager</code> serves as the central unit for managing both <code>authentication</code> and <code>authorization</code> mechanisms. While it defaults to using <code>OIDCAuth</code> for authentication, <code>OPAAuthorization</code> for http authorization and <code>GraphQLOPAAuthorization</code> for graphql authorization , it supports customization.</p> <p>When initiating the <code>OrchestratorCore</code> class, it's <code>auth_manager</code> property is set to <code>AuthManager</code>. AuthManager is provided by oauth2_lib.</p> <p><code>AuthManager</code> provides 3 methods that are called for authentication and authorization: <code>authentication</code>, <code>authentication</code> and <code>graphql_authorization</code>.</p> <p><code>authentication</code>: The default method provided by Oaut2Lib implements returning the OIDC user from the OIDC introspection endpoint.</p> <p><code>authorization</code>: A method that applies authorization decisions to HTTP requests, the decision is either true (Allowed) or false (Forbidden). Gets this payload to based decisions on. The default method provided by Oaut2Lib uses OPA and sends the payload to the opa_url specified in OPA_URL setting to get a decision.</p> <pre><code>            \"input\": {\n                **(self.opa_kwargs or {}),\n                **(user_info or {}),\n                \"resource\": request.url.path,\n                \"method\": request_method,\n                \"arguments\": {\"path\": request.path_params, \"query\": {**request.query_params}, \"json\": json},\n            }\n</code></pre> <p>Note: The default authentication method allows for the passing in of is_bypassable_request method that receives the Request object and returns a boolean. When this method returns true the request is always allowed regardless of other authorization decisions.</p> <p><code>graphql_authorization</code>: A method that applies authorization decisions to graphql requests. Specializes OPA authorization for GraphQL operations. GraphQl results always return a 200 response when authenticated but can return 403 results for partial results as may occur in federated scenarios.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#customizing","title":"Customizing","text":"<p>When initializing the app we have the option to register custom authentication and authorization methods and override the default auth(n|z) logic.</p> <pre><code>...\n    app.register_authentication(...)\n    app.register_authorization(...)\n    app.register_graphql_authorization(...)\n...\n</code></pre> <p>app.register_authentication takes an subclass of abstract class</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Authentication(ABC):\n    \"\"\"Abstract base for authentication mechanisms.\n\n    Requires an async authenticate method implementation.\n    \"\"\"\n\n    @abstractmethod\n    async def authenticate(self, request: HTTPConnection, token: str | None = None) -&gt; dict | None:\n        \"\"\"Authenticate the user.\"\"\"\n        pass\n</code></pre> <p>Authorization decisions can be made based on request properties and the token provided</p> <p>app.register_authorization takes an subclass of abstract class</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Authorization(ABC):\n    \"\"\"Defines the authorization logic interface.\n\n    Implementations must provide an async method to authorize based on request and user info.\n    \"\"\"\n\n    @abstractmethod\n    async def authorize(self, request: HTTPConnection, user: OIDCUserModel) -&gt; bool | None:\n        pass\n</code></pre> <p>Authorization decisions can be made based on request properties and user attributes</p> <p>app.register_graphql_authorization takes a subclass of abstract class</p> <pre><code>class GraphqlAuthorization(ABC):\n    \"\"\"Defines the graphql authorization logic interface.\n\n    Implementations must provide an async method to authorize based on request and user info.\n    \"\"\"\n\n    @abstractmethod\n    async def authorize(self, request: RequestPath, user: OIDCUserModel) -&gt; bool | None:\n        pass\n</code></pre> <p>Graphql Authorization decisions can be made based on request properties and user attributes.</p> <p>Additional methods exist for defining role-based access control for internal workflows.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#example","title":"Example","text":"<p>Below is an example illustrating how to override the default configurations:</p> <pre><code>from orchestrator import OrchestratorCore, app_settings\nfrom oauth2_lib.fastapi import OIDCAuth, OIDCUserModel, Authorization, RequestPath, GraphqlAuthorization\nfrom oauth2_lib.settings import oauth2lib_settings\nfrom httpx import AsyncClient\nfrom starlette.requests import HTTPConnection\nfrom typing import Optional\n\nclass CustomOIDCAuth(OIDCAuth):\n    async def userinfo(self, async_request: AsyncClient, token: str) -&gt; OIDCUserModel:\n        # Custom implementation to fetch user information\n        return OIDCUserModel(\n            sub=\"user-sub\",\n            email=\"example-user@company.org\",\n            # ...\n        )\n\nclass CustomAuthorization(Authorization):\n    async def authorize(self, request: HTTPConnection, user: OIDCUserModel) -&gt; Optional[bool]:\n        # Implement custom authorization logic\n        return True\n\nclass CustomGraphqlAuthorization(GraphqlAuthorization):\n    async def authorize(self, request: RequestPath, user: OIDCUserModel) -&gt; Optional[bool]:\n        # Implement custom GraphQL authorization logic\n        return True\n\noidc_instance = CustomOIDCAuth(\n    openid_url=oauth2lib_settings.OIDC_BASE_URL,\n    openid_config_url=oauth2lib_settings.OIDC_CONF_URL,\n    resource_server_id=oauth2lib_settings.OAUTH2_RESOURCE_SERVER_ID,\n    resource_server_secret=oauth2lib_settings.OAUTH2_RESOURCE_SERVER_SECRET,\n    oidc_user_model_cls=OIDCUserModel,\n)\n\nauthorization_instance = CustomAuthorization()\ngraphql_authorization_instance = CustomGraphqlAuthorization()\n\napp = OrchestratorCore(base_settings=app_settings)\napp.register_authentication(oidc_instance)\napp.register_authorization(authorization_instance)\napp.register_graphql_authorization(graphql_authorization_instance)\n</code></pre>"},{"location":"reference-docs/auth-backend-and-frontend/#authorization-and-workflows","title":"Authorization and Workflows","text":"<p>Warning</p> <p>Role-based access control for workflows is currently in beta. Initial support has been added to the backend, but the feature is not fully communicated through the UI yet.</p> <p>Certain <code>orchestrator-core</code> decorators accept authorization callbacks of type <code>type Authorizer = Callable[[OIDCUserModel | None], Awaitable[bool]]</code>, which return True when the input user is authorized, otherwise False. In other words, authorization callbacks are async, take a nullable OIDCUserModel (or subclass) as argument, and return a bool.</p> <p>A table (below) is available for comparing possible configuration states with the policy that will be enforced.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#workflow","title":"<code>@workflow</code>","text":"<p>The <code>@workflow</code> decorator accepts the optional parameters <code>authorize_callback: Authorizer</code> and <code>retry_auth_callback: Authorizer</code>.</p> <p><code>authorize_callback</code> will be used to determine the authorization of a user to start the workflow. If <code>authorize_callback</code> is omitted, the workflow is authorized for any logged in user.</p> <p><code>retry_auth_callback</code> will be used to determine the authorization of a user to start, resume, or retry the workflow from a failed step. If <code>retry_auth_callback</code> is omitted, then <code>authorize_callback</code> is used to authorize.</p> <p>(This does not percolate past an <code>@inputstep</code> that specifies <code>resume_auth_callback</code> or <code>retry_auth_callback</code>.)</p> <p>Examples:</p> <ul> <li><code>authorize_callback=None, retry_auth_callback=None</code>: any user may run the workflow.</li> <li><code>authorize_callback=A, retry_auth_callback=B</code>: users authorized by A may start the workflow. Users authorized by B may retry on failure.<ul> <li>Example: starting the workflow is a decision that must be made by a product owner. Retrying can be made by an on-call member of the operations team.</li> </ul> </li> <li><code>authorize_callback=None, retry_auth_callback=B</code>: any user can start the workflow, but only users authorized by B may retry on failure.</li> </ul>"},{"location":"reference-docs/auth-backend-and-frontend/#inputstep","title":"<code>@inputstep</code>","text":"<p>The <code>@inputstep</code> decorator accepts the optional parameters <code>resume_auth_callback: Authorizer</code> and <code>retry_auth_callback: Authorizer</code>.</p> <p><code>resume_auth_callback</code> will be used to determine the authorization of a user to resume the workflow when suspended at this inputstep. If <code>resume_auth_callback</code> is omitted, then the workflow's <code>authorize_callback</code> will be used.</p> <p><code>retry_auth_callback</code> will be used to determine the authorization of a user to retry the workflow from a failed step following the inputstep. If <code>retry_auth_callback</code> is omitted, then <code>resume_auth_callback</code> is used to authorize retries. If <code>resume_auth_callback</code> is also omitted, then the workflow\u2019s <code>retry_auth_callback</code> is checked, and then the workflow\u2019s <code>authorize_callback</code>.</p> <p>In summary:</p> <ul> <li>A workflow establishes <code>authorize_callback</code> for starting, resuming, or retrying.</li> <li>The workflow can also establish <code>retry_auth_callback</code>, which will override <code>authorize_callback</code> for retries.<ul> <li>An inputstep can override the existing <code>authorize_callback</code> with <code>resume_auth_callback</code> and the existing <code>retry_auth_callback</code> with its own <code>retry_auth_callback</code>.</li> </ul> </li> <li>Subsequent inputsteps can do the same, but any None will not overwrite a previous not-None.</li> </ul>"},{"location":"reference-docs/auth-backend-and-frontend/#policy-resolutions","title":"Policy resolutions","text":"<p>Below is an exhaustive table of how policies (implemented as callbacks <code>A</code>, <code>B</code>, <code>C</code>, and <code>D</code>) are prioritized in different workflow and inputstep configurations. For brevity, the <code>_callback</code> parameter suffix has been ommitted.</p> Configuration Enforcement Notes @workflow @inputstep before @inputstep @inputstep and after authorize retry_auth resume_auth retry_auth start retry resume retry None None None None Anyone Anyone Anyone Anyone Default A None None None A A A A Broadly restrict the workflow to a specific authorizer. None B None None Anyone B Anyone B original retry_auth is maintained if nothing supercedes it. Weird choice, but this provides a \"we specifically want to limit retries\" route. A B None None A B A B Workflow-level auth and retry. Allows A or B to be tighter or distinct, as needed. None None C None Anyone Anyone C C Anyone can start this workflow, but only C can continue it. A None C None A A C C Subsequent retries use C, not A! Override with retry_auth=A if desired. None B C None Anyone B C C Subsequent retries use C, not B! Override with retry_auth=B if desired. A B C None A B C C Simple override initial settings with inputstep resume_auth. None None None D Anyone Anyone Anyone D Anyone can start or retry or resume, but limit retries to D once inputstep is reached. A None None D A A A D A can start or retry or resume, but limit retries to D once inputstep is reached. None B None D Anyone B Anyone D Anyone can start or resume, but only B can retry. After inputstep, only D can retry. A B None D A B A D A can start or resume, but only B can retry. After inputstep, only D can retry. None None C D Anyone Anyone C D Anyone can start, but only C can resume and only D can retry after the resume. A None C D A A C D None B C D Anyone B C D A B C D A B C D"},{"location":"reference-docs/auth-backend-and-frontend/#authorization-for-internal-workflows","title":"Authorization for internal workflows","text":"<p>Users of Workflow Orchestrator can't directly access the <code>@workflow</code> decorators of tasks and workflows defined within <code>orchestrator-core</code>. However, authorization callbacks can still be passed via the <code>OrchestratorCore</code> class when initializing your WFO application.</p> <pre><code>from orchestrator import OrchestratorCore\n\napp = OrchestratorCore()\n\n# Let foo and bar be Authorizers\napp.register_internal_authorize_callback(foo)\napp.register_internal_retry_auth_callback(bar)\n</code></pre> <p>If these callbacks are not registered, these workflows can be started and retried by all users by default.</p> <p>For more on application startup, see the Settings Overview page.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#examples","title":"Examples","text":"<p>Assume we have the following function that can be used to create callbacks:</p> <pre><code>from oauth2_lib.fastapi import OIDCUserModel\nfrom orchestrator.workflows.utils import Authorizer\n\ndef allow_roles(*roles) -&gt; Authorizer:\n    async def f(user: OIDCUserModel) -&gt; bool:\n        if is_admin(user):  # Relative to your authorization provider\n            return True\n        for role in roles:\n            if has_role(user, role):  # Relative to your authorization provider\n                return True\n        return False\n\n    return f\n</code></pre> <p>We can now construct a variety of authorization policies.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#rubber-stamp-model","title":"Rubber Stamp Model","text":"<p>Example</p> <p>Suppose we have a workflow W that needs to pause on inputstep <code>approval</code> for approval from finance. Ops (and only ops) should be able to start the workflow and retry any failed steps. Finance (and only finance) should be able to resume at the input step.</p> <pre><code>@workflow(\"An expensive workflow\", authorize_callback=allow_roles(\"ops\"))\ndef W(...):\n    return begin &gt;&gt; A &gt;&gt; ... &gt;&gt; notify_finance &gt;&gt; approval &gt;&gt; ... &gt;&gt; Z\n\n@inputstep(\"Approval\", resume_auth_callback=allow_roles(\"finance\"), retry_auth_callback=allow_roles(\"ops\"))\ndef approval(...):\n    ...\n</code></pre>"},{"location":"reference-docs/auth-backend-and-frontend/#hand-off-model","title":"Hand-off Model","text":"<p>Example</p> <p>Suppose we have two teams, Dev and Platform, and a long workflow W that should be handed off to Platform at step <code>approval</code>.</p> <p>Dev can start the workflow and retry steps prior to S. Once step S is reached, Platform (and only Platform) can resume the workflow and retry later failed steps.</p> <p><pre><code>@workflow(\"An expensive workflow\", authorize_callback=allow_roles(\"dev\"))\ndef W(...):\n    return begin &gt;&gt; A &gt;&gt; ... &gt;&gt; notify_platform &gt;&gt; handoff &gt;&gt; ... &gt;&gt; Z\n\n@inputstep(\"Hand-off\", resume_auth_callback=allow_roles(\"platform\"))\ndef handoff(...):\n    ...\n</code></pre> Notice that default behaviors let us ignore <code>retry_auth_callback</code> arguments in both decorators.</p>"},{"location":"reference-docs/auth-backend-and-frontend/#restricted-retries-model","title":"Restricted Retries Model","text":"<p>Example</p> <p>Suppose we have a workflow that anyone can run, but with steps that should only be retried by users with certain backend access.</p> <pre><code>@workflow(\"A workflow for any user\", retry_auth_callback=allow_roles(\"admin\"))\ndef W(...):\n    return begin &gt;&gt; A &gt;&gt; ... &gt;&gt; S &gt;&gt; ... &gt;&gt; Z\n</code></pre> <p>Note that we could specify <code>authorize_callback=allow_roles(\"user\")</code> if helpful, or we can omit <code>authorize_callback</code> to fail over to any logged in user.</p>"},{"location":"reference-docs/cli/","title":"Command Line Interface Commands","text":"<p>This page documents CLI commands available in orchestrator-core.</p> <p>The syntax of a CLI command is:</p> <pre><code>python main.py &lt;command&gt; &lt;sub_command&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;command&gt;</code> is one of the top-level headings in this page</li> <li><code>&lt;sub_command&gt;</code> is one of the secondary headings in this page</li> </ul> <p>Some examples:</p> <pre><code>python main.py db migrate_tasks\n\npython main.py generate workflows\n\npython main.py scheduler show-schedule\n</code></pre> <p>Each command can also be run with <code>--help</code> to get information directly in the CLI.</p> <p>Top level options:</p> <p><code>--install-completion [bash|zsh|fish|powershell|pwsh]</code></p> <p>Install completion for the specified shell. [default: None]</p> <p><code>--show-completion [bash|zsh|fish|powershell|pwsh]</code></p> <p>Show completion for the specified shell, to copy it or customize the installation. [default: None]</p>"},{"location":"reference-docs/cli/#db","title":"db","text":"<p>Interact with the application database. By default, does nothing, specify <code>main.py db --help</code> for more information.</p>"},{"location":"reference-docs/cli/#orchestrator.cli.database.downgrade","title":"<code>downgrade</code>","text":"<p>The <code>downgrade</code> command will downgrade the database to the previous revision or to the optionally specified revision.</p> CLI Options <pre><code>Arguments:\n    [REVISION]  Rev id to upgrade to  [default: -1]\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command()\ndef downgrade(revision: str = typer.Argument(\"-1\", help=\"Rev id to downgrade to\")) -&gt; None:\n    \"\"\"The `downgrade` command will downgrade the database to the previous revision or to the optionally specified revision.\n\n    Args:\n        revision (str, optional): Optional argument to indicate where to downgrade to. [default: -1]\n\n    Returns:\n        None\n\n    CLI Options:\n        ```sh\n        Arguments:\n            [REVISION]  Rev id to upgrade to  [default: -1]\n        ```\n\n    \"\"\"\n    command.downgrade(alembic_cfg(), revision)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.heads","title":"<code>heads</code>","text":"<p>The <code>heads</code> command command shows the Alembic database heads.</p> CLI Options <p>None</p> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command(help=\"Get the database heads\")\ndef heads() -&gt; None:\n    \"\"\"The `heads` command command shows the Alembic database heads.\n\n    CLI Options:\n        None\n\n    \"\"\"\n    command.heads(alembic_cfg())\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.history","title":"<code>history</code>","text":"<p>The <code>history</code> command lists Alembic revision history/changeset scripts in chronological order.</p> CLI Options <pre><code>Options:\n    -v, --verbose  Verbose output\n    -c, --current  Indicate current revision\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command()\ndef history(\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    indicate_current: bool = typer.Option(True, \"--current\", \"-c\", help=\"Indicate current revision\"),\n) -&gt; None:\n    \"\"\"The `history` command lists Alembic revision history/changeset scripts in chronological order.\n\n    Args:\n        verbose (bool, optional): Verbose output\n        indicate_current (bool, optional): Indicate current revision\n\n    Returns:\n        None\n\n    CLI Options:\n        ```sh\n        Options:\n            -v, --verbose  Verbose output\n            -c, --current  Indicate current revision\n        ```\n    \"\"\"\n    command.history(alembic_cfg(), verbose=verbose, indicate_current=indicate_current)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.init","title":"<code>init</code>","text":"<p>Initialize the <code>migrations</code> directory.</p> <p>This command will initialize a migration directory for the orchestrator core application and setup a correct migration environment. It will also throw an exception when it detects conflicting files and directories.</p> CLI Options <p>None</p> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command(\n    help=\"Initialize an empty migrations environment. This command will throw an exception when it detects conflicting files and directories.\"\n)\ndef init() -&gt; None:\n    \"\"\"Initialize the `migrations` directory.\n\n    This command will initialize a migration directory for the orchestrator core application and setup a correct\n    migration environment. It will also throw an exception when it detects conflicting files and directories.\n\n    Returns:\n        None\n\n    CLI Options:\n        None\n\n    \"\"\"\n\n    if os.access(migration_dir, os.F_OK) and os.listdir(migration_dir):\n        raise OSError(f\"Directory {migration_dir} already exists and is not empty\")\n\n    logger.info(\"Creating directory\", directory=os.path.abspath(migration_dir))\n    os.makedirs(migration_dir)\n    versions = os.path.join(migration_dir, \"versions\")\n    logger.info(\"Creating directory\", directory=os.path.abspath(versions))\n    os.makedirs(versions)\n    versions_schema = os.path.join(migration_dir, \"versions/schema\")\n    logger.info(\"Creating directory\", directory=os.path.abspath(versions_schema))\n    os.makedirs(versions_schema)\n\n    source_env_py = os.path.join(orchestrator_module_location, f\"{migration_dir}/templates/env.py.j2\")\n    env_py = os.path.join(migration_dir, \"env.py\")\n    logger.info(\"Creating file\", file=os.path.abspath(env_py))\n    copyfile(source_env_py, env_py)\n\n    source_script_py_mako = os.path.join(orchestrator_module_location, f\"{migration_dir}/script.py.mako\")\n    script_py_mako = os.path.join(migration_dir, \"script.py.mako\")\n    logger.info(\"Creating file\", file=os.path.abspath(script_py_mako))\n    copyfile(source_script_py_mako, script_py_mako)\n\n    source_helpers_py = os.path.join(orchestrator_module_location, f\"{migration_dir}/templates/helpers.py.j2\")\n    helpers_py = os.path.join(migration_dir, \"helpers.py\")\n    logger.info(\"Creating file\", file=os.path.abspath(helpers_py))\n    copyfile(source_helpers_py, helpers_py)\n\n    template = jinja_env.get_template(\"alembic.ini.j2\")\n\n    if not os.access(os.path.join(os.getcwd(), \"alembic.ini\"), os.F_OK):\n        logger.info(\"Creating file\", file=os.path.join(os.getcwd(), \"alembic.ini\"))\n        with open(os.path.join(os.getcwd(), \"alembic.ini\"), \"w\") as alembic_ini:\n            alembic_ini.write(template.render(migrations_dir=migration_dir))\n    else:\n        logger.info(\"Skipping Alembic.ini file. It already exists\")\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.merge","title":"<code>merge</code>","text":"<p>Merge database revisions.</p> <p>It is possible when using multiple git branches in your WFO development lifecycle to have multiple Alembic heads emerge. This command will allow you to merge those two (or more) heads to resolve the issue. You also might need to run this after updating your version of orchestrator-core if there have been schema changes.</p> <p>Read More Here</p> CLI Options <pre><code>Arguments:\n    [REVISIONS]  Add the revision you would like to merge to this command.\n\nOptions:\n    -m, --message TEXT  The revision message\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command(help=\"Merge database revisions.\")\ndef merge(\n    revisions: str = typer.Argument(default=None, help=\"Add the revision you would like to merge to this command.\"),\n    message: str = typer.Option(None, \"--message\", \"-m\", help=\"The revision message\"),\n) -&gt; None:\n    \"\"\"Merge database revisions.\n\n    It is possible when using multiple git branches in your WFO development lifecycle to have\n    multiple Alembic heads emerge. This command will allow you to merge those two (or more)\n    heads to resolve the issue. You also might need to run this after updating your version\n    of orchestrator-core if there have been schema changes.\n\n    [Read More Here](https://alembic.sqlalchemy.org/en/latest/branches.html#merging-branches)\n\n    Args:\n        revisions: List of revisions to merge\n        message: Optional message for the revision.\n\n    Returns:\n        None\n\n    CLI Options:\n        ```sh\n        Arguments:\n            [REVISIONS]  Add the revision you would like to merge to this command.\n\n        Options:\n            -m, --message TEXT  The revision message\n        ```\n    \"\"\"\n    command.merge(alembic_cfg(), revisions, message=message)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.migrate_workflows","title":"<code>migrate_workflows</code>","text":"<p>The <code>migrate-workflows</code> command creates a migration file based on the difference between workflows in the database and registered WorkflowInstances in your codebase.</p> <p>BACKUP YOUR DATABASE BEFORE USING THE MIGRATION!</p> <p>You will be prompted with inputs for new models and resource type updates. Resource type updates are only handled when it's renamed in all product blocks.</p> <p>Returns None unless <code>--test</code> is used, in which case it returns:     - tuple:         - list of upgrade SQL statements in string format.         - list of downgrade SQL statements in string format.</p> CLI Arguments <pre><code>Arguments:\n    MESSAGE  Migration name  [required]\n\nOptions:\n    --test / --no-test  Optional boolean if you don't want to generate a migration\n    file  [default: no-test]\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command(help=\"Create migration file based on diff workflows in db\")\ndef migrate_workflows(\n    message: str = typer.Argument(..., help=\"Migration name\"),\n    test: bool = typer.Option(False, help=\"Optional boolean if you don't want to generate a migration file\"),\n) -&gt; tuple[list[dict], list[dict]] | None:\n    \"\"\"The `migrate-workflows` command creates a migration file based on the difference between workflows in the database and registered WorkflowInstances in your codebase.\n\n    !!! warning \"BACKUP YOUR DATABASE BEFORE USING THE MIGRATION!\"\n\n    You will be prompted with inputs for new models and resource type updates.\n    Resource type updates are only handled when it's renamed in all product blocks.\n\n    Args:\n        message: Message/description of the generated migration.\n        test: Optional boolean if you don't want to generate a migration file.\n\n    Returns None unless `--test` is used, in which case it returns:\n        - tuple:\n            - list of upgrade SQL statements in string format.\n            - list of downgrade SQL statements in string format.\n\n    CLI Arguments:\n        ```sh\n        Arguments:\n            MESSAGE  Migration name  [required]\n\n        Options:\n            --test / --no-test  Optional boolean if you don't want to generate a migration\n            file  [default: no-test]\n        ```\n    \"\"\"\n    if not app_settings.TESTING:\n        init_database(app_settings)\n\n    if test:\n        print(  # noqa: T001, T201\n            f\"{str_fmt('NOTE:', flags=[COLOR.BOLD, COLOR.CYAN])} Running in test mode. No migration file will be generated.\\n\"\n        )\n\n    workflows_to_add, workflows_to_delete = create_workflows_migration_wizard()\n\n    # String 'template' arguments\n    import_str = \"from orchestrator.migrations.helpers import create_workflow, delete_workflow\\n\"\n    tpl_preamble_lines = []\n    tpl_upgrade_lines = []\n    tpl_downgrade_lines = []\n\n    if workflows_to_add:\n        tpl_preamble_lines.append(f\"new_workflows = {json.dumps(workflows_to_add, indent=4)}\\n\")\n        tpl_upgrade_lines.extend(\n            [(\" \" * 4) + \"for workflow in new_workflows:\", (\" \" * 8) + \"create_workflow(conn, workflow)\"]\n        )\n        tpl_downgrade_lines.extend(\n            [(\" \" * 4) + \"for workflow in new_workflows:\", (\" \" * 8) + 'delete_workflow(conn, workflow[\"name\"])']\n        )\n\n    if workflows_to_delete:\n        tpl_preamble_lines.append(f\"old_workflows = {json.dumps(workflows_to_delete, indent=4)}\\n\")\n        tpl_upgrade_lines.extend(\n            [(\" \" * 4) + \"for workflow in old_workflows:\", (\" \" * 8) + 'delete_workflow(conn, workflow[\"name\"])']\n        )\n        tpl_downgrade_lines.extend(\n            [(\" \" * 4) + \"for workflow in old_workflows:\", (\" \" * 8) + \"create_workflow(conn, workflow)\"]\n        )\n\n    preamble = \"\\n\".join(\n        [\n            import_str,\n            *tpl_preamble_lines,\n        ]\n    )\n    sql_upgrade_str = \"\\n\".join(tpl_upgrade_lines)\n    sql_downgrade_str = \"\\n\".join(tpl_downgrade_lines)\n\n    if test:\n        return workflows_to_add, workflows_to_delete\n\n    create_migration_file(alembic_cfg(), sql_upgrade_str, sql_downgrade_str, message, preamble=preamble)\n    return None\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.migrate_tasks","title":"<code>migrate_tasks</code>","text":"<p>The <code>migrate-tasks</code> command creates a migration file based on the difference between tasks in the database and registered TaskInstances in your codebase.</p> <p>BACKUP YOUR DATABASE BEFORE USING THE MIGRATION!</p> <p>You will be prompted with inputs for new models and resource type updates. Resource type updates are only handled when it's renamed in all product blocks.</p> <p>Returns None unless <code>--test</code> is used, in which case it returns:     - tuple:         - list of upgrade SQL statements in string format.         - list of downgrade SQL statements in string format.</p> CLI Arguments <pre><code>Arguments:\n    MESSAGE  Migration name  [required]\n\nOptions:\n    --test / --no-test  Optional boolean if you don't want to generate a migration\n    file  [default: no-test]\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command(help=\"Create migration file based on diff tasks in db\")\ndef migrate_tasks(\n    message: str = typer.Argument(..., help=\"Migration name\"),\n    test: bool = typer.Option(False, help=\"Optional boolean if you don't want to generate a migration file\"),\n) -&gt; tuple[list[dict], list[dict]] | None:\n    \"\"\"The `migrate-tasks` command creates a migration file based on the difference between tasks in the database and registered TaskInstances in your codebase.\n\n    !!! warning \"BACKUP YOUR DATABASE BEFORE USING THE MIGRATION!\"\n\n    You will be prompted with inputs for new models and resource type updates.\n    Resource type updates are only handled when it's renamed in all product blocks.\n\n    Args:\n        message: Message/description of the generated migration.\n        test: Optional boolean if you don't want to generate a migration file.\n\n    Returns None unless `--test` is used, in which case it returns:\n        - tuple:\n            - list of upgrade SQL statements in string format.\n            - list of downgrade SQL statements in string format.\n\n    CLI Arguments:\n        ```sh\n        Arguments:\n            MESSAGE  Migration name  [required]\n\n        Options:\n            --test / --no-test  Optional boolean if you don't want to generate a migration\n            file  [default: no-test]\n        ```\n    \"\"\"\n    if not app_settings.TESTING:\n        init_database(app_settings)\n\n    if test:\n        print(  # noqa: T001, T201\n            f\"{str_fmt('NOTE:', flags=[COLOR.BOLD, COLOR.CYAN])} Running in test mode. No migration file will be generated.\\n\"\n        )\n\n    tasks_to_add, tasks_to_delete = create_tasks_migration_wizard()\n\n    # String 'template' arguments\n    import_str = \"from orchestrator.migrations.helpers import create_task, delete_workflow\\n\"\n    tpl_preamble_lines = []\n    tpl_upgrade_lines = []\n    tpl_downgrade_lines = []\n\n    if tasks_to_add:\n        tpl_preamble_lines.append(f\"new_tasks = {json.dumps(tasks_to_add, indent=4)}\\n\")\n        tpl_upgrade_lines.extend([(\" \" * 4) + \"for task in new_tasks:\", (\" \" * 8) + \"create_task(conn, task)\"])\n        tpl_downgrade_lines.extend(\n            [(\" \" * 4) + \"for task in new_tasks:\", (\" \" * 8) + 'delete_workflow(conn, task[\"name\"])']\n        )\n\n    if tasks_to_delete:\n        tpl_preamble_lines.append(f\"old_tasks = {json.dumps(tasks_to_delete, indent=4)}\\n\")\n        tpl_upgrade_lines.extend(\n            [(\" \" * 4) + \"for task in old_tasks:\", (\" \" * 8) + 'delete_workflow(conn, task[\"name\"])']\n        )\n        tpl_downgrade_lines.extend([(\" \" * 4) + \"for task in old_tasks:\", (\" \" * 8) + \"create_task(conn, task)\"])\n\n    preamble = \"\\n\".join(\n        [\n            import_str,\n            *tpl_preamble_lines,\n        ]\n    )\n    sql_upgrade_str = \"\\n\".join(tpl_upgrade_lines)\n    sql_downgrade_str = \"\\n\".join(tpl_downgrade_lines)\n\n    if test:\n        return tasks_to_add, tasks_to_delete\n\n    create_migration_file(alembic_cfg(), sql_upgrade_str, sql_downgrade_str, message, preamble=preamble)\n    return None\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.revision","title":"<code>revision</code>","text":"<p>The <code>revision</code> command creates a new Alembic revision file.</p> CLI Options <pre><code>Options:\n    -m, --message TEXT              The revision message\n    --version-path TEXT             Specify specific path from config for version file\n    --autogenerate / --no-autogenerate\n                                    Detect schema changes and add migrations [default: no-autogenerate]\n    --head TEXT                     Determine the head you need to add your migration to.\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command()\ndef revision(\n    message: str = typer.Option(None, \"--message\", \"-m\", help=\"The revision message\"),\n    version_path: str = typer.Option(None, \"--version-path\", help=\"Specify specific path from config for version file\"),\n    autogenerate: bool = typer.Option(False, help=\"Detect schema changes and add migrations\"),\n    head: str = typer.Option(\"data@head\", help=\"Determine the head you need to add your migration to.\"),\n) -&gt; None:\n    \"\"\"The `revision` command creates a new Alembic revision file.\n\n    Args:\n        message: The revision message\n        version_path: Specify specific path from config for version file\n        autogenerate: Whether to detect schema changes.\n        head: To which head the migration applies\n\n    Returns:\n        None\n\n    CLI Options:\n        ```sh\n        Options:\n            -m, --message TEXT              The revision message\n            --version-path TEXT             Specify specific path from config for version file\n            --autogenerate / --no-autogenerate\n                                            Detect schema changes and add migrations [default: no-autogenerate]\n            --head TEXT                     Determine the head you need to add your migration to.\n        ```\n    \"\"\"\n    create_data_head_if_not_exists({\"writer\": create_writer(), \"environment\": get_template_environment()})\n    command.revision(alembic_cfg(), message, version_path=version_path, autogenerate=autogenerate, head=head)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.database.upgrade","title":"<code>upgrade</code>","text":"<p>The <code>upgrade</code> command will upgrade the database to the specified revision.</p> CLI Options <pre><code>Arguments:\n    [REVISION]  Rev id to upgrade to\n\nOptions:\n    --help  Show this message and exit.\n</code></pre> Source code in <code>orchestrator/cli/database.py</code> <pre><code>@app.command()\ndef upgrade(revision: str = typer.Argument(help=\"Rev id to upgrade to\")) -&gt; None:\n    \"\"\"The `upgrade` command will upgrade the database to the specified revision.\n\n    Args:\n        revision: Optional argument to indicate where to upgrade to.\n\n    Returns:\n        None\n\n    CLI Options:\n        ```sh\n        Arguments:\n            [REVISION]  Rev id to upgrade to\n\n        Options:\n            --help  Show this message and exit.\n        ```\n\n    \"\"\"\n    command.upgrade(alembic_cfg(), revision)\n</code></pre>"},{"location":"reference-docs/cli/#migrate-domain-models","title":"migrate-domain-models","text":"<p>The <code>main.py db migrate-domain-models</code> command creates a revision based on the difference between domain models in the source code and those that are defined in the database.</p> <p>Arguments</p> <p>message - Migration name [default: None][required]</p> <p>Options</p> <p>--test | --no-test - Optional boolean if you don't want to generate a migration file [default: no-test] --inputs -  stringified dict to prefill inputs [default: {}] --updates - stringified dict to map updates instead of using inputs [default: {}]</p> <p>The <code>python main.py db migrate-domain-model</code> CLI command is used to automatically generate the data migrations that you'll need when you add or change a Domain Model.  It will inspect your DB and the existing domain models, analyse the differences and it will generate an Alembic data migration in the correct folder.</p> <p>Features:</p> <ul> <li>detect a new Domain Model attribute / resource type</li> <li>detect a renamed Domain Model attribute / resource type</li> <li>detect a removed Domain Model attribute / resource type</li> <li>detect a new Domain Model</li> <li>detect a removed Domain Model</li> <li>ability to ask for human input when needed</li> </ul> <p>Below in the documentation these features are discussed in more detail.</p> <p>BACKUP DATABASE BEFORE USING THE MIGRATION!</p> <p>Arguments</p> <ul> <li><code>message</code>: Message/description of the generated migration.</li> <li><code>--test</code>: Optional boolean if you don't want to generate a migration file.</li> <li><code>--inputs</code>: stringified dict to prefill inputs.     The inputs and updates argument is mostly used for testing, prefilling the given inputs, here examples:<ul> <li>new product: <code>inputs = { \"new_product_name\": { \"description\": \"add description\", \"product_type\": \"add_type\", \"tag\": \"add_tag\" }}</code></li> <li>new product fixed input: <code>inputs = { \"new_product_name\": { \"new_fixed_input_name\": \"value\" }}</code></li> <li>new product block: <code>inputs = { \"new_product_block_name\": { \"description\": \"add description\", \"tag\": \"add_tag\" } }</code></li> <li>new resource type: <code>inputs = { \"new_resource_type_name\": { \"description\": \"add description\", \"value\": \"add default value\", \"new_product_block_name\": \"add default value for block\" }}</code><ul> <li><code>new_product_block_name</code> prop inserts value specifically for that block.</li> <li><code>value</code> prop is inserted as default for all existing instances it is added to.</li> </ul> </li> </ul> </li> <li><code>--updates</code>: stringified dict to prefill inputs.<ul> <li>renaming a fixed input:<ul> <li><code>updates = { \"fixed_inputs\": { \"product_name\": { \"old_fixed_input_name\": \"new_fixed_input_name\" } } }</code></li> </ul> </li> <li>renaming a resource type to a new resource type:<ul> <li><code>inputs = { \"new_resource_type_name\": { \"description\": \"add description\" }}</code></li> <li><code>updates = { \"resource_types\": { \"old_resource_type_name\": \"new_resource_type_name\" } }</code></li> </ul> </li> <li>renaming a resource type to existing resource type: <code>updates = { \"resource_types\": { \"old_resource_type_name\": \"new_resource_type_name\" } }</code></li> </ul> </li> </ul> <p>Example</p> <p>You need products in the <code>SUBSCRIPTION_MODEL_REGISTRY</code>, for this example I will use these models (taken out of example-orchestrator):</p> <ul> <li> <p>UserGroup Block:     <pre><code>from orchestrator.domain.base import SubscriptionModel, ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\n\nclass UserGroupBlockInactive(\n    ProductBlockModel,\n    lifecycle=[SubscriptionLifecycle.INITIAL],\n    product_block_name=\"UserGroupBlock\",\n):\n    group_name: str | None = None\n    group_id: int | None = None\n\n\nclass UserGroupBlockProvisioning(\n    UserGroupBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    group_name: str\n    group_id: int | None = None\n\n\nclass UserGroupBlock(\n    UserGroupBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]\n):\n    group_name: str\n    group_id: int\n</code></pre></p> </li> <li> <p>UserGroup Product:     <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n\nclass UserGroupInactive(\n    SubscriptionModel, is_base=True, lifecycle=[SubscriptionLifecycle.INITIAL]\n):\n    settings: UserGroupBlockInactive\n\n\nclass UserGroupProvisioning(\n    UserGroupInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    settings: UserGroupBlockProvisioning\n\n\nclass UserGroup(UserGroupProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    settings: UserGroupBlock\n</code></pre></p> </li> <li> <p>User Block:     <pre><code>from orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n\nclass UserBlockInactive(\n    ProductBlockModel,\n    lifecycle=[SubscriptionLifecycle.INITIAL],\n    product_block_name=\"UserBlock\",\n):\n    group: UserGroupBlockInactive\n    username: str | None = None\n    age: int | None = None\n    user_id: int | None = None\n\n\nclass UserBlockProvisioning(\n    UserBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    group: UserGroupBlockProvisioning\n    username: str\n    age: int | None = None\n    user_id: int | None = None\n\n\nclass UserBlock(UserBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    group: UserGroupBlock\n    username: str\n    age: int | None = None\n    user_id: int\n</code></pre></p> </li> <li> <p>User Product:     <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle, strEnum\n\nfrom products.product_blocks.user import (\n    UserBlock,\n    UserBlockInactive,\n    UserBlockProvisioning,\n)\n\n\nclass Affiliation(strEnum):\n    internal = \"internal\"\n    external = \"external\"\n\n\nclass UserInactive(SubscriptionModel, is_base=True):\n    affiliation: Affiliation\n    settings: UserBlockInactive\n\n\nclass UserProvisioning(UserInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    affiliation: Affiliation\n    settings: UserBlockProvisioning\n\n\nclass User(UserProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    affiliation: Affiliation\n    settings: UserBlock\n</code></pre></p> </li> <li> <p><code>SUBSCRIPTION_MODEL_REGISTRY</code>:     <pre><code>from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\n\n# Register models to actual definitions for deserialization purposes\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"User group\": UserGroup,\n        \"User internal\": User,\n        \"User external\": User,\n    }\n)\n</code></pre></p> </li> </ul> <p>Running the command:</p> <ul> <li> <p>only with a message     <pre><code>python main.py db migrate-domain-models \"message\"\n</code></pre></p> </li> <li> <p>Running it as test     <pre><code>python main.py db migrate-domain-models \"message\" --test\n</code></pre></p> </li> <li> <p>Running the command with inputs prefilled     <pre><code>python main.py db migrate-domain-models \"message\" --inputs \"{ \"\" }\"\n</code></pre></p> </li> </ul> <p>The command will first go through all products and map the differences with the database. debug log example: <pre><code>2022-10-27 11:45:10 [debug] ProductTable blocks diff [orchestrator.domain.base] fixed_inputs_in_db=set() fixed_inputs_model=set() missing_fixed_inputs_in_db=set() missing_fixed_inputs_in_model=set() missing_product_blocks_in_db=set() missing_product_blocks_in_model=set() product_block_db=User group product_blocks_in_db={'UserGroupBlock'} product_blocks_in_model={'UserGroupBlock'}\n</code></pre></p> <p>You will be prompted with inputs when updates are found.</p> <ul> <li> <p>rename of resource type input (renaming <code>age</code> to <code>user_age</code> in User Block). Only works when the resource type is renamed in all Blocks:</p> <p>Update resource types Do you wish to rename resource type age to user_age? [y/N]:</p> </li> <li> <p>rename of fixed input (renaming <code>affiliation</code> to <code>affiliationing</code> in User Product):</p> <p>Update fixed inputs Do you wish to rename fixed input affiliation to affiliationing for product User internal? [y/N]:</p> </li> <li> <p>update of resource type per block (renaming <code>age</code> to <code>user_age</code> in User Block and not chosing to rename resource type). The input will loop until skipped or when there are no options anymore:</p> <ul> <li>first you get to choose which old resource type to update, skip will create/delete all resource types.     &gt; Update block resource types <pre><code>Which resource type would you want to update in UserBlock Block?\n1) age\nq) skip\n?\n</code></pre></li> <li>then you get to choose which new resource type to update with, skip will give you the first question again.     <pre><code>Which resource type should update age?\n1) user_age\nq) skip\n?\n</code></pre></li> <li>with 1 and 1, the log level difference would look like:     <pre><code>2023-02-08 14:11:25 [info] update_block_resource_types [orchestrator.cli.migrate_domain_models] update_block_resource_types={'UserBlock': {'age': 'user_age'}}\n</code></pre></li> </ul> </li> </ul> <p>It will log the differences on info level:</p> <pre><code>2022-10-27 11:45:10 [info] create_products                   [orchestrator.cli.migrate_domain_models] create_products={'User group': &lt;class 'products.product_types.user_group.UserGroup'&gt;, 'User internal': &lt;class 'products.product_types.user.User'&gt;, 'User external': &lt;class 'products.product_types.user.User'&gt;}\n2022-10-27 11:45:10 [info] delete_products                   [orchestrator.cli.migrate_domain_models] delete_products=set()\n2022-10-27 11:45:10 [info] create_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] create_product_fixed_inputs={'affiliation': {'User external', 'User internal'}}\n2022-10-27 11:45:10 [info] update_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] update_product_fixed_inputs={}\n2022-10-27 11:45:10 [info] delete_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] delete_product_fixed_inputs={}\n2022-10-27 11:45:10 [info] create_product_to_block_relations [orchestrator.cli.migrate_domain_models] create_product_to_block_relations={'UserGroupBlock': {'User group'}, 'UserBlock': {'User external', 'User internal'}}\n2022-10-27 11:45:10 [info] delete_product_to_block_relations [orchestrator.cli.migrate_domain_models] delete_product_to_block_relations={}\n2022-10-27 11:45:10 [info] create_resource_types             [orchestrator.cli.migrate_domain_models] create_resource_types={'username', 'age', 'group_name', 'user_id', 'group_id'}\n2022-10-27 11:45:10 [info] rename_resource_types             [orchestrator.cli.migrate_domain_models] rename_resource_types={}\n2022-10-27 11:45:10 [info] delete_resource_types             [orchestrator.cli.migrate_domain_models] delete_resource_types=set()\n2022-10-27 11:45:10 [info] create_resource_type_relations    [orchestrator.cli.migrate_domain_models] create_resource_type_relations={'group_name': {'UserGroupBlock'}, 'group_id': {'UserGroupBlock'}, 'username': {'UserBlock'}, 'age': {'UserBlock'}, 'user_id': {'UserBlock'}}\n2022-10-27 11:45:10 [info] delete_resource_type_relations    [orchestrator.cli.migrate_domain_models] delete_resource_type_relations={}\n2022-10-27 11:45:10 [info] create_product_blocks             [orchestrator.cli.migrate_domain_models] create_blocks={'UserGroupBlock': &lt;class 'products.product_blocks.user_group.UserGroupBlock'&gt;, 'UserBlock': &lt;class 'products.product_blocks.user.UserBlock'&gt;}\n2022-10-27 11:45:10 [info] delete_product_blocks             [orchestrator.cli.migrate_domain_models] delete_blocks=set()\n2022-10-27 11:45:10 [info] create_product_block_relations    [orchestrator.cli.migrate_domain_models] create_product_block_relations={'UserGroupBlock': {'UserBlock'}}\n2022-10-27 11:45:10 [info] delete_product_block_relations    [orchestrator.cli.migrate_domain_models] delete_product_block_relations={}\n</code></pre> <p>You will be asked to confirm the actions in order to continue:</p> <p>WARNING: Deleting products will also delete its subscriptions. Confirm the above actions [y/N]:</p> <p>After confirming, it will start generating the SQL, logging the SQL on debug level and prompt the user for new resources:</p> <ul> <li> <p>new product example:</p> <p>Create new products Product: UserGroup User group Supply the production description: User group product Supply the product tag: GROUP <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO products (name, description, product_type, tag, status) VALUES ('User group', 'User group product', 'UserGroup', 'GROUP', 'active') RETURNING products.product_id\n</code></pre></p> </li> <li> <p>new fixed input (the type isn't checked, so typing an incorrect value will insert in db):</p> <p>Create fixed inputs Supply fixed input value for product User internal and fixed input affiliation: internal Supply fixed input value for product User external and fixed input affiliation: external <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO fixed_inputs (name, value, product_id) VALUES ('affiliation', 'internal', (SELECT products.product_id FROM products WHERE products.name IN ('User internal'))), ('affiliation', 'external', (SELECT products.product_id FROM products WHERE products.name IN ('User external')))\n</code></pre></p> </li> <li> <p>new product block:</p> <p>Create product blocks Product block: UserGroupBlock Supply the product block description: User group settings Supply the product block tag: UGS <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=`#!sql INSERT INTO product_blocks (name, description, tag, status) VALUES ('UserGroupBlock', 'User group settings', 'UGS', 'active') RETURNING product_blocks.product_block_id`\n</code></pre></p> </li> <li> <p>new resource type:</p> <p>Create resource types Supply description for new resource type group_name: Unique name of user group <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO resource_types (resource_type, description) VALUES ('group_name', 'Unique name of user group') RETURNING resource_types.resource_type_id\n</code></pre></p> </li> <li> <p>default value for resource type per product block (necessary for adding a default value to existing instances):</p> <p>Create subscription instance values Supply default subscription instance value for resource type group_name and product block UserGroupBlock: group <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.resource_type_helpers] sql_string=\n                WITH subscription_instance_ids AS (\n                    SELECT subscription_instances.subscription_instance_id\n                    FROM   subscription_instances\n                    WHERE  subscription_instances.product_block_id IN (\n                        SELECT product_blocks.product_block_id\n                        FROM   product_blocks\n                        WHERE  product_blocks.name = 'UserGroupBlock'\n                    )\n                )\n\n                INSERT INTO\n                    subscription_instance_values (subscription_instance_id, resource_type_id, value)\n                SELECT\n                    subscription_instance_ids.subscription_instance_id,\n                    resource_types.resource_type_id,\n                    'group'\n                FROM resource_types\n                CROSS JOIN subscription_instance_ids\n                WHERE resource_types.resource_type = 'group_name'\n</code></pre></p> </li> </ul> <p>Last part generates the migration with the generated SQL: <pre><code>Generating migration file\n2022-10-27 11:45:10 [info] Version Locations [orchestrator.cli.database] locations=/home/tjeerddie/projects_surf/example-orchestrator/migrations/versions/schema /home/tjeerddie/projects_surf/example-orchestrator/.venv/lib/python3.10/site-packages/orchestrator/migrations/versions/schema\n  Generating /home/tjeerddie/projects_surf/example-orchestrator/migrations/versions/schema/2022-10-27_a8946b2d1647_test.py ...  done\nMigration generated. Don't forget to create a database backup before migrating!\n</code></pre></p> <p>If you are running with <code>--test</code>, the SQL file will not be generated.</p>"},{"location":"reference-docs/cli/#generate","title":"generate","text":"<p>Generate products, workflows and other artifacts.</p> <p>Products can be described in a YAML configuration file which makes it easy to generate product and product block domain models, and skeleton workflows and unit tests. Note that this is a one time thing, the generate commands do not support updating existing products, product-blocks, workflows and migrations, in this case have a look at the <code>db migrate-domain-models</code> and <code>db migrate-workflows</code> commands. But it does however help in defining new products with stakeholders, will generate code that conforms to current workfloworchestrator coding BCP, and will actually run (although limited in functionality of course).</p> <p>After describing a new product in a configuration file, the following commands are typically run:</p> <pre><code>python main.py generate product-blocks\npython main.py generate products\npython main.py generate workflows\npython main.py generate migration\n</code></pre> <p>The generate command should be called from the top level folder of your orchestrator implementation, this is the folder that contains the <code>products</code> sub folder, amongst others, except when the <code>--prefix</code> is used to point to that folder. In case there are product blocks defined that use other generated product blocks, the order in which <code>generate product-blocks</code> is run is important, the code for the blocks used in other blocks should be generated first.</p>"},{"location":"reference-docs/cli/#config-file","title":"config file","text":"<p>An example of a simple product configuration:</p> <pre><code>config:\n  summary_forms: true\nname: node\ntype: Node\ntag: NODE\ndescription: \"Network node\"\nfixed_inputs:\n  - name: node_rack_mountable\n    type: bool\n    description: \"is node rack mountable\"\n  - name: node_vendor\n    type: enum\n    description: \"vendor of node\"\n    enum_type: str\n    values:\n      - \"Cisco\"\n      - \"Nokia\"\nproduct_blocks:\n  - name: node\n    type: Node\n    tag: NODE\n    description: \"node product block\"\n    fields:\n      - name: node_name\n        description: \"Unique name of the node\"\n        type: str\n        required: provisioning\n        modifiable:\n      - name: node_description\n        description: \"Description of the node\"\n        type: str\n        modifiable:\n      - name: ims_id\n        description: \"ID of the node in the inventory management system\"\n        type: int\n        required: active\n      - name: under_maintenance\n        description: \"node is under maintenance\"\n        type: bool\n        required: initial\n        default: False\n\nworkflows:\n  - name: terminate\n    validations:\n      - id: can_only_terminate_when_under_maintenance\n        description: \"Can only terminate when the node is placed under maintenance\"\n  - name: validate\n    enabled: false\n    validations:\n      - id: validate_ims_administration\n        description: \"Validate that the node is correctly administered in IMS\"\n</code></pre> <p>Next we will describe the different sections in more detail:</p>"},{"location":"reference-docs/cli/#global-config-section","title":"global <code>config</code> section","text":"<p>This section sets some global configuration, applicable for most workflows.</p> <pre><code>config:\n  summary_forms: true\n</code></pre> <ul> <li><code>summary_forms</code> indicates if a summary form will be generated in the   create and modify workflows, default is <code>false</code>.</li> </ul>"},{"location":"reference-docs/cli/#product-type-definition","title":"product type definition","text":"<pre><code>name: node\ntype: Node\ntag: NODE\ndescription: \"Network node\"\n</code></pre> <p>Every product type is described using the following fields: - <code>name</code>: the name of type product type, used in descriptions, as variable name, and to generate           filenames - <code>type</code>: used to indicate the type of the product in Python code, types in           Python usually starts with an uppercase character - <code>tag</code>: used to register the product tag in the database, can for example be used to filter          products, will typically be all uppercase - <code>description</code>: descriptive text for the product type</p>"},{"location":"reference-docs/cli/#fixed_inputs-section","title":"<code>fixed_inputs</code> section","text":"<p>In this section we define a list of fixed inputs for a product.</p> <pre><code>fixed_inputs:\n  - name: node_vendor\n    type: enum\n    description: \"vendor of node\"\n    enum_type: str\n    values:\n      - \"Cisco\"\n      - \"Nokia\"\n  - name: node_ports\n    type: enum\n    description: \"number of ports in chassis\"\n    values:\n      - 10\n      - 20\n      - 40\n</code></pre> <p>A fixed input has a <code>name</code>, <code>type</code> and <code>description</code> field. If the type is a primitive type (for example: str, bool, int), then this is sufficient. In case of <code>node_vendor</code> an <code>enum</code> type is used, and two additional fields to describe the enumeration <code>type</code> and its possible <code>values</code>. Both <code>str</code> and <code>int</code> enums are supported.</p> <p>When one or more enum types are specified, the necessary migration and product registry code will be generated for all possible combinations of the enums. In this example six products of type Node will be generated: \"Cisco 10\", \"Cisco 20\", \"Cisco 40\", \"Nokia 10\", \"Nokia 20\" and \"Nokia 40\".</p>"},{"location":"reference-docs/cli/#product_blocks-section","title":"<code>product_blocks</code> section","text":"<pre><code>product_blocks:\n  - name: port\n    type: Port\n    tag: PORT\n    description: \"port product block\"\n    fields:\n      -\n      -\n</code></pre> <p>In this section we define the product block(s) that are part of this product.</p> <p>A product configuration should contain exactly 1 root product block. This means there should be 1 product block that is not used by any other product blocks within this product. If the configuration does contain multiple root blocks, or none at all due to a cyclic dependency, then the generator will raise a helpful error.</p> <p>The use of <code>name</code>, <code>type</code>, <code>tag</code> and <code>description</code> in the product block definition is equivalent to the product definition above. The <code>fields</code> describe the product block resource types.</p>"},{"location":"reference-docs/cli/#product-block-fields","title":"product block fields","text":"<pre><code>  - name: port_mode\n    description: 'port mode'\n    required: provisioning\n    modifiable:\n    type: enum\n    enum_type: str\n    values:\n      - \"untagged\"\n      - \"tagged\"\n      - \"link_member\"\n    default: \"tagged\"\n    validations:\n      - id: must_be_unused_to_change_mode\n        description: \"Mode can only be changed when there are no services attached to it\"\n</code></pre> <p>Resource types are described by the following:</p> <ul> <li><code>name</code>: name of the resource type, usually in snake case</li> <li><code>decription</code>: resource type description</li> <li><code>required</code>: if the resource type is required starting from lifecycle state <code>inital</code>, <code>provisioning</code>               or <code>active</code>, when omitted the resource type will always be optional</li> <li><code>modifiable</code>: indicate if the resource type can be altered by a modify workflow, when omitted                 no code will be generated to modify the resource type, currently only supported for simple types</li> <li><code>default</code>: specify the default for this resource type, this is mandatory when <code>required</code> is set to              <code>intitial</code>, will be <code>None</code> if not specified</li> <li><code>validations</code>: specify the validation <code>id</code> and <code>description</code>, generates a skeleton                  validation function used in a new annotated type</li> <li><code>type</code>: see explanation below</li> </ul>"},{"location":"reference-docs/cli/#resource-type-types","title":"resource type types","text":"<p>The following types are supported when specifying resource types:</p> <ul> <li>primitive types like <code>int</code>, <code>str</code>, and <code>bool</code> <pre><code>- name: circuit_id\n  type: int\n</code></pre></li> <li>types with a period in there name will generate code to import that type, e.q. <code>ipaddress.IPv4Address</code> <pre><code>- name: ipv4_loopback\n  type: ipaddress.IPv4Address\n</code></pre></li> <li>existing product block types like <code>UserGroup</code>, possible types will be read from <code>products/product_blocks</code> <pre><code>- name: group\n  type: UserGroup\n</code></pre></li> <li>the type <code>list</code>, used together with <code>min_items</code> and <code>max_items</code> to specify the constraints, and   <code>list_type</code> to specify the type of the list items   <pre><code>- name: link_members\n  type: list\n  list_type: Link\n  min_items: 2\n  max_items: 2\n</code></pre></li> <li>validation code for constrained a <code>int</code> is generated when <code>min_value</code> and <code>max_value</code> are specified   <pre><code>  - name: ims_id\n    type: int\n    min_value: 1\n    max_value: 32_767\n</code></pre></li> </ul>"},{"location":"reference-docs/cli/#workflows","title":"workflows","text":"<pre><code>  - name: create\n    validations:\n      - id: endpoints_cannot_be_on_same_node\n        description: \"Service endpoints must land on different nodes\"\n  - name: validate\n    enabled: false\n    validations:\n      - id: validate_ims_administration\n        description: \"Validate that the node is correctly administered in IMS\"\n</code></pre> <p>The following optional workflow configuration is supported:</p> <ul> <li><code>name</code>: can be either <code>create</code>, <code>modify</code>, <code>terminate</code> or <code>validate</code></li> <li><code>enabled</code>: to enable or disable the generation of code for that type of workflow, is <code>true</code> when omitted</li> <li><code>validations</code>: list of validations used to generate skeleton code as <code>model_validator</code> in input forms,   and validation steps in the validate workflow</li> </ul>"},{"location":"reference-docs/cli/#migration","title":"migration","text":"<p>The <code>python main.py generate migration</code> command creates a migration from a configuration file.</p> <p>Options</p> <p>--config-file - The configuration file [default: None] --python-version - Python version for generated code [default: 3.11] --skip-existing-blocks - If set, the migration will not contain product blocks for which a python implementation exists [default: False]  </p>"},{"location":"reference-docs/cli/#product","title":"product","text":"<p>The <code>python main.py generate product</code> command creates a product domain model from a configuration file.</p> <p>Options</p> <p>--config-file - The configuration file [default: None] --dryrun | --no-dryrun - Dry run [default: dryrun] --force - Force overwrite of existing files --python-version - Python version for generated code [default: 3.11] --folder-prefix - Folder prefix, e.g. /workflows [default: None]"},{"location":"reference-docs/cli/#product-blocks","title":"product-blocks","text":"<p>The <code>python main.py generate product-blocks</code> command creates product block domain models from a configuration file.</p> <p>Options</p> <p>--config-file - The configuration file [default: None] --dryrun | --no-dryrun - Dry run [default: dryrun] --force - Force overwrite of existing files --python-version - Python version for generated code [default: 3.11] --folder-prefix - Folder prefix, e.g. /workflows [default: None]"},{"location":"reference-docs/cli/#unit-tests","title":"unit-tests","text":"<p>The <code>python main.py generate unit-tests</code> command creates unit tests from a configuration file.</p> <p>Options</p> <p>--config-file - The configuration file [default: None] --dryrun | --no-dryrun - Dry run [default: dryrun] --force - Force overwrite of existing files --python-version - Python version for generated code [default: 3.11] --tdd - Force test driven development with failing asserts [default: True]  </p>"},{"location":"reference-docs/cli/#workflows_1","title":"workflows","text":"<p>The <code>python main.py generate workflows</code> command creates create, modify, terminate and validate workflows from a configuration file. The <code>--custom-templates</code> option can be used to specify a folder with custom templates to add additional import statements, input form fields and workflow steps to the create, modify and terminate workflows.</p> <p>Options</p> <p>--config-file - The configuration file [default: None] --dryrun | --no-dryrun - Dry run [default: dryrun] --force - Force overwrite of existing files --python-version - Python version for generated code [default: 3.11] --folder-prefix - Folder prefix, e.g. /workflows [default: None] --custom-templates - Custom templates folder [default: None] <p>Note</p> <p>The <code>workflows/__init__.py</code> will only be extended with the needed <code>LazyWorkflowInstance</code> declarations when <code>--force</code> is used.</p>"},{"location":"reference-docs/cli/#scheduler","title":"scheduler","text":"<p>Commands to interact with the scheduler and scheduled jobs.</p>"},{"location":"reference-docs/cli/#orchestrator.cli.scheduler.run","title":"<code>run</code>","text":"<p>Starts the scheduler in the foreground.</p> <p>While running, this process will:</p> <ul> <li>Periodically wake up when the next schedule is due for execution, and run it</li> <li>Process schedule changes made through the schedule API</li> </ul> Source code in <code>orchestrator/cli/scheduler.py</code> <pre><code>@app.command()\ndef run() -&gt; None:\n    \"\"\"Starts the scheduler in the foreground.\n\n    While running, this process will:\n\n      * Periodically wake up when the next schedule is due for execution, and run it\n      * Process schedule changes made through the schedule API\n    \"\"\"\n\n    def _get_scheduled_task_item_from_queue(redis_conn: Redis) -&gt; tuple[str, bytes] | None:\n        \"\"\"Get an item from the Redis Queue for scheduler tasks.\"\"\"\n        try:\n            return redis_conn.brpop(SCHEDULER_QUEUE, timeout=1)\n        except ConnectionError as e:\n            typer.echo(f\"There was a connection error with Redis. Retrying in 3 seconds... {e}\")\n            time.sleep(3)\n        except Exception as e:\n            typer.echo(f\"There was an unexpected error with Redis. Retrying in 1 second... {e}\")\n            time.sleep(1)\n\n        return None\n\n    with get_scheduler() as scheduler_connection:\n        redis_connection = create_redis_client(app_settings.CACHE_URI)\n        while True:\n            item = _get_scheduled_task_item_from_queue(redis_connection)\n            if not item:\n                continue\n\n            with db.database_scope():\n                workflow_scheduler_queue(item, scheduler_connection)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.scheduler.force","title":"<code>force</code>","text":"<p>Force the execution of (a) scheduler(s) based on a schedule ID.</p> <p>Use the <code>show-schedule</code> command to determine the ID of the schedule to execute.</p> CLI Arguments <pre><code>Arguments:\n    SCHEDULE_ID  ID of the schedule to execute\n</code></pre> Source code in <code>orchestrator/cli/scheduler.py</code> <pre><code>@app.command()\ndef force(task_id: str) -&gt; None:\n    \"\"\"Force the execution of (a) scheduler(s) based on a schedule ID.\n\n    Use the `show-schedule` command to determine the ID of the schedule to execute.\n\n    CLI Arguments:\n        ```sh\n        Arguments:\n            SCHEDULE_ID  ID of the schedule to execute\n        ```\n    \"\"\"\n    task = get_scheduler_task(task_id)\n\n    if not task:\n        typer.echo(f\"Task '{task_id}' not found.\")\n        raise typer.Exit(code=1)\n\n    typer.echo(f\"Running Task [{task.id}] now...\")\n    try:\n        task.func(*task.args or (), **task.kwargs or {})\n        typer.echo(\"Task executed successfully.\")\n    except Exception as e:\n        typer.echo(f\"Task execution failed: {e}\")\n        raise typer.Exit(code=1)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.scheduler.show_schedule","title":"<code>show_schedule</code>","text":"<p>The <code>show-schedule</code> command shows an overview of the scheduled jobs.</p> Source code in <code>orchestrator/cli/scheduler.py</code> <pre><code>@app.command()\ndef show_schedule() -&gt; None:\n    \"\"\"The `show-schedule` command shows an overview of the scheduled jobs.\"\"\"\n    from rich.console import Console\n    from rich.table import Table\n\n    from orchestrator.schedules.service import get_linker_entries_by_schedule_ids\n\n    console = Console()\n\n    table = Table(title=\"Scheduled Tasks\")\n    table.add_column(\"id\", no_wrap=True)\n    table.add_column(\"name\")\n    table.add_column(\"source\")\n    table.add_column(\"next run time\")\n    table.add_column(\"trigger\")\n\n    scheduled_tasks = get_all_scheduler_tasks()\n    _schedule_ids = [task.id for task in scheduled_tasks]\n    api_managed = {str(i.schedule_id) for i in get_linker_entries_by_schedule_ids(_schedule_ids)}\n\n    for task in scheduled_tasks:\n        source = \"API\" if task.id in api_managed else \"decorator\"\n        run_time = str(task.next_run_time.replace(microsecond=0))\n        table.add_row(task.id, task.name, source, str(run_time), str(task.trigger))\n\n    console.print(table)\n</code></pre>"},{"location":"reference-docs/cli/#orchestrator.cli.scheduler.load_initial_schedule","title":"<code>load_initial_schedule</code>","text":"<p>The <code>load-initial-schedule</code> command loads the initial schedule using the scheduler API.</p> The initial schedules are <ul> <li>Task Resume Workflows</li> <li>Task Clean Up Tasks</li> <li>Task Validate Subscriptions</li> </ul> <p>This command is idempotent since 4.7.1 when the scheduler is running. The schedules are only created when they do not already exist in the database.</p> Source code in <code>orchestrator/cli/scheduler.py</code> <pre><code>@app.command()\ndef load_initial_schedule() -&gt; None:\n    \"\"\"The `load-initial-schedule` command loads the initial schedule using the scheduler API.\n\n    The initial schedules are:\n      - Task Resume Workflows\n      - Task Clean Up Tasks\n      - Task Validate Subscriptions\n\n    This command is idempotent since 4.7.1 when the scheduler is running. The schedules are only\n    created when they do not already exist in the database.\n    \"\"\"\n    initial_schedules = [\n        {\n            \"name\": \"Task Resume Workflows\",\n            \"workflow_name\": \"task_resume_workflows\",\n            \"workflow_id\": \"\",\n            \"trigger\": \"interval\",\n            \"trigger_kwargs\": {\"hours\": 1},\n        },\n        {\n            \"name\": \"Task Clean Up Tasks\",\n            \"workflow_name\": \"task_clean_up_tasks\",\n            \"workflow_id\": \"\",\n            \"trigger\": \"interval\",\n            \"trigger_kwargs\": {\"hours\": 6},\n        },\n        {\n            \"name\": \"Task Validate Subscriptions\",\n            \"workflow_name\": \"task_validate_subscriptions\",\n            \"workflow_id\": \"\",\n            \"trigger\": \"cron\",\n            \"trigger_kwargs\": {\"hour\": 0, \"minute\": 10},\n        },\n    ]\n\n    for schedule in initial_schedules:\n        # enrich with workflow id\n        workflow_name = cast(str, schedule.get(\"workflow_name\"))\n        workflow = get_workflow_by_name(workflow_name)\n\n        if not workflow:\n            typer.echo(f\"Workflow '{schedule['workflow_name']}' not found. Skipping schedule.\")\n            continue\n\n        schedule[\"workflow_id\"] = workflow.workflow_id\n\n        typer.echo(f\"Initial Schedule: {schedule}\")\n        add_unique_scheduled_task_to_queue(APSchedulerJobCreate(**schedule))  # type: ignore\n</code></pre>"},{"location":"reference-docs/database/","title":"Database","text":"<p>For most operation database interactions, you will want to use the CLI tool, which is documented in depth here. The rest of this page documents other unique things that you should know about the database in the WFO.</p>"},{"location":"reference-docs/database/#architecture","title":"Architecture","text":"<p>The WFO database is built on top of the SQLAlchemy ORM, and we use Alembic for building database migrations. If you aren't familiar with these technologies, you should definitely read up on them to get a better understanding of how the orchestrator's database components work. The database models we use for the orchestrator-core live in <code>orchestrator-core/db/models.py</code>.</p> Example: <code>orchestrator-core/db/models.py</code> <pre><code># Copyright 2019-2020 SURF, G\u00c9ANT.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nimport enum\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING\nfrom uuid import UUID\n\nimport sqlalchemy\nimport structlog\nfrom more_itertools import first_true\nfrom pgvector.sqlalchemy import Vector\nfrom sqlalchemy import (\n    TEXT,\n    TIMESTAMP,\n    Boolean,\n    CheckConstraint,\n    Column,\n    Enum,\n    Float,\n    ForeignKey,\n    Index,\n    Integer,\n    LargeBinary,\n    PrimaryKeyConstraint,\n    Select,\n    String,\n    Table,\n    TypeDecorator,\n    UniqueConstraint,\n    select,\n    text,\n)\nfrom sqlalchemy.dialects import postgresql as pg\nfrom sqlalchemy.engine import Dialect\nfrom sqlalchemy.exc import DontWrapMixin\nfrom sqlalchemy.ext.associationproxy import association_proxy\nfrom sqlalchemy.ext.orderinglist import ordering_list\nfrom sqlalchemy.orm import Mapped, deferred, mapped_column, object_session, relationship, undefer\nfrom sqlalchemy.sql.functions import GenericFunction\nfrom sqlalchemy_utils import LtreeType, TSVectorType, UUIDType\n\nfrom orchestrator.config.assignee import Assignee\nfrom orchestrator.db.database import BaseModel, SearchQuery\nfrom orchestrator.llm_settings import llm_settings\nfrom orchestrator.search.core.types import FieldType\nfrom orchestrator.targets import Target\nfrom orchestrator.utils.datetime import nowtz\nfrom orchestrator.version import GIT_COMMIT_HASH\n\nif TYPE_CHECKING:\n    from orchestrator.search.query.state import QueryState\n\nlogger = structlog.get_logger(__name__)\n\nTAG_LENGTH = 20\nSTATUS_LENGTH = 255\n\n# Field length limits chosen based on expected usage patterns\n# These values are intended to be reasonable, but give lots of wiggle room\n# If these values are updated, they also need to be updated in a migration, as in migration d69e10434a04\nNOTE_LENGTH = 5000\nDESCRIPTION_LENGTH = 2000\nFAILED_REASON_LENGTH = 10000\nTRACEBACK_LENGTH = 50000\nRESOURCE_VALUE_LENGTH = 10000\nDOMAIN_MODEL_ATTR_LENGTH = 255\n\n\nclass StringThatAutoConvertsToNullWhenEmpty(TypeDecorator):\n    \"\"\"A String type that converts empty strings to NULL on save.\"\"\"\n\n    impl = String\n    cache_ok = True\n    python_type = str\n\n    def __init__(self, length: int | None = None):\n        super().__init__(length)\n\n    def process_bind_param(self, value: str | None, dialect: Dialect) -&gt; str | None:\n        \"\"\"Called when saving to DB - convert empty/whitespace to NULL.\"\"\"\n        if value is not None and value.strip() == \"\":\n            return None\n        return value\n\n    def process_result_value(self, value: str | None, dialect: Dialect) -&gt; str | None:\n        \"\"\"Called when loading from DB - return as-is.\"\"\"\n        return value\n\n\nclass UtcTimestampError(Exception, DontWrapMixin):\n    pass\n\n\nclass UtcTimestamp(TypeDecorator):\n    \"\"\"Timestamps in UTC.\n\n    This column type always returns timestamps with the UTC timezone, regardless of the database/connection time zone\n    configuration. It also guards against accidentally trying to store Python naive timestamps (those without a time\n    zone).\n    \"\"\"\n\n    impl = sqlalchemy.types.TIMESTAMP(timezone=True)\n    cache_ok = False\n    python_type = datetime\n\n    def process_bind_param(self, value: datetime | None, dialect: Dialect) -&gt; datetime | None:\n        if value is not None:\n            if value.tzinfo is None:\n                raise UtcTimestampError(f\"Expected timestamp with tzinfo. Got naive timestamp {value!r} instead\")\n        return value\n\n    def process_result_value(self, value: datetime | None, dialect: Dialect) -&gt; datetime | None:\n        return value.astimezone(timezone.utc) if value else value\n\n\nclass InputStateTable(BaseModel):\n    __tablename__ = \"input_states\"\n\n    class InputType(enum.Enum):\n        user_input = \"user_input\"\n        initial_state = \"initial_state\"\n\n    input_state_id = mapped_column(UUIDType, primary_key=True, server_default=text(\"uuid_generate_v4()\"), index=True)\n    process_id = mapped_column(\"pid\", UUIDType, ForeignKey(\"processes.pid\"), nullable=False)\n    input_state = mapped_column(pg.JSONB(), nullable=False)\n    input_time = mapped_column(UtcTimestamp, server_default=text(\"current_timestamp()\"), nullable=False)\n    input_type = mapped_column(Enum(InputType), nullable=False)\n\n\nclass ProcessTable(BaseModel):\n    __tablename__ = \"processes\"\n\n    process_id = mapped_column(\"pid\", UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True, index=True)\n    workflow_id = mapped_column(\"workflow_id\", UUIDType, ForeignKey(\"workflows.workflow_id\"), nullable=False)\n    assignee = mapped_column(String(50), server_default=Assignee.SYSTEM, nullable=False)\n    last_status = mapped_column(String(50), nullable=False)\n    last_step = mapped_column(String(255), nullable=True)\n    started_at = mapped_column(UtcTimestamp, server_default=text(\"current_timestamp()\"), nullable=False)\n    last_modified_at = mapped_column(\n        UtcTimestamp, server_default=text(\"current_timestamp()\"), onupdate=nowtz, nullable=False\n    )\n    failed_reason = mapped_column(String(FAILED_REASON_LENGTH))\n    traceback = mapped_column(String(TRACEBACK_LENGTH))\n    created_by = mapped_column(String(255), nullable=True)\n    is_task = mapped_column(Boolean, nullable=False, server_default=text(\"false\"), index=True)\n\n    steps = relationship(\n        \"ProcessStepTable\", cascade=\"delete\", passive_deletes=True, order_by=\"asc(ProcessStepTable.completed_at)\"\n    )\n    input_states = relationship(\"InputStateTable\", cascade=\"delete\", order_by=\"desc(InputStateTable.input_time)\")\n    process_subscriptions = relationship(\"ProcessSubscriptionTable\", back_populates=\"process\", passive_deletes=True)\n    workflow = relationship(\"WorkflowTable\", back_populates=\"processes\")\n\n    subscriptions = association_proxy(\"process_subscriptions\", \"subscription\")\n\n    @property\n    def workflow_name(self) -&gt; Column:\n        return self.workflow.name\n\n\nclass ProcessStepTable(BaseModel):\n    __tablename__ = \"process_steps\"\n\n    step_id = mapped_column(\"stepid\", UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    process_id = mapped_column(\n        \"pid\", UUIDType, ForeignKey(\"processes.pid\", ondelete=\"CASCADE\"), nullable=False, index=True\n    )\n    name = mapped_column(String(), nullable=False)\n    status = mapped_column(String(50), nullable=False)\n    state = mapped_column(pg.JSONB(), nullable=False)\n    created_by = mapped_column(String(255), nullable=True)\n    completed_at = mapped_column(UtcTimestamp, server_default=text(\"statement_timestamp()\"), nullable=False)\n    started_at = mapped_column(UtcTimestamp, server_default=text(\"statement_timestamp()\"), nullable=False)\n    commit_hash = mapped_column(String(40), nullable=True, default=GIT_COMMIT_HASH)\n\n\nclass ProcessSubscriptionTable(BaseModel):\n    __tablename__ = \"processes_subscriptions\"\n\n    id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    process_id = mapped_column(\n        \"pid\", UUIDType, ForeignKey(\"processes.pid\", ondelete=\"CASCADE\"), index=True, nullable=False\n    )\n    subscription_id = mapped_column(UUIDType, ForeignKey(\"subscriptions.subscription_id\"), nullable=False, index=True)\n    created_at = mapped_column(UtcTimestamp, server_default=text(\"current_timestamp()\"), nullable=False)\n\n    # FIXME: workflow_target is already stored in the workflow table, this column should get removed in a later release.\n    workflow_target = mapped_column(String(255), nullable=True)\n\n    process = relationship(\"ProcessTable\", back_populates=\"process_subscriptions\")\n    subscription = relationship(\"SubscriptionTable\", back_populates=\"processes\")\n\n\nprocesses_subscriptions_ix = Index(\n    \"processes_subscriptions_ix\", ProcessSubscriptionTable.process_id, ProcessSubscriptionTable.subscription_id\n)\n\nproduct_product_block_association = Table(\n    \"product_product_blocks\",\n    BaseModel.metadata,\n    Column(\"product_id\", UUIDType, ForeignKey(\"products.product_id\", ondelete=\"CASCADE\"), primary_key=True),\n    Column(\n        \"product_block_id\",\n        UUIDType,\n        ForeignKey(\"product_blocks.product_block_id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n    ),\n)\n\nproduct_block_resource_type_association = Table(\n    \"product_block_resource_types\",\n    BaseModel.metadata,\n    Column(\n        \"product_block_id\",\n        UUIDType,\n        ForeignKey(\"product_blocks.product_block_id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n    ),\n    Column(\n        \"resource_type_id\",\n        UUIDType,\n        ForeignKey(\"resource_types.resource_type_id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n    ),\n)\n\nproduct_workflows_association = Table(\n    \"products_workflows\",\n    BaseModel.metadata,\n    Column(\"product_id\", UUIDType, ForeignKey(\"products.product_id\", ondelete=\"CASCADE\"), primary_key=True),\n    Column(\"workflow_id\", UUIDType, ForeignKey(\"workflows.workflow_id\", ondelete=\"CASCADE\"), primary_key=True),\n)\n\n\nclass ProductTable(BaseModel):\n    __tablename__ = \"products\"\n    __table_args__ = {\"extend_existing\": True}\n\n    __allow_unmapped__ = True\n\n    product_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False, unique=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    product_type = mapped_column(String(255), nullable=False)\n    tag = mapped_column(String(TAG_LENGTH), nullable=False, index=True)\n    status = mapped_column(String(STATUS_LENGTH), nullable=False)\n    created_at = mapped_column(UtcTimestamp, nullable=False, server_default=text(\"current_timestamp()\"))\n    end_date = mapped_column(UtcTimestamp)\n\n    product_blocks = relationship(\n        \"ProductBlockTable\",\n        secondary=product_product_block_association,\n        back_populates=\"products\",\n        passive_deletes=True,\n    )\n    workflows = relationship(\n        \"WorkflowTable\",\n        secondary=product_workflows_association,\n        secondaryjoin=\"and_(products_workflows.c.workflow_id == WorkflowTable.workflow_id, \"\n        \"WorkflowTable.deleted_at == None)\",\n        back_populates=\"products\",\n        passive_deletes=True,\n    )\n    fixed_inputs = relationship(\n        \"FixedInputTable\", cascade=\"all, delete-orphan\", back_populates=\"product\", passive_deletes=True\n    )\n\n    def find_block_by_name(self, name: str) -&gt; ProductBlockTable:\n        if session := object_session(self):\n            return session.query(ProductBlockTable).with_parent(self).filter(ProductBlockTable.name == name).one()\n        raise AssertionError(\"Session should not be None\")\n\n    def fixed_input_value(self, name: str) -&gt; str:\n        if session := object_session(self):\n            return (\n                session.query(FixedInputTable)\n                .with_parent(self)\n                .filter(FixedInputTable.name == name)\n                .with_entities(FixedInputTable.value)\n                .scalar()\n            )\n        raise AssertionError(\"Session should not be None\")\n\n    def _subscription_workflow_key(self, target: Target) -&gt; str | None:\n        wfs = list(filter(lambda w: w.target == target, self.workflows))\n        return wfs[0].name if len(wfs) &gt; 0 else None\n\n    def create_subscription_workflow_key(self) -&gt; str | None:\n        return self._subscription_workflow_key(Target.CREATE)\n\n    def terminate_subscription_workflow_key(self) -&gt; str | None:\n        return self._subscription_workflow_key(Target.TERMINATE)\n\n    def modify_subscription_workflow_key(self, name: str) -&gt; str | None:\n        wfs = list(filter(lambda w: w.target == Target.MODIFY and w.name == name, self.workflows))\n        return wfs[0].name if len(wfs) &gt; 0 else None\n\n    def workflow_by_key(self, name: str) -&gt; WorkflowTable | None:\n        return first_true(self.workflows, None, lambda wf: wf.name == name)  # type: ignore\n\n\nclass FixedInputTable(BaseModel):\n    __tablename__ = \"fixed_inputs\"\n    __table_args__ = (UniqueConstraint(\"name\", \"product_id\"), {\"extend_existing\": True})\n\n    fixed_input_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False)\n    value = mapped_column(String(), nullable=False)\n    created_at = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=text(\"current_timestamp()\"))\n    product_id = mapped_column(UUIDType, ForeignKey(\"products.product_id\", ondelete=\"CASCADE\"), nullable=False)\n\n    product = relationship(\"ProductTable\", back_populates=\"fixed_inputs\")\n\n\nclass ProductBlockTable(BaseModel):\n    __tablename__ = \"product_blocks\"\n\n    __allow_unmapped__ = True\n\n    product_block_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False, unique=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    tag = mapped_column(String(TAG_LENGTH))\n    status = mapped_column(String(STATUS_LENGTH))\n    created_at = mapped_column(UtcTimestamp, nullable=False, server_default=text(\"current_timestamp()\"))\n    end_date = mapped_column(UtcTimestamp)\n\n    products = relationship(\n        \"ProductTable\", secondary=product_product_block_association, back_populates=\"product_blocks\"\n    )\n    resource_types = relationship(\n        \"ResourceTypeTable\",\n        secondary=product_block_resource_type_association,\n        back_populates=\"product_blocks\",\n        passive_deletes=True,\n    )\n    in_use_by_block_relations: Mapped[list[ProductBlockRelationTable]] = relationship(\n        \"ProductBlockRelationTable\",\n        lazy=\"subquery\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        back_populates=\"depends_on\",\n        foreign_keys=\"[ProductBlockRelationTable.depends_on_id]\",\n    )\n    depends_on_block_relations: Mapped[list[ProductBlockRelationTable]] = relationship(\n        \"ProductBlockRelationTable\",\n        lazy=\"subquery\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        back_populates=\"in_use_by\",\n        foreign_keys=\"[ProductBlockRelationTable.in_use_by_id]\",\n    )\n\n    in_use_by = association_proxy(\n        \"in_use_by_block_relations\",\n        \"in_use_by\",\n        creator=lambda in_use_by: ProductBlockRelationTable(in_use_by=in_use_by),\n    )\n    depends_on = association_proxy(\n        \"depends_on_block_relations\",\n        \"depends_on\",\n        creator=lambda depends_on: ProductBlockRelationTable(depends_on=depends_on),\n    )\n\n    @staticmethod\n    def find_by_name(name: str) -&gt; ProductBlockTable:\n        return ProductBlockTable.query.filter(ProductBlockTable.name == name).one()\n\n    @staticmethod\n    def find_by_tag(tag: str) -&gt; ProductBlockTable:\n        return ProductBlockTable.query.filter(ProductBlockTable.tag == tag).one()\n\n    def find_resource_type_by_name(self, name: str) -&gt; ResourceTypeTable:\n        if session := object_session(self):\n            return (\n                session.query(ResourceTypeTable).with_parent(self).filter(ResourceTypeTable.resource_type == name).one()\n            )\n        raise AssertionError(\"Session should not be None\")\n\n\nProductBlockTable.parent_relations = ProductBlockTable.in_use_by_block_relations\nProductBlockTable.children_relations = ProductBlockTable.depends_on_block_relations\n\n\nclass ProductBlockRelationTable(BaseModel):\n    __tablename__ = \"product_block_relations\"\n\n    in_use_by_id = mapped_column(\n        UUIDType, ForeignKey(\"product_blocks.product_block_id\", ondelete=\"CASCADE\"), primary_key=True\n    )\n    depends_on_id = mapped_column(\n        UUIDType, ForeignKey(\"product_blocks.product_block_id\", ondelete=\"CASCADE\"), primary_key=True\n    )\n    min = mapped_column(Integer())\n    max = mapped_column(Integer())\n\n    depends_on: Mapped[ProductBlockTable] = relationship(\n        \"ProductBlockTable\", back_populates=\"in_use_by_block_relations\", foreign_keys=[depends_on_id]\n    )\n    in_use_by: Mapped[ProductBlockTable] = relationship(\n        \"ProductBlockTable\", back_populates=\"depends_on_block_relations\", foreign_keys=[in_use_by_id]\n    )\n\n\nProductBlockRelationTable.parent_id = ProductBlockRelationTable.in_use_by_id\nProductBlockRelationTable.child_id = ProductBlockRelationTable.depends_on_id\n\nproduct_block_relation_index = Index(\n    \"product_block_relation_i_d_ix\",\n    ProductBlockRelationTable.in_use_by_id,\n    ProductBlockRelationTable.depends_on_id,\n    unique=True,\n)\n\n\nclass ResourceTypeTable(BaseModel):\n    __tablename__ = \"resource_types\"\n\n    resource_type_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    resource_type = mapped_column(String(510), nullable=False, unique=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH))\n\n    product_blocks = relationship(\n        \"ProductBlockTable\", secondary=product_block_resource_type_association, back_populates=\"resource_types\"\n    )\n\n\nclass WorkflowTable(BaseModel):\n    __tablename__ = \"workflows\"\n\n    workflow_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False, unique=True)\n    target = mapped_column(String(), nullable=False)\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    created_at = mapped_column(UtcTimestamp, nullable=False, server_default=text(\"current_timestamp()\"))\n    deleted_at = mapped_column(UtcTimestamp, deferred=True)\n\n    products = relationship(\n        \"ProductTable\",\n        secondary=product_workflows_association,\n        passive_deletes=True,\n        back_populates=\"workflows\",\n    )\n    processes = relationship(\"ProcessTable\", cascade=\"all, delete-orphan\", back_populates=\"workflow\")\n\n    is_task = mapped_column(Boolean, nullable=False, server_default=text(\"false\"))\n\n    @staticmethod\n    def select() -&gt; Select:\n        return (\n            select(WorkflowTable).options(undefer(WorkflowTable.deleted_at)).filter(WorkflowTable.deleted_at.is_(None))\n        )\n\n    def delete(self) -&gt; WorkflowTable:\n        self.deleted_at = nowtz()\n        return self\n\n\nclass SubscriptionInstanceRelationTable(BaseModel):\n    __tablename__ = \"subscription_instance_relations\"\n\n    in_use_by_id = mapped_column(\n        UUIDType, ForeignKey(\"subscription_instances.subscription_instance_id\", ondelete=\"CASCADE\"), primary_key=True\n    )\n    depends_on_id = mapped_column(\n        UUIDType, ForeignKey(\"subscription_instances.subscription_instance_id\", ondelete=\"CASCADE\"), primary_key=True\n    )\n    order_id = mapped_column(Integer(), primary_key=True)\n\n    # Needed to make sure subscription instance is populated in the right domain model attribute, if more than one\n    # attribute uses the same product block model.\n    domain_model_attr = Column(String(DOMAIN_MODEL_ATTR_LENGTH))\n\n    in_use_by: Mapped[SubscriptionInstanceTable] = relationship(\n        \"SubscriptionInstanceTable\", back_populates=\"depends_on_block_relations\", foreign_keys=[in_use_by_id]\n    )\n    depends_on: Mapped[SubscriptionInstanceTable] = relationship(\n        \"SubscriptionInstanceTable\", back_populates=\"in_use_by_block_relations\", foreign_keys=[depends_on_id]\n    )\n\n\nSubscriptionInstanceRelationTable.parent_id = SubscriptionInstanceRelationTable.in_use_by_id\nSubscriptionInstanceRelationTable.child_id = SubscriptionInstanceRelationTable.depends_on_id\n\nsubscription_relation_index = Index(\n    \"subscription_relation_i_d_o_ix\",\n    SubscriptionInstanceRelationTable.in_use_by_id,\n    SubscriptionInstanceRelationTable.depends_on_id,\n    SubscriptionInstanceRelationTable.order_id,\n    unique=True,\n)\n\n\nclass SubscriptionInstanceTable(BaseModel):\n    __tablename__ = \"subscription_instances\"\n\n    __allow_unmapped__ = True\n\n    subscription_instance_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    subscription_id = mapped_column(\n        UUIDType, ForeignKey(\"subscriptions.subscription_id\", ondelete=\"CASCADE\"), nullable=False, index=True\n    )\n    product_block_id = mapped_column(\n        UUIDType, ForeignKey(\"product_blocks.product_block_id\"), nullable=False, index=True\n    )\n    label = mapped_column(String(255))\n\n    subscription = relationship(\"SubscriptionTable\", back_populates=\"instances\", foreign_keys=[subscription_id])\n    product_block = relationship(\"ProductBlockTable\", lazy=\"subquery\")\n    values = relationship(\n        \"SubscriptionInstanceValueTable\",\n        lazy=\"subquery\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        order_by=\"asc(SubscriptionInstanceValueTable.value)\",\n        back_populates=\"subscription_instance\",\n    )\n    in_use_by_block_relations: Mapped[list[SubscriptionInstanceRelationTable]] = relationship(\n        \"SubscriptionInstanceRelationTable\",\n        lazy=\"subquery\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        back_populates=\"depends_on\",\n        foreign_keys=\"[SubscriptionInstanceRelationTable.depends_on_id]\",\n    )\n    depends_on_block_relations: Mapped[list[SubscriptionInstanceRelationTable]] = relationship(\n        \"SubscriptionInstanceRelationTable\",\n        lazy=\"subquery\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        order_by=SubscriptionInstanceRelationTable.order_id,\n        collection_class=ordering_list(\"order_id\"),\n        back_populates=\"in_use_by\",\n        foreign_keys=\"[SubscriptionInstanceRelationTable.in_use_by_id]\",\n    )\n\n    in_use_by = association_proxy(\n        \"in_use_by_block_relations\",\n        \"in_use_by\",\n        creator=lambda in_use_by: SubscriptionInstanceRelationTable(in_use_by=in_use_by),\n    )\n\n    depends_on = association_proxy(\n        \"depends_on_block_relations\",\n        \"depends_on\",\n        creator=lambda depends_on: SubscriptionInstanceRelationTable(depends_on=depends_on),\n    )\n\n    def value_for_resource_type(self, name: str | None) -&gt; SubscriptionInstanceValueTable | None:\n        return first_true(self.values, None, lambda x: x.resource_type.resource_type == name)  # type: ignore\n\n\nSubscriptionInstanceTable.parent_relations = SubscriptionInstanceTable.in_use_by_block_relations\nSubscriptionInstanceTable.children_relations = SubscriptionInstanceTable.depends_on_block_relations\n\nsubscription_instance_s_pb_ix = Index(\n    \"subscription_instance_s_pb_ix\",\n    SubscriptionInstanceTable.subscription_instance_id,\n    SubscriptionInstanceTable.subscription_id,\n    SubscriptionInstanceTable.product_block_id,\n)\n\n\nclass SubscriptionInstanceValueTable(BaseModel):\n    __tablename__ = \"subscription_instance_values\"\n\n    subscription_instance_value_id = mapped_column(\n        UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True\n    )\n    subscription_instance_id = mapped_column(\n        UUIDType,\n        ForeignKey(\"subscription_instances.subscription_instance_id\", ondelete=\"CASCADE\"),\n        index=True,\n        nullable=False,\n    )\n    resource_type_id = mapped_column(\n        UUIDType, ForeignKey(\"resource_types.resource_type_id\"), nullable=False, index=True\n    )\n    value = mapped_column(String(RESOURCE_VALUE_LENGTH), nullable=False)\n\n    resource_type = relationship(\"ResourceTypeTable\", lazy=\"subquery\")\n    subscription_instance = relationship(\"SubscriptionInstanceTable\", back_populates=\"values\")\n\n\nsiv_si_rt_ix = Index(\n    \"siv_si_rt_ix\",\n    SubscriptionInstanceValueTable.subscription_instance_value_id,\n    SubscriptionInstanceValueTable.subscription_instance_id,\n    SubscriptionInstanceValueTable.resource_type_id,\n)\n\n\nclass SubscriptionCustomerDescriptionTable(BaseModel):\n    __tablename__ = \"subscription_customer_descriptions\"\n    __table_args__ = (\n        UniqueConstraint(\"customer_id\", \"subscription_id\", name=\"uniq_customer_subscription_description\"),\n    )\n\n    id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    subscription_id = mapped_column(\n        UUIDType,\n        ForeignKey(\"subscriptions.subscription_id\", ondelete=\"CASCADE\"),\n        nullable=False,\n        index=True,\n    )\n    customer_id = mapped_column(String, nullable=False, index=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    created_at = mapped_column(UtcTimestamp, nullable=False, server_default=text(\"current_timestamp()\"))\n    version = mapped_column(Integer, nullable=False, server_default=\"1\")\n\n    subscription = relationship(\"SubscriptionTable\", back_populates=\"customer_descriptions\")\n\n\nclass SubscriptionTable(BaseModel):\n    __tablename__ = \"subscriptions\"\n\n    subscription_id = mapped_column(\n        UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True, nullable=False\n    )\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    status = mapped_column(String(STATUS_LENGTH), nullable=False, index=True)\n    product_id = mapped_column(UUIDType, ForeignKey(\"products.product_id\"), nullable=False, index=True)\n    customer_id = mapped_column(String, index=True, nullable=False)\n    insync = mapped_column(Boolean(), nullable=False)\n    start_date = mapped_column(UtcTimestamp, nullable=True)\n    end_date = mapped_column(UtcTimestamp)\n    note = mapped_column(StringThatAutoConvertsToNullWhenEmpty(NOTE_LENGTH))\n    version = mapped_column(Integer, nullable=False, server_default=\"1\")\n\n    product = relationship(\"ProductTable\", foreign_keys=[product_id])\n    instances = relationship(\n        \"SubscriptionInstanceTable\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        order_by=\"asc(SubscriptionInstanceTable.subscription_instance_id)\",\n        back_populates=\"subscription\",\n        foreign_keys=\"[SubscriptionInstanceTable.subscription_id]\",\n    )\n    customer_descriptions = relationship(\n        \"SubscriptionCustomerDescriptionTable\",\n        cascade=\"all, delete-orphan\",\n        passive_deletes=True,\n        back_populates=\"subscription\",\n    )\n    processes = relationship(\"ProcessSubscriptionTable\", back_populates=\"subscription\")\n\n    @staticmethod\n    def find_by_product_tag(tag: str) -&gt; SearchQuery:\n        return SubscriptionTable.query.join(ProductTable).filter(ProductTable.tag == tag)\n\n    def find_instance_by_block_name(self, name: str) -&gt; list[SubscriptionInstanceTable]:\n        return [instance for instance in self.instances if instance.product_block.name == name]\n\n    def find_values_for_resource_type(self, name: str | None) -&gt; list[SubscriptionInstanceValueTable]:\n        return list(filter(None, (instance.value_for_resource_type(name) for instance in self.instances)))\n\n    def product_blocks_with_values(self) -&gt; list[dict[str, list[dict[str, str]]]]:\n        return [\n            {instance.product_block.name: [{v.resource_type.resource_type: v.value} for v in instance.values]}\n            for instance in sorted(self.instances, key=lambda si: si.subscription_instance_id)\n        ]\n\n\nsubscription_product_ix = Index(\n    \"subscription_product_ix\", SubscriptionTable.subscription_id, SubscriptionTable.product_id\n)\nsubscription_customer_ix = Index(\n    \"subscription_customer_ix\", SubscriptionTable.subscription_id, SubscriptionTable.customer_id\n)\n\n\nclass SubscriptionMetadataTable(BaseModel):\n    __tablename__ = \"subscription_metadata\"\n    subscription_id = mapped_column(\n        UUIDType,\n        ForeignKey(\"subscriptions.subscription_id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n        index=True,\n    )\n    metadata_ = mapped_column(\"metadata\", pg.JSONB(), nullable=False)\n\n    @staticmethod\n    def find_by_subscription_id(subscription_id: str) -&gt; SubscriptionMetadataTable | None:\n        return SubscriptionMetadataTable.query.get(subscription_id)\n\n\nclass SubscriptionSearchView(BaseModel):\n    __tablename__ = \"subscriptions_search\"\n    __table_args__ = {\"info\": {\"materialized_view\": True}}\n\n    subscription_id = mapped_column(\n        UUIDType, ForeignKey(\"subscriptions.subscription_id\"), nullable=False, index=True, primary_key=True\n    )\n\n    tsv = deferred(mapped_column(TSVectorType))\n\n    subscription = relationship(\"SubscriptionTable\", foreign_keys=[subscription_id])\n\n\nclass AgentRunTable(BaseModel):\n    \"\"\"Agent conversation/session tracking.\"\"\"\n\n    __tablename__ = \"agent_runs\"\n\n    run_id = mapped_column(\"run_id\", UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    agent_type = mapped_column(String(50), nullable=False)\n    created_at = mapped_column(UtcTimestamp, server_default=text(\"current_timestamp()\"), nullable=False)\n\n    queries = relationship(\"SearchQueryTable\", back_populates=\"run\", cascade=\"delete\", passive_deletes=True)\n\n    __table_args__ = (Index(\"ix_agent_runs_created_at\", \"created_at\"),)\n\n\nclass SearchQueryTable(BaseModel):\n    \"\"\"Search query execution - used by both agent runs and regular API searches.\n\n    When run_id is NULL: standalone API search query\n    When run_id is NOT NULL: query belongs to an agent conversation run\n    \"\"\"\n\n    __tablename__ = \"search_queries\"\n\n    query_id = mapped_column(\"query_id\", UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    run_id = mapped_column(\n        \"run_id\", UUIDType, ForeignKey(\"agent_runs.run_id\", ondelete=\"CASCADE\"), nullable=True, index=True\n    )\n    query_number = mapped_column(Integer, nullable=False)\n\n    # Search parameters as JSONB (maps to BaseQuery subclasses)\n    parameters = mapped_column(pg.JSONB, nullable=False)\n\n    # Query embedding for semantic search (pgvector)\n    query_embedding = mapped_column(Vector(llm_settings.EMBEDDING_DIMENSION), nullable=True)\n\n    executed_at = mapped_column(UtcTimestamp, server_default=text(\"current_timestamp()\"), nullable=False)\n\n    run = relationship(\"AgentRunTable\", back_populates=\"queries\")\n\n    __table_args__ = (\n        Index(\"ix_search_queries_run_id\", \"run_id\"),\n        Index(\"ix_search_queries_executed_at\", \"executed_at\"),\n        Index(\"ix_search_queries_query_id\", \"query_id\"),\n    )\n\n    @classmethod\n    def from_state(\n        cls,\n        state: \"QueryState\",\n        run_id: \"UUID | None\" = None,\n        query_number: int = 1,\n    ) -&gt; \"SearchQueryTable\":\n        \"\"\"Create a SearchQueryTable instance from a QueryState.\n\n        Args:\n            state: QueryState wrapping the query and embedding\n            run_id: Optional agent run ID (NULL for regular API searches)\n            query_number: Query number within the run (default=1)\n\n        Returns:\n            SearchQueryTable instance ready to be added to the database.\n        \"\"\"\n        return cls(\n            run_id=run_id,\n            query_number=query_number,\n            parameters=state.query.model_dump(),\n            query_embedding=state.query_embedding,\n        )\n\n\nclass EngineSettingsTable(BaseModel):\n    __tablename__ = \"engine_settings\"\n    global_lock = mapped_column(Boolean(), default=False, nullable=False, primary_key=True)\n    running_processes = mapped_column(Integer(), default=0, nullable=False)\n    __table_args__: tuple = (CheckConstraint(running_processes &gt;= 0, name=\"check_running_processes_positive\"), {})\n\n\nclass SubscriptionInstanceAsJsonFunction(GenericFunction):\n    # Added in migration 42b3d076a85b\n    name = \"subscription_instance_as_json\"\n\n    type = pg.JSONB()\n    inherit_cache = True\n\n    def __init__(self, sub_inst_id: UUID):\n        super().__init__(sub_inst_id)\n\n\nclass AiSearchIndex(BaseModel):\n\n    __tablename__ = \"ai_search_index\"\n\n    entity_type = mapped_column(\n        TEXT,\n        nullable=False,\n        index=True,\n    )\n    entity_id = mapped_column(\n        UUIDType,\n        nullable=False,\n    )\n    entity_title = mapped_column(TEXT, nullable=True)\n\n    # Ltree path for hierarchical data\n    path = mapped_column(LtreeType, nullable=False, index=True)\n    value = mapped_column(TEXT, nullable=False)\n\n    value_type = mapped_column(\n        Enum(FieldType, name=\"field_type\", values_callable=lambda obj: [e.value for e in obj]), nullable=False\n    )\n\n    # Embedding\n    embedding = mapped_column(Vector(llm_settings.EMBEDDING_DIMENSION), nullable=True)\n\n    # SHA-256\n    content_hash = mapped_column(String(64), nullable=False, index=True)\n\n    __table_args__ = (PrimaryKeyConstraint(\"entity_id\", \"path\", name=\"pk_ai_search_index\"),)\n\n\nclass APSchedulerJobStoreModel(BaseModel):\n    __tablename__ = \"apscheduler_jobs\"\n\n    id = mapped_column(String(191), primary_key=True)\n    next_run_time = mapped_column(Float, nullable=True)\n    job_state = mapped_column(LargeBinary, nullable=False)\n\n\nclass WorkflowApschedulerJob(BaseModel):\n    __tablename__ = \"workflows_apscheduler_jobs\"\n\n    workflow_id = mapped_column(\n        UUIDType, ForeignKey(\"workflows.workflow_id\", ondelete=\"CASCADE\"), primary_key=True, nullable=False\n    )\n\n    # Notice the VARCHAR(512) for schedule_id to accommodate longer IDs so\n    # that if APScheduler changes its ID format in the future, we are covered.\n    schedule_id = mapped_column(\n        String(512), ForeignKey(\"apscheduler_jobs.id\", ondelete=\"CASCADE\"), primary_key=True, nullable=False\n    )\n\n    __table_args__ = (UniqueConstraint(\"workflow_id\", \"schedule_id\", name=\"uq_workflow_schedule\"),)\n</code></pre>"},{"location":"reference-docs/database/#setting-up-the-database-initially","title":"Setting Up the Database Initially","text":"<p>With a blank WFO instance, to setup the database properly, you simply need to run the <code>init</code> CLI command. More docs on how to use that command can be found here.</p>"},{"location":"reference-docs/database/#saving-a-transaction-in-your-workflow","title":"Saving a Transaction in Your Workflow","text":"<p>An important thing to understand about interacting with the database inside of workflow steps is that saving to the database in disabled during the workflow step. When a subsription is returned at the end of a step, then all of the appropriate saving in the database occurs. You can see how we do this with the <code>WrappedSession</code> class we made around the SQLAlchemy <code>Session</code> object:</p>"},{"location":"reference-docs/database/#orchestrator.db.database.WrappedSession","title":"orchestrator.db.database.WrappedSession","text":"<p>               Bases: <code>sqlalchemy.orm.Session</code></p> <p>This Session class allows us to disable commit during steps.</p> Source code in <code>orchestrator/db/database.py</code> <pre><code>class WrappedSession(Session):\n    \"\"\"This Session class allows us to disable commit during steps.\"\"\"\n\n    def commit(self) -&gt; None:\n        if self.info.get(\"disabled\", False):\n            self.info.get(\"logger\", logger).warning(\n                \"Step function tried to issue a commit. It should not! \"\n                \"Will execute commit on behalf of step function when it returns.\"\n            )\n        else:\n            super().commit()\n</code></pre>"},{"location":"reference-docs/database/#multiple-heads","title":"Multiple Heads","text":"<p>When you have multiple features in flight at a time with your WFO development process, you might come across this error when starting up your WFO instance, especially after performing <code>git</code> merges:</p> <pre><code>Only a single head is supported. The script directory has multiple heads (due branching), which must be resolved by manually editing the revision files to form a linear sequence.\nRun `alembic branches` to see the divergence(s).\n</code></pre> <p>Thankfully alembic is great at handling this and you can use the WFO CLI <code>db merge</code> command to resolve this, documented in depth here.</p>"},{"location":"reference-docs/forms/","title":"Pydantic Forms","text":"<p>In the orchestrator core, the forms a user will use in the web UI are created by defining your form input elements as a class that is a subclass of the <code>FormPage</code> class in the core. This allows you to write your form data for your WFO instance as simple python classes, but still have a useful form rendered in the frontend Web UI. This is achieved by using the library pydantic-forms. One benefit of this model is that it lets WFO developers not have to understand much about frontend technologies to build complex form structures. It's definitely worth poking around in that module to see the various input types the core library exposes.</p>"},{"location":"reference-docs/forms/#form-examples","title":"Form Examples","text":""},{"location":"reference-docs/forms/#importing-form-elements","title":"Importing Form Elements","text":"<p>To use the pydantic forms, you must import the <code>FormPage</code> class:</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\n</code></pre> <p>Additionally, the validators module exposes validators that also function as \"input type widgets\". Here is an example of importing some of the built-in validators for use in a workflow:</p> <pre><code>from orchestrator.forms.validators import CustomerId, choice_list, Choice\n</code></pre> <p>You can, of course, define you own validators as well simply by using pydantic validators, as described in the pydantic documentation.</p>"},{"location":"reference-docs/forms/#writing-forms","title":"Writing Forms","text":"<p>Writing forms is quite straight forward, here is a relatively simple input form:</p> <pre><code>equipment = get_planned_equipment() # grab available equipment from inventory system\nchoices = [f\"{eq['name']}\" for eq in equipment]\n\nEquipmentList = choice_list(\n    Choice(\"EquipmentEnum\", zip(choices, choices)),\n    min_items=1,\n    max_items=1,\n    unique_items=True,\n)\n\n\nclass CreateNodeEnrollmentForm(FormPage):\n    class Config:\n        title = product_name\n\n    customer_id: CustomerId = ReadOnlyField(CustomerId(DEFAULT_ORG_UUID))\n    select_node_choice: EquipmentList\n\n\n# Don't call like this CreateNodeEnrollmentForm() or you'll\n# get a vague error.\nuser_input = yield CreateNodeEnrollmentForm\n</code></pre> <p>The only real data gathered with this form are <code>customer_id</code>, which is read-only, so the user cannot modify it, and the form exposes a list of devices pulled from a network inventory system for the user to choose from.</p>"},{"location":"reference-docs/forms/#choice-widgets","title":"Choice Widgets","text":"<p>Of note: <code>min_items</code> and <code>max_items</code> do not refer to the number of elements in the list. This UI construct allows for an arbitrary number of choices to be made. There are <code>+</code> and <code>-</code> options exposed in the UI allowing for multiple choices selected by the user. So <code>min 1 / max 1</code> tells the UI to display one pull down list of choices one of which must be selected, and additional choices can not be added.</p> <p>If one defined something like <code>min 1 / max 3</code> it would display one pulldown box by default and expose a <code>+</code> element in the UI. The user could click on it to arbitrarily add a second or a third pulldown list. <code>min 0</code> would not display any list by default but the user could use <code>+</code> to add some, etc.</p> <p>Since multiple choices are allowed, the results are returned as a list even if there is only a single choice element:</p> <pre><code>eq_name = user_input.select_node_choice[0]\n</code></pre> <p>The <code>zip()</code> maneuver takes the list and makes it into a dict with the same keys and values so that the display text doesn't have to be the same as the value returned.</p>"},{"location":"reference-docs/forms/#accept-actions","title":"Accept Actions","text":"<p>Confirming actions is a common bit of functionality. This bit of code displays a read only payload from an external system, lets the user check that value and then approve that the payload is correct as a form of a dry run:</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\nfrom orchestrator.forms.validators import Accept, LongText\n\n\ndef confirm_dry_run_results(dry_run_results: str) -&gt; State:\n    class ConfirmDryRun(FormPage):\n        nso_dry_run_results: LongText = ReadOnlyField(dry_run_results)\n        confirm_dry_run_results: Accept\n\n    user_input = yield ConfirmDryRun\n\n    return user_input\n</code></pre>"},{"location":"reference-docs/forms/#generic-python-types","title":"Generic Python Types","text":"<p>It is possible to mix generic python types in the with the defined validation fields:</p> <pre><code>class CreateLightPathForm(FormPage):\n    class Config:\n        title = product_name\n\n    customer_id: CustomerId\n    contact_persons: ContactPersonList = []  # type: ignore\n    ticket_id: JiraTicketId = \"\"  # type: ignore\n    service_ports: ListOfTwo[ServicePort]  # type: ignore\n    service_speed: bandwidth(\"service_ports\")  # type: ignore # noqa: F821\n    speed_policer: bool = False\n    remote_port_shutdown: bool = True\n</code></pre>"},{"location":"reference-docs/forms/#multi-step-form-input","title":"Multi-step Form Input","text":"<p>To do a multistep form in your workflow, you simply yield multiple times and then combine the results at the end:</p> <pre><code>def initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class CreateNodeForm(FormPage):\n        class Config:\n            title = product_name\n\n        customer_id: CustomerId = ReadOnlyField(CustomerId(SURFNET_NETWORK_UUID))\n        location_code: LocationCode\n        ticket_id: JiraTicketId = \"\"  # type:ignore\n\n    user_input = yield CreateNodeForm\n\n    class NodeIdForm(SubmitFormPage):\n        class Config:\n            title = product_name\n\n        ims_node_id: ims_node_id(\n            user_input.location_code, node_status=\"PL\"\n        )  # type:ignore # noqa: F821\n\n    user_input_node = yield NodeIdForm\n\n    return {**user_input.dict(), **user_input_node.dict()}\n</code></pre> <p>For multistep forms especially, it can be useful to use the <code>orchestrator.forms.SubmitFormPage</code> class, which is just a subclass of <code>orchestrator.forms.FormPage</code> that has some metadata informing the frontend that this form is the last page in the flow so it can style the submit button differently. This is entirely optional.</p>"},{"location":"reference-docs/forms/#custom-form-fields","title":"Custom Form Fields","text":"<p>You can create a custom field component in the frontend. The components in <code>orchestrator-gui/src/lib/uniforms-surfnet/src</code> can be used to study reference implementations for a couple of custom form field types.</p> <p>For it to show up in the form, you have to do 2 things, a pydantic type/class in the backend and add the component to the <code>AutoFieldLoader.tsx</code>.</p> <p>as an example I will create a custom field with name field and group select field.</p>"},{"location":"reference-docs/forms/#pydantic-typeclass-in-backend","title":"Pydantic Type/Class In Backend","text":"<p>Create a pydantic type/class.</p> <pre><code>from uuid import UUID\n\n\nclass ChooseUser(str):\n    group_id: UUID  # type:ignore\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict[str, Any]) -&gt; None:\n        uniforms: dict[str, Any] = {}\n\n        if cls.group_id:\n            uniforms[\"groupId\"] = cls.group_id\n\n        field_schema.update(format=\"ChooseUser\", uniforms=uniforms)\n</code></pre> <p>And add it to a form:</p> <pre><code>def initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class ChoseUserForm(FormPage):\n        class Config:\n            title = product_name\n\n        user: ChooseUser\n\n    user_input = yield ChoseUserForm\n</code></pre> <p>To prefill the user_id, you need to add the value to the prop, for prefilling the group_id you need to create a new class:</p> <pre><code>def user_choice(group_id: int | None = None) -&gt; type:\n    namespace = {\"group_id\": group_id}\n    return new_class(\n        \"ChooseUserValue\", (ChooseUser,), {}, lambda ns: ns.update(namespace)\n    )\n\n\ndef initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class ChoseUserForm(FormPage):\n        class Config:\n            title = product_name\n\n        user: user_choice(\"group_id_1\") = \"user_id_1\"\n\n    user_input = yield ChoseUserForm\n</code></pre>"},{"location":"reference-docs/forms/#auto-field-loader","title":"Auto Field Loader","text":"<p>The auto field loader is for loading the correct field component in the form. It has switches that check the field type and the field format. You have to add your new form field here.</p> <p>for this example, we would need to add to a <code>ChooseUser</code> case to the String switch:</p> <pre><code>...\nimport ChooseUserField from \"custom/uniforms/ChooseUserField\";\n\nexport function autoFieldFunction(props: GuaranteedProps&lt;unknown&gt; &amp; Record&lt;string, any&gt;, uniforms: Context&lt;unknown&gt;) {\n    const { allowedValues, checkboxes, fieldType, field } = props;\n    const { format } = field;\n\n    switch (fieldType) {\n        ...\n        case String:\n            switch (format) {\n               ...\n                case \"ChooseUser\":\n                    return ChooseUserField;\n            }\n            break;\n    }\n\n    ...\n}\n</code></pre>"},{"location":"reference-docs/forms/#custom-field-example","title":"Custom Field Example","text":"<p>example custom field to select a user by group.</p> <pre><code>import { EuiFlexItem, EuiFormRow, EuiText } from \"@elastic/eui\";\nimport { FieldProps } from \"lib/uniforms-surfnet/src/types\";\nimport React, { useCallback, useContext, useEffect, useState } from \"react\";\nimport { WrappedComponentProps, injectIntl } from \"react-intl\";\nimport ReactSelect, { SingleValue } from \"react-select\";\nimport { getReactSelectTheme } from \"stylesheets/emotion/utils\";\nimport { connectField, filterDOMProps } from \"uniforms\";\nimport ApplicationContext from \"utils/ApplicationContext\";\nimport { Option } from \"utils/types\";\nimport { css } from \"@emotion/core\";\n\nexport const ChoosePersonFieldStyling = css`\n    section.group-user {\n        display: flex;\n        flex-direction: row;\n        flex-wrap: wrap;\n\n        div.group-select {\n            width: 50%;\n        }\n        div.user-select {\n            width: 50%;\n            padding-left: 5px;\n        }\n    }\n`;\n\ninterface Group {\n    id: string;\n    name: string;\n}\n\ninterface User {\n    id: string;\n    name: string;\n    age: number;\n}\n\n\nexport type ChooseUserFieldProps = FieldProps&lt;\n    string,\n    {\n        groupId?: string;\n    } &amp; WrappedComponentProps\n&gt;;\n\nconst groupToOption = (group: Group): Option =&gt; {\n    return {\n        value: group.id,\n        label: `${group.id.substring(0, 8)} ${group.name}`,\n    };\n}\n\nconst userToOption = (user: User): Option =&gt; {\n    return {\n        value: user.id,\n        label: `${user.name} (${user.age})`,\n    };\n}\n\ndeclare module \"uniforms\" {\n    interface FilterDOMProps {\n        groupId: never;\n    }\n}\nfilterDOMProps.register(\"groupId\");\n\nfunction ChoosePerson({\n    id,\n    name,\n    label,\n    description,\n    onChange,\n    value,\n    disabled,\n    placeholder,\n    readOnly,\n    error,\n    showInlineError,\n    errorMessage,\n    groupId,\n    intl,\n    ...props\n}: ChooseUserFieldProps) {\n    const { apiClient, customApiClient, theme } = useContext(ApplicationContext);\n\n    const [groups, setGroups] = useState&lt;Group[]&gt;([]);\n    const [selectedGroupId, setGroupId] = useState&lt;number | string | undefined&gt;(groupId);\n    const [users, setUsers] = useState&lt;User[]&gt;([]);\n    const [loading, setLoading] = useState(true);\n\n    const onChangeGroup = useCallback(\n        (option: SingleValue&lt;Option&gt;) =&gt; {\n            let value = option?.value;\n            if (value === undefined) return;\n\n            setLoading(true);\n            setGroupId(value);\n            setUsers([]);\n\n            // do api call to get users by group id and set users with the fetched data.\n            setTimeout(() =&gt; {\n                let users = [{ id: \"user_id_1\", name: \"user 1\", age: 25 }, { id: \"user_id_2\", name: \"user 2\", age: 30 }]\n                if (value == \"group_id_2\") {\n                    users = [{ id: \"user_id_3\", name: \"user 3\", age: 35 }]\n                } else if (value == \"group_id_3\") {\n                    users = [{ id: \"user_id_4\", name: \"user 4\", age: 40 }, { id: \"user_id_5\", name: \"user 5\", age: 45 }]\n                }\n                setUsers(users)\n                setLoading(false);\n            }, 1000)\n        },\n        [customApiClient]\n    );\n\n    useEffect(() =&gt; {\n        setLoading(true);\n\n        // do api call to get groups for the first select.\n\n        setTimeout(() =&gt; {\n            setGroups([\n                { id: \"group_id_1\", name: \"group 1\" },\n                { id: \"group_id_2\", name: \"group 2\" },\n                { id: \"group_id_3\", name: \"group 3\" }\n            ]);\n            setLoading(false);\n\n            if (groupId) {\n                onChangeGroup({ value: groupId } as Option);\n            }\n        }, 1000)\n    }, [onChangeGroup, apiClient, groupId]);\n\n    // use i18n translations.\n    const groupsPlaceholder = loading ? \"Loading...\" : \"Select a group\";\n    const userPlaceholder = loading ? \"Loading...\" : selectedGroupId ? \"Select a user\" : \"Select a group first\";\n\n    const group_options: Option[] = (groups as Group[])\n        .map(groupToOption)\n        .sort((x, y) =&gt; x.label.localeCompare(y.label));\n    const group_value = group_options.find((option) =&gt; option.value === selectedGroupId?.toString());\n\n    const user_options: Option&lt;string&gt;[] = users\n        .map(userToOption)\n        .sort((x, y) =&gt; x.label.localeCompare(y.label));\n    const user_value = user_options.find((option) =&gt; option.value === value);\n\n    const customStyles = getReactSelectTheme(theme);\n\n    return (\n        &lt;EuiFlexItem css={ChoosePersonFieldStyling}&gt;\n            &lt;section {...filterDOMProps(props)}&gt;\n                &lt;EuiFormRow\n                    label={label}\n                    labelAppend={&lt;EuiText size=\"m\"&gt;{description}&lt;/EuiText&gt;}\n                    error={showInlineError ? errorMessage : false}\n                    isInvalid={error}\n                    id={id}\n                    fullWidth\n                &gt;\n                    &lt;section className=\"group-user\"&gt;\n                        &lt;div className=\"group-select\"&gt;\n                            &lt;EuiFormRow label=\"Group\" id={`${id}.group`} fullWidth&gt;\n                                &lt;ReactSelect&lt;Option, false&gt;\n                                    inputId={`${id}.group.search`}\n                                    name={`${name}.group`}\n                                    onChange={onChangeGroup}\n                                    options={group_options}\n                                    placeholder={groupsPlaceholder}\n                                    value={group_value}\n                                    isSearchable={true}\n                                    isDisabled={disabled || readOnly || groups.length === 0}\n                                    styles={customStyles}\n                                /&gt;\n                            &lt;/EuiFormRow&gt;\n                        &lt;/div&gt;\n                        &lt;div className=\"user-select\"&gt;\n                            &lt;EuiFormRow label=\"User\" id={id} fullWidth&gt;\n                                &lt;ReactSelect&lt;Option&lt;string&gt;, false&gt;\n                                    inputId={`${id}.search`}\n                                    name={name}\n                                    onChange={(selected) =&gt; {\n                                        onChange(selected?.value);\n                                    }}\n                                    options={user_options}\n                                    placeholder={userPlaceholder}\n                                    value={user_value}\n                                    isSearchable={true}\n                                    isDisabled={disabled || readOnly || users.length === 0}\n                                    styles={customStyles}\n                                /&gt;\n                            &lt;/EuiFormRow&gt;\n                        &lt;/div&gt;\n                    &lt;/section&gt;\n                &lt;/EuiFormRow&gt;\n            &lt;/section&gt;\n        &lt;/EuiFlexItem&gt;\n    );\n}\n\nexport default connectField(injectIntl(ChoosePerson), { kind: \"leaf\" });\n</code></pre>"},{"location":"reference-docs/graphql/","title":"GraphQL documentation","text":"<p>The <code>orchestrator-core</code> comes with a GraphQL interface that can to be registered after you create your OrchestratorApp. If you add it after registering your <code>SUBSCRIPTION_MODEL_REGISTRY</code> it will automatically create GraphQL types for them.</p> <p>Example:</p> <pre><code>app = OrchestratorCore(base_settings=AppSettings())\n# register SUBSCRIPTION_MODEL_REGISTRY\napp.register_graphql()\n</code></pre>"},{"location":"reference-docs/graphql/#how-we-use-strawberry-for-graphql","title":"How we use Strawberry for GraphQL","text":""},{"location":"reference-docs/graphql/#what-is-strawberry","title":"What is Strawberry?","text":"<p>Strawberry is a Python library for building GraphQL APIs using a code-first approach. It allows you to define your GraphQL schema using Python classes and type annotations.</p> <p>Here is a simple example of how Strawberry create's a schema:</p> <pre><code>import typing\nimport strawberry\n\n\n@strawberry.type\nclass Book:\n    title: str\n    author: \"Author\"\n\n\n@strawberry.type\nclass Author:\n    name: str\n    books: typing.List[Book]\n\n\nschema = strawberry.Schema(query=Query)\n</code></pre> <p>The schema defines a hierarchy of types with fields that are populated from data stores. The schema also specifies exactly which queries and mutations are available for clients to execute.</p>"},{"location":"reference-docs/graphql/#how-do-we-use-strawberry-graphql-in-the-orchestrator-core","title":"How do we use Strawberry GraphQL in the <code>orchestrator-core</code>?","text":"<p>Strawberry GraphQL is used to define and expose a GraphQL API for orchestrating products, subscriptions, and related entities. Here\u2019s a brief overview of how GraphQL is used:</p> <ul> <li> <p>Schema Definition: GraphQL types, interfaces, and inputs are defined using Strawberry decorators (e.g., <code>@strawberry.field</code>, <code>@strawberry.type</code>, <code>@strawberry.interface</code>, <code>@strawberry.input</code>). These are mapped to Pydantic models and SQLAlchemy tables for type safety and data validation.</p> </li> <li> <p>Resolvers: Resolver functions (e.g., <code>resolve_subscriptions</code>, <code>resolve_products</code>) are implemented to fetch and return data from the database, often using SQLAlchemy queries. These resolvers are attached to fields in the schema.</p> </li> <li> <p>Pagination, Filtering, Sorting: The API supports pagination, filtering, and sorting for list queries, using custom types like <code>GraphqlFilter</code>, <code>GraphqlSort</code>, and a <code>Connection</code> type for paginated results.</p> </li> <li> <p>Federation: Some types use Strawberry\u2019s federation features (e.g., <code>@strawberry.federation.interface</code>) to support a federated GraphQL architecture.</p> </li> </ul> <p>Federation</p> <p>Federation allows you to combine multiple, distributed GraphQL services into one unified API. This is extremely useful when working with multiple services, as it enables you to develop, deploy, and scale GraphQL services independently while presenting a single schema to clients.</p>"},{"location":"reference-docs/graphql/#how-to-enable-federation-in-the-orchestrator-core","title":"How to enable Federation in the <code>orchestrator-core</code>?","text":"<ul> <li>Set <code>FEDERATION_ENABLED=True</code> in your <code>.env</code> file</li> <li>See the <code>example-orchestrator</code> documentation for a detailed example of setting up federation with the orchestrator-core and other backend services.</li> </ul>"},{"location":"reference-docs/graphql/#extending-the-query-and-mutation","title":"Extending the Query and Mutation","text":"<p>You are not able to remove resolvers from a Query, so we split the Query into 2 and merged them back for a default Query. Our usecase for this is that we use an external GraphQL source as our customers root.</p> <ul> <li><code>OrchestratorQuery</code> all resolvers except for customers.</li> <li><code>CustomerQuery</code> only has <code>customers</code> resolver.</li> <li><code>Query</code> Merges of <code>OrchestratorQuery</code> and <code>CustomerQuery</code> and serves as the default.</li> </ul> <p>This is an basic example of how to extend the query. You can do the same to extend Mutation.</p> <pre><code>from orchestrator.graphql import Query, Mutation, OrchestratorQuery\n\n\n# Queries\ndef resolve_new(info) -&gt; str:\n    return \"resolve new...\"\n\n\n# with customers.\n@strawberry.federation.type(description=\"Orchestrator queries\")\nclass NewQuery(Query):\n    other_processes: Connection[ProcessType] = authenticated_field(\n        resolver=resolve_processes,\n        description=\"resolve_processes used for another field\",\n    )\n    new: str = strawberry.field(resolve_new, description=\"new resolver\")\n\n# without customers.\n@strawberry.federation.type(description=\"Orchestrator queries\")\nclass NewQueryWithoutCustomers(OrchestratorQuery):\n    other_processes: Connection[ProcessType] = authenticated_field(\n        resolver=resolve_processes,\n        description=\"resolve_processes used for another field\",\n    )\n    new: str = strawberry.field(resolve_new, description=\"new resolver\")\n\n\napp = OrchestratorCore(base_settings=AppSettings())\n# register SUBSCRIPTION_MODEL_REGISTRY\napp.register_graphql(query=NewQuery)\n</code></pre>"},{"location":"reference-docs/graphql/#adding-federated-types-to-the-graphql","title":"Adding federated types to the GraphQL","text":"<p>For an introduction to federation using Strawberry, see the Strawberry federation docs.</p> <p>Within a federation, it is possible to add orchestrator data to GraphQL types from other sources by extending the <code>DEFAULT_GRAPHL_MODELS</code> dictionary with your own federated classes and adding them as parameter to <code>app.register_graphql(graphql_models={})</code>.</p> <p>Here is an example for when instead of overriding the customers resolver, you instead use a different GraphQL source (know that not storing any customer data in the orchestator will make filtering and sorting unavailable and very tricky to implement):</p> <pre><code>import strawberry\nfrom sqlalchemy import select\n\nfrom oauth2_lib.strawberry import authenticated_field\nfrom orchestrator.db import db\nfrom orchestrator.graphql.pagination import Connection\nfrom orchestrator.graphql.schemas.subscription import SubscriptionInterface\nfrom orchestrator.graphql.types import GraphqlFilter, GraphqlSort, OrchestratorInfo\n\n\n@strawberry.federation.type(description=\"Customer\", keys=[\"customer_id\"])\nclass Customer:\n    customer_id: str\n\n    @classmethod\n    async def resolve_reference(cls, customer_id: str) -&gt; \"Customer\":  # noqa: N803\n        return Customer(customer_id=customer_id)\n\n    @authenticated_field(description=\"Returns subscriptions of a customer\")  # type: ignore\n    async def subscriptions(\n        self,\n        info: OrchestratorInfo,\n        filter_by: list[GraphqlFilter] | None = None,\n        sort_by: list[GraphqlSort] | None = None,\n        first: int = 10,\n        after: int = 0,\n    ) -&gt; Connection[SubscriptionInterface]:\n        from orchestrator.graphql.resolvers.subscription import resolve_subscriptions\n\n        filter_by_customer_id = (filter_by or []) + [GraphqlFilter(field=\"customerId\", value=str(self.uuid))]  # type: ignore\n        return await resolve_subscriptions(info, filter_by_customer_id, sort_by, first, after)\n\nUPDATED_GRAPHQL_MODELS = DEFAULT_GRAPHQL_MODELS | {\n    \"Customer\": Customer,\n}\n\napp.register_graphql(query=OrchestratorQuery, graphql_models=UPDATED_GRAPHQL_MODELS)\n</code></pre> <p>Types that are added in this way but aren't used in a resolver, will be viewable outside of a federation inside the types in the GraphQL UI interface.</p> <p>Adding product or product block Strawberry types to the <code>graphql_models</code> will skip their generation inside <code>register_domain_models</code>. More info here</p>"},{"location":"reference-docs/graphql/#add-json-schema-for-metadata","title":"Add JSON schema for metadata","text":"<p>The metadata in a subscription is completely unrestricted and can have anything. This functionality is to make metadata descriptive in a <code>__schema__</code> for the frontend to be able to render the metadata and know what to do with typing.</p> <p>example how to update the <code>__schema__</code>:</p> <pre><code>from orchestrator.graphql.schemas.subscription import MetadataDict\n\n\nclass Metadata(BaseModel):\n    some_metadata_prop: list[str]\n\n\nMetadataDict.update({\"metadata\": Metadata})\n</code></pre> <p>This will result in json schema:</p> <pre><code>{\n    \"title\": \"Metadata\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"some_metadata_prop\": {\n            \"title\": \"Some Metadata Prop\",\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"string\"\n            }\n        }\n    },\n    \"required\": [\n        \"some_metadata_prop\"\n    ]\n}\n</code></pre>"},{"location":"reference-docs/graphql/#domain-models-auto-registration-for-graphql","title":"Domain Models Auto Registration for GraphQL","text":"<p>When using the <code>app.register_graphql()</code> function, all products in the <code>SUBSCRIPTION_MODEL_REGISTRY</code> will be automatically converted into GraphQL types. You are able to turn this off with <code>app.register_graphql(register_models=False)</code>, but then you can only query fields from the default <code>SubscriptionModel</code>. The registration process iterates through the list, starting from the deepest product block and working its way back up to the product level.</p> <p>However, there is a potential issue when dealing with a <code>ProductBlock</code> that references itself, as it could lead to an error expecting the <code>ProductBlock</code> type to exist.</p> <p>Here is an example of the expected error with a self referenced <code>ProductBlock</code>:</p> <pre><code>strawberry.experimental.pydantic.exceptions.UnregisteredTypeException: Cannot find a Strawberry Type for &lt;class 'products.product_blocks.product_block_file.ProductBlock'&gt; did you forget to register it?\n</code></pre> <p>To handle this situation, you must manually create the GraphQL type for that <code>ProductBlock</code> and add it to the <code>DEFAULT_GRAPHQL_MODELS</code> list.</p> <p>Here's an example of how to do it:</p> <pre><code># product_block_file.py\nimport strawberry\nfrom typing import Annotated\nfrom app.product_blocks import ProductBlock\nfrom orchestrator.graphql import DEFAULT_GRAPHQL_MODELS\n\n\n# It is necessary to use pydantic type, so that other product blocks can recognize it when typing to GraphQL.\n@strawberry.experimental.pydantic.type(model=ProductBlock)\nclass ProductBlockGraphql:\n    name: strawberry.auto\n    self_reference_block: Annotated[\n        \"ProductBlockGraphql\", strawberry.lazy(\".product_block_file\")\n    ] | None = None\n    ...\n\n\n# Add the ProductBlockGraphql type to GRAPHQL_MODELS, which skips its auto-register and used for products or product blocks dependant on it.\nUPDATED_GRAPHQL_MODELS = DEFAULT_GRAPHQL_MODELS | {\n    \"ProductBlockGraphql\": ProductBlockGraphql,\n}\n\napp.register_graphql(query=OrchestratorQuery, graphql_models=UPDATED_GRAPHQL_MODELS)\n</code></pre> <p>By following this example, you can effectively create the necessary GraphQL type for <code>ProductBlock</code> and ensure proper registration with <code>app.register_graphql()</code>. This will help you avoid any <code>Cannot find a Strawberry Type</code> scenarios and enable smooth integration of domain models with GraphQL.</p>"},{"location":"reference-docs/graphql/#scalars-for-auto-registration","title":"Scalars for Auto Registration","text":"<p>When working with special types such as <code>VlanRanges</code> or <code>IPv4Interface</code> in the core module, scalar types are essential for the auto registration process. Scalar types enable smooth integration of these special types into the GraphQL schema, They need to be initialized and can be added with a dict to <code>app.register_graphql(scalar_overrides={})</code>.</p> <p>Here's an example of how to add a new scalar:</p> <pre><code>import strawberry\nfrom typing import NewType\nfrom orchestrator.graphql import SCALAR_OVERRIDES\n\nVlanRangesType = strawberry.scalar(\n    NewType(\"VlanRangesType\", str),\n    description=\"Represent the Orchestrator VlanRanges data type\",\n    serialize=lambda v: v.to_list_of_tuples(),\n    parse_value=lambda v: v,\n)\n\n# Add the scalar to the SCALAR_OVERRIDES dictionary, with the type in the product block as the key and the scalar as the value\nUPDATED_SCALAR_OVERRIDES = SCALAR_OVERRIDES | {\n    VlanRanges: VlanRangesType,\n}\n\napp.register_graphql(other_params..., scalar_overrides=UPDATED_SCALAR_OVERRIDES)\n</code></pre> <p>You can find more examples of scalar usage in the <code>orchestrator/graphql/types.py</code> file. For additional information on Scalars, please refer to the Strawberry documentation on Scalars: https://strawberry.rocks/docs/types/scalars.</p> <p>By using scalar types for auto registration, you can seamlessly incorporate special types into your GraphQL schema, making it easier to work with complex data in the Orchestrator application.</p>"},{"location":"reference-docs/graphql/#federating-with-autogenerated-types","title":"Federating with Autogenerated Types","text":"<p>To enable federation, set the <code>FEDERATION_ENABLED</code> environment variable to <code>True</code>.</p> <p>Info</p> <p>The dockerized example-orchestrator contains a working Federation setup that demonstrates how the below works in practice.</p> <p>Federation allows you to federate with subscriptions using the <code>subscriptionId</code> and with product blocks inside the subscription by utilizing any property that includes <code>_id</code> in its name.</p> <p>Below is an example of a GraphQL app that extends the <code>SubscriptionInterface</code>:</p> <pre><code>from typing import Any\n\nimport strawberry\nfrom starlette.applications import Starlette\nfrom starlette.routing import Route\nfrom strawberry.asgi import GraphQL\nfrom uuid import UUID\n\n\n@strawberry.federation.interface_object(keys=[\"subscriptionId\"])\nclass SubscriptionInterface:\n    subscription_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"SubscriptionInterface\":\n        if not (subscription_id := data.get(\"subscriptionId\")):\n            raise ValueError(\n                f\"Need 'subscriptionId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = new_value_resolver(subscription_id)\n        return SubscriptionInterface(subscription_id=subscription_id, new_value=value)\n\n\n@strawberry.type\nclass Query:\n    hi: str = strawberry.field(resolver=lambda: \"query for other graphql\")\n\n\n# Add `SubscriptionInterface` in types array.\nschema = strawberry.federation.Schema(\n    query=Query,\n    types=[SubscriptionInterface],\n    enable_federation_2=True,\n)\n\napp = Starlette(debug=True, routes=[Route(\"/\", GraphQL(schema, graphiql=True))])\n</code></pre> <p>To run this example, execute the following command:</p> <pre><code>uvicorn app:app --port 4001 --host 0.0.0.0 --reload\n</code></pre> <p>In the <code>supergraph.yaml</code> file, you can federate the GraphQL endpoints together as shown below:</p> <pre><code>federation_version: 2\nsubgraphs:\n  orchestrator:\n    routing_url: https://orchestrator-graphql-endpoint\n    schema:\n      subgraph_url: https://orchestrator-graphql-endpoint\n  new_graphql:\n    routing_url: http://localhost:4001\n    schema:\n      subgraph_url: http://localhost:4001\n</code></pre> <p>When both GraphQL endpoints are available, you can compose the supergraph schema using the following command:</p> <pre><code>rover supergraph compose --config ./supergraph.yaml &gt; supergraph-schema.graphql\n</code></pre> <p>The command will return errors if incorrect keys or other issues are present. Then, you can run the federation with the following command:</p> <pre><code>./router --supergraph supergraph-schema.graphql\n</code></pre> <p>Now you can query the endpoint to obtain <code>newValue</code> from all subscriptions using the payload below:</p> <pre><code>{\n    \"rationName\":  \"ExampleQuery\",\n    \"query\": \"query ExampleQuery {\\n  subscriptions {\\n    page {\\n      newValue\\n    }\\n  }\\n}\\n\",\n    \"variables\": {}\n}\n</code></pre>"},{"location":"reference-docs/graphql/#federating-with-specific-subscriptions","title":"Federating with Specific Subscriptions","text":"<p>To federate with specific subscriptions, you need to make a few changes. Here's an example of a specific subscription:</p> <pre><code># `type` instead of `interface_object` and name the class exactly the same as the one in orchestrator.\n@strawberry.federation.type(keys=[\"subscriptionId\"])\nclass YourProductSubscription:\n    subscription_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"SubscriptionInterface\":\n        if not (subscription_id := data.get(\"subscriptionId\")):\n            raise ValueError(\n                f\"Need 'subscriptionId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = new_value_resolver(subscription_id)\n        return SubscriptionInterface(subscription_id=subscription_id, new_value=value)\n</code></pre>"},{"location":"reference-docs/graphql/#federating-with-specific-subscription-product-blocks","title":"Federating with Specific Subscription Product Blocks","text":"<p>You can also federate a ProductBlock. In this case, the <code>subscriptionInstanceId</code> can be replaced with any product block property containing <code>Id</code>:</p> <pre><code>@strawberry.federation.interface_object(keys=[\"subscriptionInstanceId\"])\nclass YourProductBlock:\n    subscription_instance_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"YourProductBlock\":\n        if not (subscription_id := data.get(\"subscriptionInstanceId\")):\n            raise ValueError(\n                f\"Need 'subscriptionInstanceId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = \"new value\"\n        return YourProductBlock(subscription_id=subscription_id, new_value=\"new value\")\n</code></pre> <p>By following these examples, you can effectively federate autogenerated types (<code>subscriptions</code> and <code>product blocks</code>) enabling seamless integration across multiple GraphQL endpoints.</p>"},{"location":"reference-docs/graphql/#usage-of-use_pydantic_alias_model_mapping","title":"Usage of USE_PYDANTIC_ALIAS_MODEL_MAPPING","text":"<p><code>USE_PYDANTIC_ALIAS_MODEL_MAPPING</code> is a mapping to prevent pydantic field alias from being used as field names when creating Strawberry types in the domain model autoregistration. Our use case for this is that functions decorated with pydantics <code>@computed_field</code> and <code>@property</code> in domain models are not converted to Strawberry fields inside the Strawberry types. To add the function properties, we use a aliased pydantic field:</p> <pre><code>class ExampleProductInactive(SubscriptionModel, is_base=True):\n    # this aliased property is used to add `property_example` as Strawberry field.\n    # you need a default it the value can't be `None` since it doesn't directly add the return value of property_example\n    aliased_property_example: Field(alias=\"property_example\", default=\"\")\n\n    # this computed property function does not get converted into the Strawberry type.\n    @computed_field  # type: ignore[misc]\n    @property\n    def property_example(self) -&gt; str:\n        return \"example\"\n\n\nclass ExampleProductProvisioning(\n    ExampleProductInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    pass\n\n\nclass ExampleProduct(ExampleProductInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    pass\n</code></pre> <p>The problem with this is that Strawberry automatically uses the alias name and doesn't camelcase it so the Strawberry field becomes <code>property_example</code>. To fix it and have CamelCasing, we can prevent aliases from being used in <code>strawberry.type</code> using the created mapping <code>USE_PYDANTIC_ALIAS_MODEL_MAPPING</code>:</p> <pre><code>from orchestrator.graphql.autoregistration import USE_PYDANTIC_ALIAS_MODEL_MAPPING\n\nUSE_PYDANTIC_ALIAS_MODEL_MAPPING.update({\"ExampleProductSubscription\": False})\n</code></pre> <p>which would now give us a Strawberry field <code>aliasedPropertyExample</code>. To name it <code>propertyExample</code> you can't override the function property name and have two choices:</p> <ol> <li> <p>CamelCase the aliased property:    <pre><code>class ExampleProductInactive(SubscriptionModel, is_base=True):\n    # this aliased property is used to add `property_example` as Strawberry field.\n    propertyExample: Field(alias=\"property_example\")\n</code></pre></p> </li> <li> <p>Rename the property function and name the aliased field correctly, when accessing outside of the GraphQLfield, you do need to use <code>computed_property_example</code> instead of <code>property_example</code>:    <pre><code>class ExampleProductInactive(SubscriptionModel, is_base=True):\n    # this aliased property is used to add `property_example` as Strawberry field.\n    property_example: Field(alias=\"computed_property_example\")\n\n    # this computed property function does not get converted into the Strawberry type.\n    @computed_field  # type: ignore[misc]\n    @property\n    def computed_property_example(self) -&gt; str:\n        return \"example\"\n</code></pre></p> </li> </ol>"},{"location":"reference-docs/graphql/#overriding-types","title":"Overriding Types","text":"<p>Overriding Strawberry types can be achieved through various methods. One less desirable approach involves extending classes using class inheritance. However, this method becomes cumbersome when updating a single class, as it necessitates updating all associated types and their corresponding resolvers, essentially impacting the entire structure.</p> <p>For instance, consider the scenario of overriding the <code>CustomerType</code>. you would need to update the related <code>SubscriptionInterface</code>, <code>ProcessType</code> and their respective resolvers. due to these modifications, all their related types and resolvers would also require updates, resulting in a tedious and error-prone process.</p> <p>To enhance the override process, we created a helper function <code>override_class</code> to override fields. It takes the base class as well as a list of fields that will replace their counterparts within the class or add new fields.</p> <p>It's worth noting that <code>SubscriptionInterface</code> poses a unique challenge due to its auto-generated types. The issue arises from the fact that the models inherited from <code>SubscriptionInterface</code> do not automatically update. This can be addressed by utilizing the <code>override_class</code> function and incorporating the returned class into the <code>app.register_graphql</code> function. This ensures that the updated class, with overridden fields, becomes the basis for generating the auto-generated models.</p> <pre><code># Define a custom subscription interface using the `override_class` function, incorporating specified override fields.\ncustom_subscription_interface = override_class(SubscriptionInterface, override_fields)\n\n# Register the customized subscription interface when setting up GraphQL in your application.\napp.register_graphql(subscription_interface=custom_subscription_interface)\n</code></pre> <p>Quick example (for more indebt check customerType override):</p> <pre><code>import strawberry\nfrom orchestrator.graphql.utils.override_class import override_class\n\n\n# Define a Strawberry type representing an example entity\n@strawberry.type()\nclass ExampleType:\n    @strawberry.field(description=\"Existing field\")  # type: ignore\n    def existing(self) -&gt; int:\n        return 1\n\n\n# Define a Strawberry type for example queries\n@strawberry.type(description=\"Example queries\")\nclass Query:\n    example: ExampleType = strawberry.field(resolver=lambda: ExampleType())\n\n\n# Create a resolver for updating the existing field\nasync def update_existing_resolver() -&gt; str:\n    return \"updated to new type\"\n\n\n# Create a Strawberry field with the resolver for the existing field\nexisting_field = strawberry.field(resolver=update_existing_resolver, description=\"update existing field\")  # type: ignore\n# Assign a new name to the Strawberry field; this name will override the existing field in the class\nexisting_field.name = \"existing\"\n\n\n# Create a new field with a resolver\nasync def new_resolver() -&gt; int:\n    return 1\n\n\nnew_field = strawberry.field(resolver=new_resolver, description=\"a new field\")  # type: ignore\n# Assign a name that is not present in the class yet\nnew_field.name = \"new\"\n\n# Use the override_class function to replace fields in the ExampleType\noverride_class(ExampleType, [new_field, existing_field])\n</code></pre>"},{"location":"reference-docs/graphql/#overriding-customertype-and-resolvers","title":"Overriding CustomerType and Resolvers","text":"<p>Within the orchestrator core, there exists a base <code>CustomerType</code> designed to provide a default customer, allowing for the customization of data through environment variables. This approach minimizes the necessity for everyone to implement custom customer logic.</p> <p>Below, I present an example illustrating how to override the <code>CustomerType</code> and its associated resolvers.</p>"},{"location":"reference-docs/graphql/#customertype-override","title":"CustomerType Override","text":"<p>Here's a generic override for the <code>CustomerType</code> that introduces a new <code>subscriptions</code> relation:</p> <pre><code>from typing import Annotated\n\nimport strawberry\n\nfrom oauth2_lib.strawberry import authenticated_field\nfrom orchestrator.graphql.pagination import Connection\nfrom orchestrator.graphql.schemas.customer import CustomerType\nfrom orchestrator.graphql.schemas.subscription import (\n    SubscriptionInterface,\n)  # noqa: F401\nfrom orchestrator.graphql.types import GraphqlFilter, GraphqlSort, OrchestratorInfo\nfrom orchestrator.graphql.utils.override_class import override_class\n\n# Type annotation for better readability rather than having this directly as a return type\nSubscriptionInterfaceType = Connection[\n    Annotated[\n        \"SubscriptionInterface\",\n        strawberry.lazy(\"orchestrator.graphql.schemas.subscription\"),\n    ]\n]\n\n\n# Resolver for fetching subscriptions of a customer\nasync def resolve_subscriptions(\n    root: CustomerType,\n    info: OrchestratorInfo,\n    filter_by: list[GraphqlFilter] | None = None,\n    sort_by: list[GraphqlSort] | None = None,\n    first: int = 10,\n    after: int = 0,\n) -&gt; SubscriptionInterfaceType:\n    from orchestrator.graphql.resolvers.subscription import resolve_subscriptions\n\n    # Include the filter for the customer ID; since 'customerId' exists in the subscription, filtering updates are not required.\n    filter_by_customer_id = (filter_by or []) + [GraphqlFilter(field=\"customerId\", value=str(root.customer_id))]  # type: ignore\n    return await resolve_subscriptions(\n        info, filter_by_customer_id, sort_by, first, after\n    )\n\n\n# Create an authenticated field for customer subscriptions\ncustomer_subscriptions_field = authenticated_field(\n    resolver=resolve_subscriptions, description=\"Returns subscriptions of a customer\"\n)\n# Assign a new name to the Strawberry field; this name will add the 'subscriptions' field in the class\ncustomer_subscriptions_field.name = \"subscriptions\"\n\n# Override the CustomerType with the new 'subscriptions' field\noverride_class(CustomerType, [customer_subscriptions_field])\n</code></pre>"},{"location":"reference-docs/graphql/#customertype-resolver-override","title":"CustomerType Resolver Override","text":"<p>In this example code, we introduce a resolver override for the <code>CustomerType</code>. The scenario involves a supplementary <code>CustomerTable</code> in the database, encompassing the default values of <code>CustomerType</code>\u2014namely, <code>customer_id</code>, <code>fullname</code>, and <code>shortcode</code>.</p> <pre><code>import structlog\nfrom sqlalchemy import func, select\n\nfrom orchestrator.db import db\nfrom orchestrator.db.filters import Filter\nfrom orchestrator.db.range.range import apply_range_to_statement\nfrom orchestrator.db.sorting import Sort\nfrom orchestrator.graphql.pagination import Connection\nfrom orchestrator.graphql.resolvers.helpers import rows_from_statement\nfrom orchestrator.graphql.schemas.customer import CustomerType\nfrom orchestrator.graphql.types import GraphqlFilter, GraphqlSort, OrchestratorInfo\nfrom orchestrator.graphql.utils.create_resolver_error_handler import (\n    create_resolver_error_handler,\n)\nfrom orchestrator.graphql.utils.to_graphql_result_page import to_graphql_result_page\nfrom orchestrator.utils.search_query import create_sqlalchemy_select\nfrom your_customer_table_location.db.models import CustomerTable\n\n# # Import custom sorting and filtering modules used with `sort_by` and `filter_by`.\n# from sort_loc import sort_customers, sort_customers_fields\n# from filter_loc import filter_customers, filter_customers_fields\n\nlogger = structlog.get_logger(__name__)\n\n\n# Queries\ndef resolve_customers(\n    info: OrchestratorInfo,\n    filter_by: list[GraphqlFilter] | None = None,\n    sort_by: list[GraphqlSort] | None = None,\n    first: int = 10,\n    after: int = 0,\n    query: str | None = None,\n) -&gt; Connection[CustomerType]:\n    # ---- DEFAULT RESOLVER LOGIC ----\n    _error_handler = create_resolver_error_handler(info)\n\n    pydantic_filter_by: list[Filter] = [item.to_pydantic() for item in filter_by] if filter_by else []  # type: ignore\n    pydantic_sort_by: list[Sort] = [item.to_pydantic() for item in sort_by] if sort_by else []  # type: ignore\n    logger.debug(\n        \"resolve_customers() called\",\n        range=[after, after + first],\n        sort=pydantic_sort_by,\n        filter=pydantic_filter_by,\n    )\n    # ---- END OF DEFAULT RESOLVER LOGIC ----\n\n    select_stmt = select(CustomerTable)\n\n    # # Include custom filtering logic if imported\n    # select_stmt = filter_customers(select_stmt, pydantic_filter_by, _error_handler)\n\n    if query is not None:\n        stmt = create_sqlalchemy_select(\n            select_stmt,\n            query,\n            mappings={},\n            base_table=CustomerTable,\n            join_key=CustomerTable.customer_id,\n        )\n    else:\n        stmt = select_stmt\n\n    # # Include custom sorting logic if imported\n    # stmt = sort_customers(stmt, pydantic_sort_by, _error_handler)\n\n    # ---- DEFAULT RESOLVER LOGIC ----\n    total = db.session.scalar(select(func.count()).select_from(stmt.subquery()))\n    stmt = apply_range_to_statement(stmt, after, after + first + 1)\n\n    customers = rows_from_statement(stmt, CustomerTable)\n    graphql_customers = [\n        CustomerType(\n            customer_id=c.customer_id, fullname=c.fullname, shortcode=c.shortcode\n        )\n        for c in customers\n    ]\n    return to_graphql_result_page(\n        graphql_customers,\n        first,\n        after,\n        total,\n        sort_customers_fields,\n        filter_customers_fields,\n    )\n    # ---- END OF DEFAULT RESOLVER LOGIC ----\n</code></pre>"},{"location":"reference-docs/graphql/#customertype-related-type-overrides","title":"CustomerType Related Type Overrides","text":"<p>Having overridden the <code>customer_resolver</code> and added the <code>subscriptions</code> field to the <code>CustomerType</code>, the final step involves updating the related Strawberry types, namely <code>SubscriptionInterface</code> and <code>ProcessType</code>.</p> <p>For both types, the <code>customer_id</code> is at the root, allowing us to create a generic override resolver for both. As we modify <code>SubscriptionInterface</code>, it's essential to utilize the returned type (stored in the <code>custom_subscription_interface</code> variable) when registering GraphQL in the application using <code>app.register_graphql(subscription_interface=custom_subscription_interface)</code>.</p> <pre><code>async def resolve_customer(root: CustomerType) -&gt; CustomerType:\n    stmt = select(CustomerTable).where(CustomerTable.customer_id == root.customer_id)\n\n    if not (customer := db.session.execute(stmt).scalars().first()):\n        return CustomerType(\n            customer_id=root.customer_id, fullname=\"missing\", shortcode=\"missing\"\n        )\n\n    return CustomerType(\n        customer_id=customer.customer_id,\n        fullname=customer.fullname,\n        shortcode=customer.shortcode,\n    )\n\n\n# Create a Strawberry field with the resolver for the customer field\ncustomer_field = strawberry.field(resolver=resolve_customer, description=\"Returns customer of a subscription\")  # type: ignore\n# Assign a new name to the Strawberry field; this name will add the 'customer' field in the class\ncustomer_field.name = \"customer\"\n\n# Override the SubscriptionInterface and ProcessType with the new 'customer' field\noverride_class(ProcessType, [customer_field])\ncustom_subscription_interface = override_class(SubscriptionInterface, [customer_field])\n</code></pre>"},{"location":"reference-docs/graphql/#behavior-of-filterby","title":"Behavior of filterBy","text":"<p>By default, string matching is configured for exact matches, i.e a search for <code>10</code> will return ONLY <code>10</code> and won't include <code>10G</code> or <code>100G</code>. Searching can also be configured for partial matching as well, where a search for <code>10</code> would include <code>10G</code> and <code>100G</code>.</p> <p>This can be controlled by setting the variable <code>FILTER_BY_MODE</code> can be set to a value of <code>exact</code> or <code>partial</code> as needed.</p>"},{"location":"reference-docs/python/","title":"Python versions","text":"<p>The orchestrator is ensured to work with multiple versions of Python 3.</p> <p>At SURF the orchestrator runs in a Docker image which makes updating the \"installed python\" as trivial as bumping the base image. But not everyone can or wants to use containers. To this end, the oldest supported Python version is chosen such that it can run on bare metal or virtual Linux servers, without the complexities and risks of compiling a newer version of Python.</p>"},{"location":"reference-docs/python/#adding-support-for-new-versions","title":"Adding support for new versions","text":"<p>We aim to add support for the latest Python 3 release within months of it becoming publicly available.</p>"},{"location":"reference-docs/python/#dropping-support-for-old-versions","title":"Dropping support for old versions","text":"<p>Our policy is to support the same version of Python 3 available in Debian-stable.</p> <p>When there is a new Debian-stable release, we will update the oldest Python version supported by orchestator-core to match it.</p>"},{"location":"reference-docs/python/#example","title":"Example","text":"<p>At the time of writing (April 2023), the latest Debian is 11 which supports Python 3.9, and the latest Python 3.x release is 3.11. Thus the supported versions are: <code>3.9 3.10 3.11</code></p> <p>Debian 12's release is currently estimated for June 2023, and it looks like it will ship with Python 3.11. That means we can reduce the supported versions to: <code>3.11</code></p> <p>Python 3.12 is scheduled to release in October 2023, which we'll then add to the supported versions: <code>3.11 3.12</code></p>"},{"location":"reference-docs/search/","title":"Search","text":"<ul> <li>Search</li> <li>Usage</li> <li>Implementation 1: Filter on DB Table</li> <li>Implementation 2: Subscription Search<ul> <li>Postgres Text Search</li> <li>Materialized View subscriptions_search</li> </ul> </li> </ul> <p>The orchestrator-core provides search functionality to find data within the Postgres DB.</p> <p>The below overview describes which objects can be searched through which endpoint, and the underlying search implementation.</p> Object REST endpoint GraphQL endpoint Implementation Subscriptions <code>/api/subscriptions/search?query=</code> <code>subscriptions(query: \"...\")</code> Subscription Search Processes <code>processes(query: \"...\")</code> Filter on DB table Product Blocks <code>product_blocks(query: \"...\")</code> Filter on DB table Products <code>products(query: \"...\")</code> Filter on DB table Resource types <code>resource_types(query: \"...\")</code> Filter on DB table Workflows <code>workflows(query: \"...\")</code> Filter on DB table <p>There are 2 implementations:</p> <ol> <li>Filter on DB Table: generic implementation to search on values in the DB table for an object, or related tables</li> <li>Subscription Search: specialized implementation to search for values anywhere in a subscription, such as instance values or customer descriptions</li> </ol>"},{"location":"reference-docs/search/#usage","title":"Usage","text":"<p>Call the REST or GraphQL endpoint with the <code>query</code> parameter set to the search query.</p> <p>Some search query examples for Subscriptions:</p> Query Matches subscriptions with <code>test</code> <code>test</code> in any field <code>description:test</code> <code>test</code> in description field <code>L2VPN</code> <code>L2VPN</code> in any field <code>tag:L2VPN</code> <code>L2VPN</code> in product tag field <code>tag:(FW \\| L2VPN)</code>  Or: <code>tag:FW \\| tag:L2VPN</code> <code>FW</code> OR <code>L2VPN</code> in product tag field <code>description:test tag:L2VPN</code> <code>test</code> in description field and <code>L2VPN</code> in product tag field <code>description:test -tag:L2VPN</code> <code>test</code> in description field and NOT <code>L2VPN</code> in product tag field <code>test 123</code>  Or: <code>123 test</code> <code>test</code> AND <code>123</code> anywhere <code>\"test 123\"</code> <code>test 123</code> anywhere (Phrase search) <code>test*</code> any field starting with <code>test</code> (Prefix search) <p>Note that:</p> <ul> <li>For other objects the query syntax is the same</li> <li>Searching is case-insensitive</li> <li>Ordering of words does not matter (unless it is a Phrase)</li> <li>Search words cannot contain the characters <code>|-*():\"</code> as they are part of the search query grammar</li> </ul>"},{"location":"reference-docs/search/#implementation-1-filter-on-db-table","title":"Implementation 1: Filter on DB Table","text":"<p>This implementation translates the user's search query to <code>WHERE</code> clauses on DB columns of the object's DB table. For some objects this extends to related DB tables.</p> <p>We can distinguish these steps:</p> <ul> <li>The module <code>search_query.py</code> parses the user's query and generates a sequence of sqlalchemy <code>.filter()</code> clauses</li> <li>The REST/GraphQL endpoint appends these clauses to the sqlalchemy <code>select()</code> to find objects that match the user's query</li> </ul>"},{"location":"reference-docs/search/#implementation-2-subscription-search","title":"Implementation 2: Subscription Search","text":"<p>This is a specialized implementation to allow searching values anywhere in a subscription or in related entities, without sacrificing performance.</p> <p>We can distinguish these steps/components:</p> <ul> <li>The DB view <code>subscriptions_search</code> is a search index with Text Search (TS) vectors. Both are explained in the next sections</li> <li>The module <code>search_query.py</code> parses the user's query into a string that is used to create a TS query</li> <li>The TS query is wrapped in a single sqlalchemy <code>.filter()</code> clause to match TS documents in <code>subscriptions_search</code> and get the corresponding <code>subscription_id</code></li> <li>The REST/GraphQL endpoint appends this clause to the sqlalchemy <code>select()</code> to find subscription objects that match the user's query</li> </ul> <p>The diagram below visualizes the flow and dependencies between components in the Database, API and Frontend.</p> <p></p>"},{"location":"reference-docs/search/#postgres-text-search","title":"Postgres Text Search","text":"<p>Postgres Full Text Search (TS) has extensive documentation but we'll cover the fundamentals in this section. TS queries can be done on \"normal\" DB tables, i.e. without a search index, but this is too slow. It is recommended to maintain a search index which we do in DB view <code>subscriptions_search</code>.</p> <p>Creating TS vectors</p> <p>The query behind <code>subscriptions_search</code> retrieves Subscriptions joined with several other tables (as shown in the previous diagram), forming a \"document\" of keywords that in some way relate to the subscription.</p> <p>Each document is turned into a tsvector with Postgres function <code>to_tsvector()</code> which consists of these phases:</p> <ul> <li>Parse document into tokens: split text into tokens using special characters as delimiters</li> <li>Convert tokens into lexemes: a lexeme is a normalized token, i.e. this folds upper-case to lower-case. This step can also normalize based on language, but we disable that by using the <code>'simple'</code> dictionary (shown in diagram above)</li> <li>Create vector optimized for search: store array of lexemes along with positional information</li> </ul> <p>The array of lexemes makes up the tsvector document that we store in <code>subsriptions_search</code>, and which we can query through the Postgres function <code>to_tsquery()</code>.</p> <p>We'll demonstrate how this works through a few examples. You can follow along in any  postgres shell (v14 or above). If you have Docker installed, run these commands in separate shells.</p> <pre><code>docker run --rm -e POSTGRES_HOST_AUTH_METHOD=trust --name pg17 pgvector/pgvector:pg17\ndocker exec -i -t pg17 su - postgres -c psql\n</code></pre> <p>Translating the input text <code>'color:Light_Blue count:4'</code> to a tsvector:</p> <pre><code>postgres=# select to_tsvector('simple', 'color:Light_Blue count:4');\n                 to_tsvector\n----------------------------------------------\n '4':5 'blue':3 'color':1 'count':4 'light':2\n(1 row)\n</code></pre> <p>The result <code>'4':5 'blue':3 'color':1 'count':4 'light':2</code> is an array of lexemes combined with information about their position in the original text.</p> <p>With Postgres function <code>to_tsdebug()</code> one can investigate how the input was parsed.</p> <p><pre><code>postgres=# select * from ts_debug('simple', 'color:Light_Blue count:4');\n   alias   |   description    | token | dictionaries | dictionary | lexemes\n-----------+------------------+-------+--------------+------------+---------\n asciiword | Word, all ASCII  | color | {simple}     | simple     | {color}\n blank     | Space symbols    | :     | {}           |            |\n asciiword | Word, all ASCII  | Light | {simple}     | simple     | {light}\n blank     | Space symbols    | _     | {}           |            |\n asciiword | Word, all ASCII  | Blue  | {simple}     | simple     | {blue}\n blank     | Space symbols    |       | {}           |            |\n asciiword | Word, all ASCII  | count | {simple}     | simple     | {count}\n blank     | Space symbols    | :     | {}           |            |\n uint      | Unsigned integer | 4     | {simple}     | simple     | {4}\n ```\n\nNote: when we would write \"Light-Blue\" with a dash instead of underscore, Postgres translates this into the vector `'blue':4 'light':3 'light-blue':2`, which makes it very complicated to query, in particular for UUID strings. To mitigate this we replace all occurrences of `-` with `_` when creating TS Vectors and before executing TS Queries.\n\n**Querying TS Vectors**\n\nTo run queries against TS Vectors we have to prepare a TS Query with Postgres function `to_tsquery()`. We also pass the `'simple'` dictionary here to prevent language-specific normalization.\n</code></pre> postgres=# select to_tsquery('simple', 'Light_Blue');      to_tsquery</p> <p>'light' &lt;-&gt; 'blue'  ```</p> <p>The resulting TS query means as much as: <code>the vector must contain 'light' followed by 'blue'</code>. Note that the input string has been tokenized similar to TS vectors.</p> <p>We can execute this TS query against the TS vector:</p> <pre><code>postgres=# select to_tsvector('simple', 'color:Light_Blue count:4') @@ to_tsquery('simple', 'Light_Blue');\n ?column?\n----------\n t\n(1 row)\n</code></pre> <p>Returns <code>t</code> for <code>true</code>.</p> <p>As a final example we can also add an <code>OR</code> condition:</p> <pre><code>postgres=# select to_tsvector('simple', 'color:Light_Blue count:4') @@ to_tsquery('simple', 'color &lt;-&gt; Green | count &lt;-&gt; 4');\n ?column?\n----------\n t\n(1 row)\n</code></pre> <p>Which returns true because while the vector does not contain the color green, it does contain count 4.</p> <p>Note that the <code>color &lt;-&gt; Green | count &lt;-&gt; 4</code> string passed to <code>ts_query()</code> must be constructed in a specific way. This happens in the orchestrator-core module <code>search_query.py</code> as shown in the overview diagram.</p>"},{"location":"reference-docs/search/#materialized-view-subscriptions_search","title":"Materialized View subscriptions_search","text":"<p>As mentioned before, <code>subscriptions_search</code> is a DB view which lies at the heart of the implementation. If you're not familiar with database views; they represent a (usually complicated) database query in the form of a \"virtual table\".</p> <p>In this case we're using a Materialized View which is like a normal view, except that the \"virtual table\" is persisted to save resources and increase performance. The view's data is persisted until it is refreshed, which can be done manually or through an update trigger. (further explained below)</p> <p>This table has a GIN index for efficient search queries.</p> <p>Triggers</p> <p>Database function <code>refresh_subscriptions_search_view</code> takes care of refreshing <code>subscriptions_search</code>. It is called by triggers on the following tables:</p> <ul> <li><code>fi_refresh_search</code> on table <code>fixed_inputs</code></li> <li><code>products_refresh_search</code> on table <code>products</code></li> <li><code>siv_refresh_search</code> on table <code>subscription_instance_values</code></li> <li><code>sub_cust_desc_refresh_search</code> on table <code>subscription_customer_descriptions</code></li> <li><code>sub_refresh_search</code> on table <code>subscriptions</code></li> </ul> <p>The following query shows the current state of the triggers:</p> <pre><code>SELECT tgname, tgenabled\nFROM pg_trigger\nwhere pg_trigger.tgname in\n    ('fi_refresh_search',\n     'products_refresh_search',\n     'sub_cust_desc_refresh_search',\n     'siv_refresh_search',\n     'sub_refresh_search');\n</code></pre> <p>When all triggers are disabled the output looks like:</p> <pre><code>            tgname            | tgenabled\n------------------------------+-----------\n fi_refresh_search            | D\n products_refresh_search      | D\n siv_refresh_search           | D\n sub_cust_desc_refresh_search | D\n sub_refresh_search           | D\n</code></pre> <p>And when all triggers are enabled the output looks like this: <pre><code>            tgname            | tgenabled\n------------------------------+-----------\n fi_refresh_search            | O\n products_refresh_search      | O\n siv_refresh_search           | O\n sub_cust_desc_refresh_search | O\n sub_refresh_search           | O\n</code></pre></p> <p>Enabling all triggers is done with these statements:</p> <pre><code>ALTER TABLE fixed_inputs ENABLE TRIGGER fi_refresh_search;\nALTER TABLE products ENABLE TRIGGER products_refresh_search;\nALTER TABLE subscription_customer_descriptions ENABLE TRIGGER sub_cust_desc_refresh_search;\nALTER TABLE subscription_instance_values ENABLE TRIGGER siv_refresh_search;\nALTER TABLE subscriptions ENABLE TRIGGER sub_refresh_search;\n</code></pre> <p>Disabling all triggers is done with these statements:</p> <pre><code>ALTER TABLE fixed_inputs DISABLE TRIGGER fi_refresh_search;\nALTER TABLE products DISABLE TRIGGER products_refresh_search;\nALTER TABLE subscription_customer_descriptions DISABLE TRIGGER sub_cust_desc_refresh_search;\nALTER TABLE subscription_instance_values DISABLE TRIGGER siv_refresh_search;\nALTER TABLE subscriptions DISABLE TRIGGER sub_refresh_search;\n</code></pre> <p>The following query returns the number of seconds since the last refresh, which can be useful for debugging.</p> <pre><code>SELECT extract(epoch from now())::int - coalesce(pg_catalog.obj_description('subscriptions_search'::regclass)::int, 0);\n</code></pre> <p>Limitations</p> <p>Updating the <code>subscriptions_search</code> materialized view is expensive and limited to once every 2 minutes.</p> <p>This means that when 2 changes happen within, say, 5 seconds of each other, the first change will be picked up directly. However, the second change will only be processed on the next refresh of the view. So during that period the second change will not show up in the search results.</p>"},{"location":"reference-docs/tldr/","title":"TL;DR","text":"<p>This reference documentation is for developers who want to learn more about the internals of the Workflow Orchestrator. For more conceptual and high-level documentation, head over to the Architecture section of the documentation. This is a mix of user-guide style docs and documentation of the actual python classes and functions in use in the <code>orchestrator-core</code>. Many of these function and class docs are also available in the code base directly from your IDE.</p>"},{"location":"reference-docs/websockets/","title":"Websockets","text":"<p>Orchestrator provides a websocket interface through which the frontend can receive real-time updates. This includes:</p> <ul> <li>The process overview pages</li> <li>The process detail page</li> <li>Engine status</li> </ul>"},{"location":"reference-docs/websockets/#implementation","title":"Implementation","text":"<p>To function properly in a scalable architecture, the websocket implementation consists of multiple layers.</p> <p>The main component is the <code>WebSocketManager</code> (WSM) which has the following responsibilities:</p> <ol> <li>Keep track of connected frontend clients</li> <li>Forward messages to all frontend clients</li> <li>Provide an interface to pass messages from a backend process (workflow/task)</li> </ol> <p>In a setup with multiple isolated Orchestrator instances the WSM is initialized multiple times as well, therefore clients can be connected to any arbitrary WSM instance. Letting a backend process broadcast messages to all clients thus requires a message broker, for which we use Redis Pub/Sub.</p> <p>There are 2 WSM implementations: a <code>MemoryWebsocketManager</code> for development/testing, and a <code>BroadcastWebsocketManager</code> that connects to Redis. We'll continue to discuss the latter.</p> <ul> <li><code>BroadcastWebsocketManager.broadcast_data()</code> is called by backend processes, and publishes messages to a channel in Redis [1]</li> <li><code>BroadcastWebsocketManager.sender()</code> starts in a loop for each connected client, subscribes to a channel in Redis, and forwards messages into the websocket connection</li> </ul> <p>[1] When using <code>EXECUTOR=\"threadpool\"</code> this function is not called directly, refer to the ProcessDataBroadcastThread section</p> <p>Roughly speaking a message travels through these components: <pre><code>Process\n  -&gt; BroadcastWebsocketManager.broadcast_data()\n  -&gt; Redis channel\n  -&gt; BroadcastWebsocketManager.sender()\n  -&gt; Websocket connection\n  -&gt; Frontend client\n</code></pre></p>"},{"location":"reference-docs/websockets/#processdatabroadcastthread","title":"ProcessDataBroadcastThread","text":"<p>Note: this section is only relevant when the orchestrator is configured with <code>EXECUTOR=\"threadpool\"</code>.</p> <p>Backend processes are executed in a threadpool and therefore access the same WSM instance. This caused asyncio RuntimeErrors as the async Redis Pub/Sub implementation is not thread-safe.</p> <p>To solve this there is a dedicated <code>ProcessDataBroadcastThread</code> (attached to and managed by the <code>OrchestratorCore</code> app) to perform the actual <code>broadcast_data()</code> call.</p> <p>The API endpoints which start/resume/abort a process, call <code>api_broadcast_process_data(request)</code> to acquire a function that can be used to submit process updates into a <code>threading.Queue</code> on which <code>ProcessDataBroadcastThread</code> listens.</p>"},{"location":"reference-docs/websockets/#channel-overview","title":"Channel overview","text":"<p>As mentioned in Implementation, messages are organized into channels that clients can listen to. Each channel has its own usecase and specific message format.</p>"},{"location":"reference-docs/websockets/#events","title":"Events","text":"<p>The <code>events</code> channel is designed for the Orchestrator UI v2. Events are notifications to the UI/user that something happened in the backend API or workers. They only include the bare minimum of information and can be sent at a high volume.</p> <p>The API endpoint for this channel is <code>/api/ws/events</code> .</p> <p>Messages in this channel are of the format: <pre><code>{\"name\": name, \"value\": value}\n</code></pre></p> <p>Where <code>name</code> and <code>value</code> are one of the following combinations:</p> name value Notifies that <code>\"invalidateCache\"</code> <code>{\"type\": \"processes\", \"id\": \"LIST\"}</code> A process was started, updated or finished [1] <code>\"invalidateCache\"</code> <code>{\"type\": \"processes\", \"id\": \"&lt;process UUID&gt;\"}</code> A process was started, updated or finished [1] <code>\"invalidateCache\"</code> <code>{\"type\": \"subscriptions\", \"id\": \"LIST\"}</code> A subscription was created, updated or terminated [1] <code>\"invalidateCache\"</code> <code>{\"type\": \"subscriptions\", \"id\": \"&lt;subscription UUID&gt;\"}</code> A subscription was created, updated or terminated [1] <code>\"invalidateCache\"</code> <code>{\"type\": \"processStatusCounts\"}</code> A process transitioned to/from a failed state [2] <code>\"invalidateCache\"</code> <code>{\"type\": \"engineStatus\"}</code> The workflow engine has been enabled or disabled <p>Notes: 1. The <code>LIST</code> and <code>&lt;uuid&gt;</code> combinations currently mean one and the same. The reason to keep them separate is that we may want to implement throttling on the <code>LIST</code> event. 2. The process status count event is triggered when a process:    * Transitions to non-failed state -&gt; count may go down:      1. Non-running process is scheduled to be resumed      2. Non-running process is deleted from database    * Transitions to a failed state -&gt; count goes up:      1. Running process finishes with an error</p> <p>Example of a complete message:</p> <pre><code>{\"name\":\"invalidateCache\",\"value\":{\"type\":\"engineStatus\"}}\n</code></pre>"},{"location":"reference-docs/websockets/#engine-settings-deprecated","title":"Engine settings (deprecated)","text":"<p>The <code>engine-settings</code> channel was designed for the now deprecated Orchestrator UI v1.</p> <p>The API endpoint for this channel is <code>/api/settings/ws-status/</code> .</p>"},{"location":"reference-docs/websockets/#processes-deprecated","title":"Processes (deprecated)","text":"<p>The <code>processes</code> channel was designed for the now deprecated Orchestrator UI v1. It sent process list/detail data to the client which it would use to directly update the frontend.</p> <p>The API endpoint for this channel is <code>/api/processes/all/</code> .</p>"},{"location":"reference-docs/app/agentic-app/","title":"agentic_app.py","text":"<p>The agentic_app.py module is used in <code>orchestrator-core</code> for running the LLM enabled orchestrator. Functionality in this class is toggled by two different environment variables:</p> <ul> <li><code>SEARCH_ENABLED</code>: When set to <code>True</code> the orchestrator will enable the LLM Based search</li> <li><code>AGENT_ENABLED</code>: When set to <code>True</code> the orchestrator will activate the Agent module of the orchestrator.</li> </ul>"},{"location":"reference-docs/app/agentic-app/#fastapi-backend","title":"FastAPI Backend","text":"<p>The code for the WFO's Fast API backend is very well documented, so look through the functions used in this module here:</p>"},{"location":"reference-docs/app/agentic-app/#orchestrator.agentic_app","title":"orchestrator.agentic_app","text":"<p>The main application module.</p> <p>This module contains the main <code>LLMOrchestratorCore</code> class for the <code>FastAPI</code> backend and provides the ability to run the CLI with LLM features (search and/or agent).</p>"},{"location":"reference-docs/app/agentic-app/#orchestrator.agentic_app.LLMOrchestratorCore","title":"LLMOrchestratorCore","text":"<p>               Bases: <code>orchestrator.app.OrchestratorCore</code></p> Source code in <code>orchestrator/agentic_app.py</code> <pre><code>class LLMOrchestratorCore(OrchestratorCore):\n    def __init__(\n        self,\n        *args: Any,\n        llm_settings: LLMSettings = llm_settings,\n        agent_model: \"OpenAIChatModel | str | None\" = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the `LLMOrchestratorCore` class.\n\n        This class extends `OrchestratorCore` with LLM features (search and agent).\n        It runs the search migration based on feature flags.\n\n        Args:\n            *args: All the normal arguments passed to the `OrchestratorCore` class.\n            llm_settings: A class of settings for the LLM\n            agent_model: Override the agent model (defaults to llm_settings.AGENT_MODEL)\n            **kwargs: Additional arguments passed to the `OrchestratorCore` class.\n\n        Returns:\n            None\n        \"\"\"\n        self.llm_settings = llm_settings\n        self.agent_model = agent_model or llm_settings.AGENT_MODEL\n\n        super().__init__(*args, **kwargs)\n\n        # Run search migration if search or agent is enabled\n        if self.llm_settings.SEARCH_ENABLED or self.llm_settings.AGENT_ENABLED:\n            logger.info(\"Running search migration\")\n            try:\n                from orchestrator.db import db\n                from orchestrator.search.llm_migration import run_migration\n\n                with db.engine.begin() as connection:\n                    run_migration(connection)\n            except ImportError as e:\n                logger.error(\n                    \"Unable to run search migration. Please install search dependencies: \"\n                    \"`pip install orchestrator-core[search]`\",\n                    error=str(e),\n                )\n                raise\n</code></pre>"},{"location":"reference-docs/app/agentic-app/#orchestrator.agentic_app.LLMOrchestratorCore.__init__","title":"__init__","text":"<pre><code>__init__(\n    *args: typing.Any,\n    llm_settings: orchestrator.llm_settings.LLMSettings = llm_settings,\n    agent_model: pydantic_ai.models.openai.OpenAIChatModel | str | None = None,\n    **kwargs: typing.Any\n) -&gt; None\n</code></pre> <p>Initialize the <code>LLMOrchestratorCore</code> class.</p> <p>This class extends <code>OrchestratorCore</code> with LLM features (search and agent). It runs the search migration based on feature flags.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>typing.Any</code>, default:                   <code>()</code> )           \u2013            <p>All the normal arguments passed to the <code>OrchestratorCore</code> class.</p> </li> <li> <code>llm_settings</code>               (<code>orchestrator.llm_settings.LLMSettings</code>, default:                   <code>orchestrator.agentic_app.LLMOrchestratorCore.llm_settings</code> )           \u2013            <p>A class of settings for the LLM</p> </li> <li> <code>agent_model</code>               (<code>pydantic_ai.models.openai.OpenAIChatModel | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the agent model (defaults to llm_settings.AGENT_MODEL)</p> </li> <li> <code>**kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed to the <code>OrchestratorCore</code> class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/agentic_app.py</code> <pre><code>def __init__(\n    self,\n    *args: Any,\n    llm_settings: LLMSettings = llm_settings,\n    agent_model: \"OpenAIChatModel | str | None\" = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the `LLMOrchestratorCore` class.\n\n    This class extends `OrchestratorCore` with LLM features (search and agent).\n    It runs the search migration based on feature flags.\n\n    Args:\n        *args: All the normal arguments passed to the `OrchestratorCore` class.\n        llm_settings: A class of settings for the LLM\n        agent_model: Override the agent model (defaults to llm_settings.AGENT_MODEL)\n        **kwargs: Additional arguments passed to the `OrchestratorCore` class.\n\n    Returns:\n        None\n    \"\"\"\n    self.llm_settings = llm_settings\n    self.agent_model = agent_model or llm_settings.AGENT_MODEL\n\n    super().__init__(*args, **kwargs)\n\n    # Run search migration if search or agent is enabled\n    if self.llm_settings.SEARCH_ENABLED or self.llm_settings.AGENT_ENABLED:\n        logger.info(\"Running search migration\")\n        try:\n            from orchestrator.db import db\n            from orchestrator.search.llm_migration import run_migration\n\n            with db.engine.begin() as connection:\n                run_migration(connection)\n        except ImportError as e:\n            logger.error(\n                \"Unable to run search migration. Please install search dependencies: \"\n                \"`pip install orchestrator-core[search]`\",\n                error=str(e),\n            )\n            raise\n</code></pre>"},{"location":"reference-docs/app/app/","title":"app.py","text":"<p>The app.py module is used in <code>orchestrator-core</code> for actually running the entire WFO FastAPI backend and the CLI.</p>"},{"location":"reference-docs/app/app/#fastapi-backend","title":"FastAPI Backend","text":"<p>The code for the WFO's Fast API backend is very well documented, so look through the functions used in this module here:</p> <p>A great example of how to use the functions available in app.py with your own <code>main.py</code> when you instantiate your own instance of the orchestrator can be seen in the example orchestrator repository's <code>main.py</code> file.</p> <pre><code># Copyright 2019-2025 SURF, ESnet, G\u00c9ANT.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport typer\nfrom orchestrator import app_settings\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.db import init_database\nfrom orchestrator.log_config import LOGGER_OVERRIDES\n\nfrom nwastdlib.logging import initialise_logging\n\n\ndef init_cli_app() -&gt; typer.Typer:\n    initialise_logging(LOGGER_OVERRIDES)\n    init_database(app_settings)\n    return core_cli()\n\n\nif __name__ == \"__main__\":\n    init_cli_app()\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app","title":"orchestrator.app","text":"<p>The main application module.</p> <p>This module contains the main <code>OrchestratorCore</code> class for the <code>FastAPI</code> backend and provides the ability to run the CLI.</p>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore","title":"OrchestratorCore","text":"<p>               Bases: <code>fastapi.applications.FastAPI</code></p> Source code in <code>orchestrator/app.py</code> <pre><code>class OrchestratorCore(FastAPI):\n    graphql_router: Any | None = None\n    broadcast_thread: ProcessDataBroadcastThread | None = None\n\n    def __init__(\n        self,\n        title: str = \"The Orchestrator\",\n        description: str = \"The orchestrator is a project that enables users to run workflows.\",\n        openapi_url: str = \"/api/openapi.json\",\n        docs_url: str = \"/api/docs\",\n        redoc_url: str = \"/api/redoc\",\n        version: str = __version__,\n        default_response_class: type[Response] = JSONResponse,\n        base_settings: AppSettings = app_settings,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the Orchestrator.\n\n        Args:\n            title: Name of the application.\n            description: Description of the application.\n            openapi_url: Location of the OpenAPI endpoint.\n            docs_url: Location of the docs endpoint.\n            redoc_url: Location of the redoc endpoint.\n            version: Version of the application.\n            default_response_class: Override the default response class.\n            base_settings: Settings for the application.\n            **kwargs: Any additional keyword arguments are sent to the\n\n        Returns:\n            None\n        \"\"\"\n        initialise_logging(LOGGER_OVERRIDES)\n        init_model_loaders()\n        if base_settings.ENABLE_GRAPHQL_STATS_EXTENSION:\n            monitor_sqlalchemy_queries()\n\n        self.auth_manager = AuthManager()\n        self.base_settings = base_settings\n        websocket_manager = init_websocket_manager(base_settings)\n        distlock_manager = init_distlock_manager(base_settings)\n\n        startup_functions: list[Callable] = [distlock_manager.connect_redis]\n        shutdown_functions: list[Callable] = [distlock_manager.disconnect_redis]\n        if websocket_manager.enabled:\n            startup_functions.append(websocket_manager.connect_redis)\n            shutdown_functions.extend([websocket_manager.disconnect_all, websocket_manager.disconnect_redis])\n\n        # Initialize worker status monitor for accurate running process counts\n        self.worker_status_monitor = get_worker_status_monitor()\n        shutdown_functions.append(self.worker_status_monitor.stop)\n\n        if base_settings.EXECUTOR == ExecutorType.THREADPOOL:\n            # Only need broadcast thread when using threadpool executor\n            self.broadcast_thread = ProcessDataBroadcastThread(websocket_manager)\n            startup_functions.append(self.broadcast_thread.start)\n            shutdown_functions.append(self.broadcast_thread.stop)\n\n        super().__init__(\n            title=title,\n            description=description,\n            openapi_url=openapi_url,\n            docs_url=docs_url,\n            redoc_url=redoc_url,\n            version=version,\n            default_response_class=default_response_class,\n            on_startup=startup_functions,\n            on_shutdown=shutdown_functions,\n            **kwargs,\n        )\n\n        self.include_router(api_router, prefix=\"/api\")\n\n        init_database(base_settings)\n\n        self.add_middleware(ClearStructlogContextASGIMiddleware)\n        self.add_middleware(SessionMiddleware, secret_key=base_settings.SESSION_SECRET)\n        self.add_middleware(DBSessionMiddleware, database=db)\n        origins = base_settings.CORS_ORIGINS.split(\",\")\n        self.add_middleware(\n            CORSMiddleware,\n            allow_origins=origins,\n            allow_methods=base_settings.CORS_ALLOW_METHODS,\n            allow_headers=base_settings.CORS_ALLOW_HEADERS,\n            expose_headers=base_settings.CORS_EXPOSE_HEADERS,\n        )\n\n        self.add_exception_handler(FormException, form_error_handler)  # type: ignore[arg-type]\n        self.add_exception_handler(ProblemDetailException, problem_detail_handler)  # type: ignore[arg-type]\n        add_exception_handler(self)\n\n        if base_settings.ENABLE_PROMETHEUS_METRICS_ENDPOINT:\n            initialize_default_metrics()\n            metrics_app = make_asgi_app(registry=ORCHESTRATOR_METRICS_REGISTRY)\n            self.mount(\"/api/metrics\", metrics_app)\n\n        @self.router.get(\"/\", response_model=str, response_class=JSONResponse, include_in_schema=False)\n        def _index() -&gt; str:\n            return \"Orchestrator Core\"\n\n    def add_sentry(\n        self,\n        sentry_dsn: str,\n        trace_sample_rate: float,\n        server_name: str,\n        environment: str,\n        release: str | None = GIT_COMMIT_HASH,\n        **sentry_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Register sentry to your application.\n\n        Sentry is an application monitoring toolkit.\n\n        Args:\n            sentry_dsn: The location where sentry traces are posted to.\n            trace_sample_rate: The sample rate\n            server_name: The name of the application\n            environment: Production or development\n            release: Version of the application\n            **sentry_kwargs: Any sentry keyword arguments\n\n        Returns:\n            None\n\n        \"\"\"\n        logger.info(\"Adding Sentry middleware to app\", app=self.title)\n        if self.base_settings.EXECUTOR == ExecutorType.WORKER:\n            from sentry_sdk.integrations.celery import CeleryIntegration\n\n            sentry_integrations.append(CeleryIntegration())\n\n        if self.graphql_router:\n            sentry_integrations.append(StrawberryIntegration(async_execution=True))\n\n        sentry_sdk.init(\n            dsn=sentry_dsn,\n            traces_sample_rate=trace_sample_rate,\n            server_name=server_name,\n            environment=environment,\n            release=f\"orchestrator@{release}\",\n            integrations=sentry_integrations,\n            propagate_traces=True,\n            profiles_sample_rate=trace_sample_rate,\n            **sentry_kwargs,\n        )\n\n    @staticmethod\n    def register_subscription_models(product_to_subscription_model_mapping: dict[str, type[SubscriptionModel]]) -&gt; None:\n        \"\"\"Register your subscription models.\n\n        This method is needed to register your subscription models inside the orchestrator core.\n\n        Args:\n            product_to_subscription_model_mapping: The dictionary should contain a mapping of products to SubscriptionModels.\n                The selection will be done depending on the name of the product.\n\n        Returns:\n            None:\n\n        Examples:\n            &gt;&gt;&gt; product_to_subscription_model_mapping = { # doctest:+SKIP\n            ...     \"Generic Product One\": GenericProductModel,\n            ...     \"Generic Product Two\": GenericProductModel,\n            ... }\n\n        \"\"\"\n        SUBSCRIPTION_MODEL_REGISTRY.update(product_to_subscription_model_mapping)\n\n    def register_graphql(\n        self: \"OrchestratorCore\",\n        # mypy 1.9 cannot properly inspect these, fixed in 1.15\n        query: Any = Query,  # type: ignore\n        mutation: Any = Mutation,  # type: ignore\n        register_models: bool = True,\n        subscription_interface: Any = SubscriptionInterface,\n        graphql_models: StrawberryModelType | None = None,\n        scalar_overrides: ScalarOverrideType | None = None,\n        extensions: list | None = None,\n        custom_context_getter: ContextGetterFactory | None = None,\n    ) -&gt; None:\n        new_router = create_graphql_router(\n            self.auth_manager,\n            query,\n            mutation,\n            register_models,\n            subscription_interface,\n            self.broadcast_thread,\n            graphql_models,\n            scalar_overrides,\n            extensions=extensions,\n            custom_context_getter=custom_context_getter,\n        )\n        if not self.graphql_router:\n            self.graphql_router = new_router\n            self.include_router(new_router, prefix=\"/api/graphql\")\n        else:\n            self.graphql_router.schema = new_router.schema\n\n    def register_authentication(self, authentication_instance: OIDCAuth) -&gt; None:\n        \"\"\"Registers a custom authentication instance for the application.\n\n        Use this method to replace the default OIDC authentication mechanism with a custom one,\n        enhancing the security and tailoring user authentication to specific needs of the application.\n\n        Args:\n            authentication_instance (OIDCAuth): The custom OIDCAuth instance to use.\n\n        Returns:\n            None\n        \"\"\"\n        self.auth_manager.authentication = authentication_instance\n\n    def register_authorization(self, authorization_instance: Authorization) -&gt; None:\n        \"\"\"Registers a custom authorization instance to manage user permissions and access controls.\n\n        This method enables customization of the authorization logic, defining what authenticated users\n        can do within the application. It integrates with the application's security framework to enforce\n        permission checks tailored to your requirements.\n\n        Args:\n            authorization_instance (Authorization): The custom Authorization instance to use.\n\n        Returns:\n            None\n        \"\"\"\n        self.auth_manager.authorization = authorization_instance\n\n    def register_graphql_authorization(self, graphql_authorization_instance: GraphqlAuthorization) -&gt; None:\n        \"\"\"Registers a custom GraphQL-specific authorization instance for managing access controls in GraphQL operations.\n\n        This provides an opportunity to apply specialized authorization rules and policies for GraphQL interactions,\n        enhancing security where the default settings do not suffice.\n\n        Args:\n            graphql_authorization_instance (GraphqlAuthorization): The instance responsible for GraphQL-specific authorization.\n\n        Returns:\n            None\n        \"\"\"\n        self.auth_manager.graphql_authorization = graphql_authorization_instance\n\n    def register_internal_authorize_callback(self, callback: Authorizer) -&gt; None:\n        \"\"\"Registers the authorize_callback for WFO's internal workflows and tasks.\n\n        Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks\n        for workflows defined in orchestrator-core itself.\n        However, this assignment MUST be made before any workflows are run.\n\n        Args:\n            callback (Authorizer): The async Authorizer to run for the `authorize_callback` argument of internal workflows.\n\n        Returns:\n            None\n        \"\"\"\n        authorizers = get_authorizers()\n        authorizers.internal_authorize_callback = callback\n\n    def register_internal_retry_auth_callback(self, callback: Authorizer) -&gt; None:\n        \"\"\"Registers the retry_auth_callback for WFO's internal workflows and tasks.\n\n        Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks\n        for workflows defined in orchestrator-core itself.\n        However, this assignment MUST be made before any workflows are run.\n\n        Args:\n            callback (Authorizer): The async Authorizer to run for the `retry_auth_callback` argument of internal workflows.\n\n        Returns:\n            None\n        \"\"\"\n        authorizers = get_authorizers()\n        authorizers.internal_retry_auth_callback = callback\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.__init__","title":"__init__","text":"<pre><code>__init__(\n    title: str = \"The Orchestrator\",\n    description: str = \"The orchestrator is a project that enables users to run workflows.\",\n    openapi_url: str = \"/api/openapi.json\",\n    docs_url: str = \"/api/docs\",\n    redoc_url: str = \"/api/redoc\",\n    version: str = __version__,\n    default_response_class: type[starlette.responses.Response] = JSONResponse,\n    base_settings: orchestrator.settings.AppSettings = app_settings,\n    **kwargs: typing.Any\n) -&gt; None\n</code></pre> <p>Initialize the Orchestrator.</p> <p>Parameters:</p> <ul> <li> <code>title</code>               (<code>str</code>, default:                   <code>'The Orchestrator'</code> )           \u2013            <p>Name of the application.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>'The orchestrator is a project that enables users to run workflows.'</code> )           \u2013            <p>Description of the application.</p> </li> <li> <code>openapi_url</code>               (<code>str</code>, default:                   <code>'/api/openapi.json'</code> )           \u2013            <p>Location of the OpenAPI endpoint.</p> </li> <li> <code>docs_url</code>               (<code>str</code>, default:                   <code>'/api/docs'</code> )           \u2013            <p>Location of the docs endpoint.</p> </li> <li> <code>redoc_url</code>               (<code>str</code>, default:                   <code>'/api/redoc'</code> )           \u2013            <p>Location of the redoc endpoint.</p> </li> <li> <code>version</code>               (<code>str</code>, default:                   <code>orchestrator.__version__</code> )           \u2013            <p>Version of the application.</p> </li> <li> <code>default_response_class</code>               (<code>type[starlette.responses.Response]</code>, default:                   <code>starlette.responses.JSONResponse</code> )           \u2013            <p>Override the default response class.</p> </li> <li> <code>base_settings</code>               (<code>orchestrator.settings.AppSettings</code>, default:                   <code>orchestrator.settings.app_settings</code> )           \u2013            <p>Settings for the application.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Any additional keyword arguments are sent to the</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def __init__(\n    self,\n    title: str = \"The Orchestrator\",\n    description: str = \"The orchestrator is a project that enables users to run workflows.\",\n    openapi_url: str = \"/api/openapi.json\",\n    docs_url: str = \"/api/docs\",\n    redoc_url: str = \"/api/redoc\",\n    version: str = __version__,\n    default_response_class: type[Response] = JSONResponse,\n    base_settings: AppSettings = app_settings,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the Orchestrator.\n\n    Args:\n        title: Name of the application.\n        description: Description of the application.\n        openapi_url: Location of the OpenAPI endpoint.\n        docs_url: Location of the docs endpoint.\n        redoc_url: Location of the redoc endpoint.\n        version: Version of the application.\n        default_response_class: Override the default response class.\n        base_settings: Settings for the application.\n        **kwargs: Any additional keyword arguments are sent to the\n\n    Returns:\n        None\n    \"\"\"\n    initialise_logging(LOGGER_OVERRIDES)\n    init_model_loaders()\n    if base_settings.ENABLE_GRAPHQL_STATS_EXTENSION:\n        monitor_sqlalchemy_queries()\n\n    self.auth_manager = AuthManager()\n    self.base_settings = base_settings\n    websocket_manager = init_websocket_manager(base_settings)\n    distlock_manager = init_distlock_manager(base_settings)\n\n    startup_functions: list[Callable] = [distlock_manager.connect_redis]\n    shutdown_functions: list[Callable] = [distlock_manager.disconnect_redis]\n    if websocket_manager.enabled:\n        startup_functions.append(websocket_manager.connect_redis)\n        shutdown_functions.extend([websocket_manager.disconnect_all, websocket_manager.disconnect_redis])\n\n    # Initialize worker status monitor for accurate running process counts\n    self.worker_status_monitor = get_worker_status_monitor()\n    shutdown_functions.append(self.worker_status_monitor.stop)\n\n    if base_settings.EXECUTOR == ExecutorType.THREADPOOL:\n        # Only need broadcast thread when using threadpool executor\n        self.broadcast_thread = ProcessDataBroadcastThread(websocket_manager)\n        startup_functions.append(self.broadcast_thread.start)\n        shutdown_functions.append(self.broadcast_thread.stop)\n\n    super().__init__(\n        title=title,\n        description=description,\n        openapi_url=openapi_url,\n        docs_url=docs_url,\n        redoc_url=redoc_url,\n        version=version,\n        default_response_class=default_response_class,\n        on_startup=startup_functions,\n        on_shutdown=shutdown_functions,\n        **kwargs,\n    )\n\n    self.include_router(api_router, prefix=\"/api\")\n\n    init_database(base_settings)\n\n    self.add_middleware(ClearStructlogContextASGIMiddleware)\n    self.add_middleware(SessionMiddleware, secret_key=base_settings.SESSION_SECRET)\n    self.add_middleware(DBSessionMiddleware, database=db)\n    origins = base_settings.CORS_ORIGINS.split(\",\")\n    self.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_methods=base_settings.CORS_ALLOW_METHODS,\n        allow_headers=base_settings.CORS_ALLOW_HEADERS,\n        expose_headers=base_settings.CORS_EXPOSE_HEADERS,\n    )\n\n    self.add_exception_handler(FormException, form_error_handler)  # type: ignore[arg-type]\n    self.add_exception_handler(ProblemDetailException, problem_detail_handler)  # type: ignore[arg-type]\n    add_exception_handler(self)\n\n    if base_settings.ENABLE_PROMETHEUS_METRICS_ENDPOINT:\n        initialize_default_metrics()\n        metrics_app = make_asgi_app(registry=ORCHESTRATOR_METRICS_REGISTRY)\n        self.mount(\"/api/metrics\", metrics_app)\n\n    @self.router.get(\"/\", response_model=str, response_class=JSONResponse, include_in_schema=False)\n    def _index() -&gt; str:\n        return \"Orchestrator Core\"\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.add_sentry","title":"add_sentry","text":"<pre><code>add_sentry(\n    sentry_dsn: str,\n    trace_sample_rate: float,\n    server_name: str,\n    environment: str,\n    release: str | None = GIT_COMMIT_HASH,\n    **sentry_kwargs: typing.Any\n) -&gt; None\n</code></pre> <p>Register sentry to your application.</p> <p>Sentry is an application monitoring toolkit.</p> <p>Parameters:</p> <ul> <li> <code>sentry_dsn</code>               (<code>str</code>)           \u2013            <p>The location where sentry traces are posted to.</p> </li> <li> <code>trace_sample_rate</code>               (<code>float</code>)           \u2013            <p>The sample rate</p> </li> <li> <code>server_name</code>               (<code>str</code>)           \u2013            <p>The name of the application</p> </li> <li> <code>environment</code>               (<code>str</code>)           \u2013            <p>Production or development</p> </li> <li> <code>release</code>               (<code>str | None</code>, default:                   <code>orchestrator.version.GIT_COMMIT_HASH</code> )           \u2013            <p>Version of the application</p> </li> <li> <code>**sentry_kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Any sentry keyword arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def add_sentry(\n    self,\n    sentry_dsn: str,\n    trace_sample_rate: float,\n    server_name: str,\n    environment: str,\n    release: str | None = GIT_COMMIT_HASH,\n    **sentry_kwargs: Any,\n) -&gt; None:\n    \"\"\"Register sentry to your application.\n\n    Sentry is an application monitoring toolkit.\n\n    Args:\n        sentry_dsn: The location where sentry traces are posted to.\n        trace_sample_rate: The sample rate\n        server_name: The name of the application\n        environment: Production or development\n        release: Version of the application\n        **sentry_kwargs: Any sentry keyword arguments\n\n    Returns:\n        None\n\n    \"\"\"\n    logger.info(\"Adding Sentry middleware to app\", app=self.title)\n    if self.base_settings.EXECUTOR == ExecutorType.WORKER:\n        from sentry_sdk.integrations.celery import CeleryIntegration\n\n        sentry_integrations.append(CeleryIntegration())\n\n    if self.graphql_router:\n        sentry_integrations.append(StrawberryIntegration(async_execution=True))\n\n    sentry_sdk.init(\n        dsn=sentry_dsn,\n        traces_sample_rate=trace_sample_rate,\n        server_name=server_name,\n        environment=environment,\n        release=f\"orchestrator@{release}\",\n        integrations=sentry_integrations,\n        propagate_traces=True,\n        profiles_sample_rate=trace_sample_rate,\n        **sentry_kwargs,\n    )\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_authentication","title":"register_authentication","text":"<pre><code>register_authentication(\n    authentication_instance: oauth2_lib.fastapi.OIDCAuth,\n) -&gt; None\n</code></pre> <p>Registers a custom authentication instance for the application.</p> <p>Use this method to replace the default OIDC authentication mechanism with a custom one, enhancing the security and tailoring user authentication to specific needs of the application.</p> <p>Parameters:</p> <ul> <li> <code>authentication_instance</code>               (<code>oauth2_lib.fastapi.OIDCAuth</code>)           \u2013            <p>The custom OIDCAuth instance to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def register_authentication(self, authentication_instance: OIDCAuth) -&gt; None:\n    \"\"\"Registers a custom authentication instance for the application.\n\n    Use this method to replace the default OIDC authentication mechanism with a custom one,\n    enhancing the security and tailoring user authentication to specific needs of the application.\n\n    Args:\n        authentication_instance (OIDCAuth): The custom OIDCAuth instance to use.\n\n    Returns:\n        None\n    \"\"\"\n    self.auth_manager.authentication = authentication_instance\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_authorization","title":"register_authorization","text":"<pre><code>register_authorization(\n    authorization_instance: oauth2_lib.fastapi.Authorization,\n) -&gt; None\n</code></pre> <p>Registers a custom authorization instance to manage user permissions and access controls.</p> <p>This method enables customization of the authorization logic, defining what authenticated users can do within the application. It integrates with the application's security framework to enforce permission checks tailored to your requirements.</p> <p>Parameters:</p> <ul> <li> <code>authorization_instance</code>               (<code>oauth2_lib.fastapi.Authorization</code>)           \u2013            <p>The custom Authorization instance to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def register_authorization(self, authorization_instance: Authorization) -&gt; None:\n    \"\"\"Registers a custom authorization instance to manage user permissions and access controls.\n\n    This method enables customization of the authorization logic, defining what authenticated users\n    can do within the application. It integrates with the application's security framework to enforce\n    permission checks tailored to your requirements.\n\n    Args:\n        authorization_instance (Authorization): The custom Authorization instance to use.\n\n    Returns:\n        None\n    \"\"\"\n    self.auth_manager.authorization = authorization_instance\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_graphql_authorization","title":"register_graphql_authorization","text":"<pre><code>register_graphql_authorization(\n    graphql_authorization_instance: oauth2_lib.fastapi.GraphqlAuthorization,\n) -&gt; None\n</code></pre> <p>Registers a custom GraphQL-specific authorization instance for managing access controls in GraphQL operations.</p> <p>This provides an opportunity to apply specialized authorization rules and policies for GraphQL interactions, enhancing security where the default settings do not suffice.</p> <p>Parameters:</p> <ul> <li> <code>graphql_authorization_instance</code>               (<code>oauth2_lib.fastapi.GraphqlAuthorization</code>)           \u2013            <p>The instance responsible for GraphQL-specific authorization.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def register_graphql_authorization(self, graphql_authorization_instance: GraphqlAuthorization) -&gt; None:\n    \"\"\"Registers a custom GraphQL-specific authorization instance for managing access controls in GraphQL operations.\n\n    This provides an opportunity to apply specialized authorization rules and policies for GraphQL interactions,\n    enhancing security where the default settings do not suffice.\n\n    Args:\n        graphql_authorization_instance (GraphqlAuthorization): The instance responsible for GraphQL-specific authorization.\n\n    Returns:\n        None\n    \"\"\"\n    self.auth_manager.graphql_authorization = graphql_authorization_instance\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_internal_authorize_callback","title":"register_internal_authorize_callback","text":"<pre><code>register_internal_authorize_callback(\n    callback: orchestrator.utils.auth.Authorizer,\n) -&gt; None\n</code></pre> <p>Registers the authorize_callback for WFO's internal workflows and tasks.</p> <p>Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks for workflows defined in orchestrator-core itself. However, this assignment MUST be made before any workflows are run.</p> <p>Parameters:</p> <ul> <li> <code>callback</code>               (<code>orchestrator.utils.auth.Authorizer</code>)           \u2013            <p>The async Authorizer to run for the <code>authorize_callback</code> argument of internal workflows.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def register_internal_authorize_callback(self, callback: Authorizer) -&gt; None:\n    \"\"\"Registers the authorize_callback for WFO's internal workflows and tasks.\n\n    Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks\n    for workflows defined in orchestrator-core itself.\n    However, this assignment MUST be made before any workflows are run.\n\n    Args:\n        callback (Authorizer): The async Authorizer to run for the `authorize_callback` argument of internal workflows.\n\n    Returns:\n        None\n    \"\"\"\n    authorizers = get_authorizers()\n    authorizers.internal_authorize_callback = callback\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_internal_retry_auth_callback","title":"register_internal_retry_auth_callback","text":"<pre><code>register_internal_retry_auth_callback(\n    callback: orchestrator.utils.auth.Authorizer,\n) -&gt; None\n</code></pre> <p>Registers the retry_auth_callback for WFO's internal workflows and tasks.</p> <p>Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks for workflows defined in orchestrator-core itself. However, this assignment MUST be made before any workflows are run.</p> <p>Parameters:</p> <ul> <li> <code>callback</code>               (<code>orchestrator.utils.auth.Authorizer</code>)           \u2013            <p>The async Authorizer to run for the <code>retry_auth_callback</code> argument of internal workflows.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>orchestrator/app.py</code> <pre><code>def register_internal_retry_auth_callback(self, callback: Authorizer) -&gt; None:\n    \"\"\"Registers the retry_auth_callback for WFO's internal workflows and tasks.\n\n    Since RBAC policies are applied to workflows via decorator, this enables registration of callbacks\n    for workflows defined in orchestrator-core itself.\n    However, this assignment MUST be made before any workflows are run.\n\n    Args:\n        callback (Authorizer): The async Authorizer to run for the `retry_auth_callback` argument of internal workflows.\n\n    Returns:\n        None\n    \"\"\"\n    authorizers = get_authorizers()\n    authorizers.internal_retry_auth_callback = callback\n</code></pre>"},{"location":"reference-docs/app/app/#orchestrator.app.OrchestratorCore.register_subscription_models","title":"register_subscription_models  <code>staticmethod</code>","text":"<pre><code>register_subscription_models(\n    product_to_subscription_model_mapping: dict[\n        str, type[orchestrator.domain.SubscriptionModel]\n    ],\n) -&gt; None\n</code></pre> <p>Register your subscription models.</p> <p>This method is needed to register your subscription models inside the orchestrator core.</p> <p>Parameters:</p> <ul> <li> <code>product_to_subscription_model_mapping</code>               (<code>dict[str, type[orchestrator.domain.SubscriptionModel]]</code>)           \u2013            <p>The dictionary should contain a mapping of products to SubscriptionModels. The selection will be done depending on the name of the product.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product_to_subscription_model_mapping = {\n...     \"Generic Product One\": GenericProductModel,\n...     \"Generic Product Two\": GenericProductModel,\n... }\n</code></pre> Source code in <code>orchestrator/app.py</code> <pre><code>@staticmethod\ndef register_subscription_models(product_to_subscription_model_mapping: dict[str, type[SubscriptionModel]]) -&gt; None:\n    \"\"\"Register your subscription models.\n\n    This method is needed to register your subscription models inside the orchestrator core.\n\n    Args:\n        product_to_subscription_model_mapping: The dictionary should contain a mapping of products to SubscriptionModels.\n            The selection will be done depending on the name of the product.\n\n    Returns:\n        None:\n\n    Examples:\n        &gt;&gt;&gt; product_to_subscription_model_mapping = { # doctest:+SKIP\n        ...     \"Generic Product One\": GenericProductModel,\n        ...     \"Generic Product Two\": GenericProductModel,\n        ... }\n\n    \"\"\"\n    SUBSCRIPTION_MODEL_REGISTRY.update(product_to_subscription_model_mapping)\n</code></pre>"},{"location":"reference-docs/app/app/#cli-app","title":"CLI App","text":"<p>The orchestrator core also has a CLI application that is documented in detail here. You can bring this into your <code>main.py</code> file so that you can run the orchestrator CLI for development like so:</p> <pre><code>if __name__ == \"__main__\":\n    core_cli()\n</code></pre>"},{"location":"reference-docs/app/settings-overview/","title":"Settings overview in Orchestrator","text":"<p>You can use the <code>api/settings/overview</code> endpoint to get an overview of the settings that are used in the application. This endpoint provides a JSON response that contains the settings that are defined in the application. The settings are grouped by their names and sensitive values are masked for security reasons. Per default, the application settings are used to configure the application. The settings are defined in the <code>orchestrator.settings.py</code> module and can be used to configure the application.</p> <p>An example of the settings is shown below:</p> <pre><code>from orchestrator.settings import BaseSettings\n\n\nclass AppSettings(BaseSettings):\n    TESTING: bool = True\n    SESSION_SECRET: OrchSecretStr = \"\".join(secrets.choice(string.ascii_letters) for i in range(16))  # type: ignore\n    CORS_ORIGINS: str = \"*\"\n    ...\n    EXPOSE_SETTINGS: bool = False\n    EXPOSE_OAUTH_SETTINGS: bool = False\n\n\napp_settings = AppSettings()\n\nif app_settings.EXPOSE_SETTINGS:\n    expose_settings(\"app_settings\", app_settings)  # type: ignore\n\nif app_settings.EXPOSE_OAUTH_SETTINGS:\n    expose_settings(\"oauth2lib_settings\", oauth2lib_settings)  # type: ignore\n</code></pre> <p>What you see above is the default settings for the application. The settings are defined in the <code>orchestrator.settings.py</code> module and can be used to configure the application. The <code>EXPOSE_SETTINGS</code> and <code>EXPOSE_OAUTH_SETTINGS</code> flags are used to control whether the settings should be exposed via the <code>api/settings/overview</code> endpoint, the result looks like this:</p> <pre><code>[\n  {\n    \"name\": \"app_settings\",\n    \"variables\": [\n      {\n        \"env_name\": \"TESTING\",\n        \"env_value\": false\n      },\n      {\n        \"env_name\": \"SESSION_SECRET\",\n        \"env_value\": \"**********\"\n      },\n      {\n        \"env_name\": \"CORS_ORIGINS\",\n        \"env_value\": \"*\"\n      }\n    ]\n  }\n]\n</code></pre> <p>The <code>app_settings</code> in the example above is a name of the settings class that is registered to be exposed.</p>"},{"location":"reference-docs/app/settings-overview/#exposing-your-settings","title":"Exposing your settings","text":"<p>In order to expose your settings, you need to register them using the <code>expose_settings()</code> function. This function takes two arguments: the name of the settings class and the instance of the settings class.</p> <pre><code>from orchestrator.settings import expose_settings, BaseSettings\n\n\nclass MySettings(BaseSettings):\n    debug: bool = True\n\n\nmy_settings = MySettings()\n\nexpose_settings(\"my_settings\", my_settings)\n</code></pre>"},{"location":"reference-docs/app/settings-overview/#lifecycle-validation-mode","title":"Lifecycle Validation Mode","text":"<p>The Lifecycle Validation Mode is used to validate in workflow steps that a subscription model has been instantiated with the correct product type class for its lifecycle status. E.g. a subscription model with a lifecycle status of <code>PROVISIONING</code> should be instantiated with a product type class that has a lifecycle status of <code>PROVISIONING</code>.</p> <p>You can run the application with three different lifecycle validation modes:</p> <pre><code>class LifecycleValidationMode(strEnum):\n    STRICT = \"strict\"\n    LOOSE = \"loose\" # default\n    IGNORED = \"ignored\"\n</code></pre> <p>The Lifecycle Validation Mode can be set using the <code>LIFECYCLE_VALIDATION_MODE</code> environment variable. The default setting is <code>loose</code>. The different modes are explained below:</p> <ul> <li><code>strict</code>: The application will enforce strict checks on the lifecycle status of subscription models in workflow steps. If any issues are found, the application will raise an error and stop running.</li> </ul> <p>Error in <code>strict</code> mode</p> <pre><code>2025-09-26 13:46:56 [error    ] Subscription of type &lt;class 'products.product_types.l3vpn.L3Vpn'&gt; should use &lt;class 'products.product_types.l3vpn.L3VpnProvisioning'&gt; for lifecycle status 'provisioning' [orchestrator.domain.lifecycle] func=re_deploy_nso process_id=6a483f61-c21d-47be-8390-bbd608c59a77 workflow_name=create_l3vpn\n</code></pre> <p>Warning</p> <p>A workflow failing on this error will not be recoverable, so it is advised to test this in development first.</p> <ul> <li><code>loose</code>: The application will log warnings for any issues with the lifecycle validation in workflow steps, but it will still run normally.</li> </ul> <p>Warning in <code>loose</code> mode</p> <pre><code>2025-09-26 13:46:56 [warning    ] Subscription of type &lt;class 'products.product_types.l3vpn.L3Vpn'&gt; should use &lt;class 'products.product_types.l3vpn.L3VpnProvisioning'&gt; for lifecycle status 'provisioning' [orchestrator.domain.lifecycle] func=re_deploy_nso\n</code></pre> <ul> <li><code>ignored</code>: The application will ignore all lifecycle validation issues and run normally.</li> </ul>"},{"location":"reference-docs/app/settings-overview/#masking-secrets","title":"Masking Secrets","text":"<p>The following rules apply when exposing settings:</p>"},{"location":"reference-docs/app/settings-overview/#rules-for-masking-secrets","title":"Rules for Masking Secrets","text":"<ul> <li>Keys containing <code>\"password\"</code> or <code>\"secret\"</code> in their names are masked.</li> <li><code>SecretStr</code> from <code>from pydantic import SecretStr</code> are masked.</li> <li><code>SecretStr</code> from <code>from orchestrator.utils.expose_settings import SecretStr</code> are masked.</li> <li><code>PostgresDsn</code> from <code>from pydantic import PostgresDsn</code> are masked.</li> </ul>"},{"location":"reference-docs/app/settings-overview/#overview-of-appsettings-class","title":"Overview of AppSettings class","text":"<p>Toggle the source code block below to get a complete overview of the current application settings. </p>"},{"location":"reference-docs/app/settings-overview/#orchestrator.settings.AppSettings","title":"orchestrator.settings.AppSettings","text":"<p>               Bases: <code>pydantic_settings.BaseSettings</code></p> Source code in <code>orchestrator/settings.py</code> <pre><code>class AppSettings(BaseSettings):\n    TESTING: bool = True\n    SESSION_SECRET: OrchSecretStr = \"\".join(secrets.choice(string.ascii_letters) for i in range(16))  # type: ignore\n    CORS_ORIGINS: str = \"*\"\n    CORS_ALLOW_METHODS: list[str] = [\"GET\", \"PUT\", \"PATCH\", \"POST\", \"DELETE\", \"OPTIONS\", \"HEAD\"]\n    CORS_ALLOW_HEADERS: list[str] = [\"If-None-Match\", \"Authorization\", \"If-Match\", \"Content-Type\"]\n    CORS_EXPOSE_HEADERS: list[str] = [\n        \"Cache-Control\",\n        \"Content-Language\",\n        \"Content-Length\",\n        \"Content-Type\",\n        \"Expires\",\n        \"Last-Modified\",\n        \"Pragma\",\n        \"Content-Range\",\n        \"ETag\",\n    ]\n    ENVIRONMENT: str = \"local\"\n    EXECUTOR: str = ExecutorType.THREADPOOL\n    WORKFLOWS_SWAGGER_HOST: str = \"localhost\"\n    WORKFLOWS_GUI_URI: str = \"http://localhost:3000\"\n    BASE_URL: str = \"http://localhost:8080\"  # Base URL for the API (used for generating export URLs)\n    DATABASE_URI: PostgresDsn = \"postgresql://nwa:nwa@localhost/orchestrator-core\"  # type: ignore\n    MAX_WORKERS: int = 5\n    WORKER_STATUS_INTERVAL: int = Field(\n        5, description=\"Interval in seconds for updating worker status count in the background monitor\"\n    )\n    MAIL_SERVER: str = \"localhost\"\n    MAIL_PORT: int = 25\n    MAIL_STARTTLS: bool = False\n    CACHE_URI: RedisDsn = \"redis://localhost:6379/0\"  # type: ignore\n    CACHE_HMAC_SECRET: OrchSecretStr | None = None  # HMAC signing key, used when pickling results in the cache\n    REDIS_RETRY_COUNT: NonNegativeInt = Field(\n        2, description=\"Number of retries for redis connection errors/timeouts, 0 to disable\"\n    )  # More info: https://redis-py.readthedocs.io/en/stable/retry.html\n    ENABLE_DISTLOCK_MANAGER: bool = True\n    DISTLOCK_BACKEND: str = \"memory\"\n    CC_NOC: int = 0\n    SERVICE_NAME: str = \"orchestrator-core\"\n    LOGGING_HOST: str = \"localhost\"\n    LOG_LEVEL: str = \"DEBUG\"\n    SLACK_ENGINE_SETTINGS_HOOK_ENABLED: bool = False\n    SLACK_ENGINE_SETTINGS_HOOK_URL: str = \"\"\n    TRACING_ENABLED: bool = False\n    TRACE_HOST: str = \"http://localhost:4317\"\n    TRANSLATIONS_DIR: Path | None = None\n    WEBSOCKET_BROADCASTER_URL: OrchSecretStr = \"memory://\"  # type: ignore\n    ENABLE_WEBSOCKETS: bool = True\n    DISABLE_INSYNC_CHECK: bool = False\n    DEFAULT_PRODUCT_WORKFLOWS: list[str] = [\"modify_note\"]\n    SKIP_MODEL_FOR_MIGRATION_DB_DIFF: list[str] = []\n    SERVE_GRAPHQL_UI: bool = True\n    FEDERATION_ENABLED: bool = False\n    DEFAULT_CUSTOMER_FULLNAME: str = \"Default::Orchestrator-Core Customer\"\n    DEFAULT_CUSTOMER_SHORTCODE: str = \"default-cust\"\n    DEFAULT_CUSTOMER_IDENTIFIER: str = \"59289a57-70fb-4ff5-9c93-10fe67b12434\"\n    TASK_LOG_RETENTION_DAYS: int = 3\n    ENABLE_GRAPHQL_DEPRECATION_CHECKER: bool = True\n    ENABLE_GRAPHQL_PROFILING_EXTENSION: bool = False\n    ENABLE_GRAPHQL_STATS_EXTENSION: bool = False\n    ENABLE_PROMETHEUS_METRICS_ENDPOINT: bool = False\n    VALIDATE_OUT_OF_SYNC_SUBSCRIPTIONS: bool = False\n    FILTER_BY_MODE: Literal[\"partial\", \"exact\"] = \"exact\"\n    EXPOSE_SETTINGS: bool = False\n    EXPOSE_OAUTH_SETTINGS: bool = False\n    LIFECYCLE_VALIDATION_MODE: LifecycleValidationMode = LifecycleValidationMode.LOOSE\n</code></pre>"},{"location":"reference-docs/domain_models/generator/","title":"Generator","text":"<p>If all of this domain modelling process seems like too much work, then good news, as all clever engineers before us have done, we've fixed that with YAML! Using the WFO CLI, you can generate your product types directly from a YAML. For more information on how to do that, check out the CLI <code>generate</code> command documentation.</p>"},{"location":"reference-docs/domain_models/model_attributes/","title":"Model Attributes","text":""},{"location":"reference-docs/domain_models/model_attributes/#overview","title":"Overview","text":"<p>For understanding the various attributes that can live on a domain model, let's look again at the example <code>Node</code> product and the <code>NodeBlock</code> product block from the example workflow orchestrator:</p>"},{"location":"reference-docs/domain_models/model_attributes/#node-product-type","title":"<code>Node</code> Product Type","text":"Example: <code>example-orchestrator/products/product_types/node.py</code> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.node import NodeBlock, NodeBlockInactive, NodeBlockProvisioning\nfrom pydantic_forms.types import strEnum\n\n\nclass Node_Type(strEnum):\n    Cisco = \"Cisco\"\n    Nokia = \"Nokia\"\n    Cumulus = \"Cumulus\"\n    FRR = \"FRR\"\n\n\nclass NodeInactive(SubscriptionModel, is_base=True):\n    node_type: Node_Type\n    node: NodeBlockInactive\n\n\nclass NodeProvisioning(NodeInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    node_type: Node_Type\n    node: NodeBlockProvisioning\n\n\nclass Node(NodeProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    node_type: Node_Type\n    node: NodeBlock\n</code></pre>"},{"location":"reference-docs/domain_models/model_attributes/#nodeblock-product-block","title":"<code>NodeBlock</code> Product Block","text":"Example: <code>example-orchestrator/products/product_blocks/node.py</code> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\nfrom pydantic import computed_field\n\n\nclass NodeBlockInactive(ProductBlockModel, product_block_name=\"Node\"):\n    role_id: int | None = None\n    type_id: int | None = None\n    site_id: int | None = None\n    node_status: str | None = None  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str | None = None\n    node_description: str | None = None\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    ipv4_ipam_id: int | None = None\n    ipv6_ipam_id: int | None = None\n\n\nclass NodeBlockProvisioning(NodeBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    role_id: int\n    type_id: int\n    site_id: int\n    node_status: str  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str\n    node_description: str | None = None\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    ipv4_ipam_id: int | None = None\n    ipv6_ipam_id: int | None = None\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def title(self) -&gt; str:\n        return f\"node {self.node_name} status {self.node_status}\"\n\n\nclass NodeBlock(NodeBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    role_id: int\n    type_id: int\n    site_id: int\n    node_status: str  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str\n    node_description: str | None = None\n    ims_id: int\n    nrm_id: int\n    ipv4_ipam_id: int\n    ipv6_ipam_id: int\n</code></pre>"},{"location":"reference-docs/domain_models/model_attributes/#resource-types","title":"Resource Types","text":"<p>A resource type is simply an attribute on a product block's python class. These are used to store values on a domain model that are mutable and will be changed over the lifecycle of the product. These are type-annotated so that they can be safely serialized and de-serialized from the database and so that pydantic can validate what you store on your domain model. When these attributes are added to a domain model, the appropriate database table must be populated via a migration. This can be handled automatically for you by the <code>migrate-domain-models</code> command in the WFO CLI. To better understand how this looks from a database standpoint, you can see the database table that needs to be populated here:</p> <p>You can see what a generated migration looks like that includes a new resource-type here by looking at the <code>\"resources\":</code> key inside of the <code>Node</code> product block :</p> Example: <code>example-orchestrator/migrations/versions/schema/2023-10-27_a84ca2e5e4db_add_node.py</code> <pre><code>\"\"\"Add node product.\n\nRevision ID: a84ca2e5e4db\nRevises: a77227fe5455\nCreate Date: 2023-10-27 11:25:40.994878\n\n\"\"\"\n\nfrom uuid import uuid4\n\nfrom alembic import op\nfrom orchestrator.migrations.helpers import (\n    create,\n    create_workflow,\n    delete,\n    delete_workflow,\n    ensure_default_workflows,\n)\nfrom orchestrator.targets import Target\n\n# revision identifiers, used by Alembic.\nrevision = \"a84ca2e5e4db\"\ndown_revision = \"a77227fe5455\"\nbranch_labels = None\ndepends_on = None\n\nnew_products = {\n    \"products\": {\n        \"node Cisco\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Cisco\",\n            },\n        },\n        \"node Nokia\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Nokia\",\n            },\n        },\n        \"node Cumulus\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"Cumulus\",\n            },\n        },\n        \"node FRR\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"FRR\",\n            },\n        },\n    },\n    \"product_blocks\": {\n        \"Node\": {\n            \"product_block_id\": uuid4(),\n            \"description\": \"node product block\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"resources\": {\n                \"role_id\": \"ID in CMDB of role of the node in the network\",\n                \"type_id\": \"ID in CMDB of type of the node\",\n                \"site_id\": \"ID in CMDB of site where the node is located\",\n                \"node_status\": \"Operational status of the node\",\n                \"node_name\": \"Unique name of the node\",\n                \"node_description\": \"Description of the node\",\n                \"ims_id\": \"ID of the node in the inventory management system\",\n                \"nrm_id\": \"ID of the node in the network resource manager\",\n                \"ipv4_ipam_id\": \"ID of the node\u2019s iPv4 loopback address in IPAM\",\n                \"ipv6_ipam_id\": \"ID of the node\u2019s iPv6 loopback address in IPAM\",\n            },\n            \"depends_on_block_relations\": [],\n        },\n    },\n    \"workflows\": {},\n}\n\nnew_workflows = [\n    {\n        \"name\": \"create_node\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_node\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Modify node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_sync_ports\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Update node interfaces\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"terminate_node\",\n        \"target\": Target.TERMINATE,\n        \"description\": \"Terminate node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"validate_node\",\n        \"target\": Target.SYSTEM,\n        \"description\": \"Validate node\",\n        \"product_type\": \"Node\",\n    },\n]\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    create(conn, new_products)\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n    ensure_default_workflows(conn)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n\n    delete(conn, new_products)\n</code></pre> <p>Finally, when a domain model is populated, the values that you put onto the subscription are stored in the <code>SubscriptionInstanceValueTable</code> in the database, as seen here:</p>"},{"location":"reference-docs/domain_models/model_attributes/#orchestrator.db.models.ResourceTypeTable","title":"orchestrator.db.models.ResourceTypeTable","text":"<p>               Bases: <code>orchestrator.db.database.BaseModel</code></p> Source code in <code>orchestrator/db/models.py</code> <pre><code>class ResourceTypeTable(BaseModel):\n    __tablename__ = \"resource_types\"\n\n    resource_type_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    resource_type = mapped_column(String(510), nullable=False, unique=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH))\n\n    product_blocks = relationship(\n        \"ProductBlockTable\", secondary=product_block_resource_type_association, back_populates=\"resource_types\"\n    )\n</code></pre>"},{"location":"reference-docs/domain_models/model_attributes/#orchestrator.db.models.SubscriptionInstanceValueTable","title":"orchestrator.db.models.SubscriptionInstanceValueTable","text":"<p>               Bases: <code>orchestrator.db.database.BaseModel</code></p> Source code in <code>orchestrator/db/models.py</code> <pre><code>class SubscriptionInstanceValueTable(BaseModel):\n    __tablename__ = \"subscription_instance_values\"\n\n    subscription_instance_value_id = mapped_column(\n        UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True\n    )\n    subscription_instance_id = mapped_column(\n        UUIDType,\n        ForeignKey(\"subscription_instances.subscription_instance_id\", ondelete=\"CASCADE\"),\n        index=True,\n        nullable=False,\n    )\n    resource_type_id = mapped_column(\n        UUIDType, ForeignKey(\"resource_types.resource_type_id\"), nullable=False, index=True\n    )\n    value = mapped_column(String(RESOURCE_VALUE_LENGTH), nullable=False)\n\n    resource_type = relationship(\"ResourceTypeTable\", lazy=\"subquery\")\n    subscription_instance = relationship(\"SubscriptionInstanceTable\", back_populates=\"values\")\n</code></pre>"},{"location":"reference-docs/domain_models/model_attributes/#fixed-inputs","title":"Fixed Inputs","text":"<p>Fixed inputs are an immutable attribute that will not be changed over the lifecycle of the product. These are attributes with a hard-coded value that live on the root of the Product Type class definition. Fixed Inputs only live on Product Types, not Product Blocks. To better understand how this looks from a database standpoint, you can see the database table that needs to be populated here:</p> <p>Using the same example as above, you can see what a generated migration file looks like that includes a new resource-type here by looking at the <code>\"fixed_inputs\":</code> key inside of the <code>node Cisco</code> or <code>node Nokia</code> product type:</p> Example <pre><code>\"\"\"Add node product.\n\nRevision ID: a84ca2e5e4db\nRevises: a77227fe5455\nCreate Date: 2023-10-27 11:25:40.994878\n\n\"\"\"\n\nfrom uuid import uuid4\n\nfrom alembic import op\nfrom orchestrator.migrations.helpers import (\n    create,\n    create_workflow,\n    delete,\n    delete_workflow,\n    ensure_default_workflows,\n)\nfrom orchestrator.targets import Target\n\n# revision identifiers, used by Alembic.\nrevision = \"a84ca2e5e4db\"\ndown_revision = \"a77227fe5455\"\nbranch_labels = None\ndepends_on = None\n\nnew_products = {\n    \"products\": {\n        \"node Cisco\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Cisco\",\n            },\n        },\n        \"node Nokia\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Nokia\",\n            },\n        },\n        \"node Cumulus\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"Cumulus\",\n            },\n        },\n        \"node FRR\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"FRR\",\n            },\n        },\n    },\n    \"product_blocks\": {\n        \"Node\": {\n            \"product_block_id\": uuid4(),\n            \"description\": \"node product block\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"resources\": {\n                \"role_id\": \"ID in CMDB of role of the node in the network\",\n                \"type_id\": \"ID in CMDB of type of the node\",\n                \"site_id\": \"ID in CMDB of site where the node is located\",\n                \"node_status\": \"Operational status of the node\",\n                \"node_name\": \"Unique name of the node\",\n                \"node_description\": \"Description of the node\",\n                \"ims_id\": \"ID of the node in the inventory management system\",\n                \"nrm_id\": \"ID of the node in the network resource manager\",\n                \"ipv4_ipam_id\": \"ID of the node\u2019s iPv4 loopback address in IPAM\",\n                \"ipv6_ipam_id\": \"ID of the node\u2019s iPv6 loopback address in IPAM\",\n            },\n            \"depends_on_block_relations\": [],\n        },\n    },\n    \"workflows\": {},\n}\n\nnew_workflows = [\n    {\n        \"name\": \"create_node\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_node\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Modify node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_sync_ports\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Update node interfaces\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"terminate_node\",\n        \"target\": Target.TERMINATE,\n        \"description\": \"Terminate node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"validate_node\",\n        \"target\": Target.SYSTEM,\n        \"description\": \"Validate node\",\n        \"product_type\": \"Node\",\n    },\n]\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    create(conn, new_products)\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n    ensure_default_workflows(conn)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n\n    delete(conn, new_products)\n</code></pre> <p>Note that because the <code>Node_Type</code> enum has two choices (<code>Cisco</code> or <code>Nokia</code>), we generated two different products, one for each choice. Using fixed inputs in this manner allows you to easily create multiple products that share all of the same attributes aside from their fixed input without having to duplicate a bunch of domain model code. Then, when you are writing your workflows, you can handle the difference between these products by being aware of this fixed input.</p>"},{"location":"reference-docs/domain_models/model_attributes/#orchestrator.db.models.FixedInputTable","title":"orchestrator.db.models.FixedInputTable","text":"<p>               Bases: <code>orchestrator.db.database.BaseModel</code></p> Source code in <code>orchestrator/db/models.py</code> <pre><code>class FixedInputTable(BaseModel):\n    __tablename__ = \"fixed_inputs\"\n    __table_args__ = (UniqueConstraint(\"name\", \"product_id\"), {\"extend_existing\": True})\n\n    fixed_input_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False)\n    value = mapped_column(String(), nullable=False)\n    created_at = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=text(\"current_timestamp()\"))\n    product_id = mapped_column(UUIDType, ForeignKey(\"products.product_id\", ondelete=\"CASCADE\"), nullable=False)\n\n    product = relationship(\"ProductTable\", back_populates=\"fixed_inputs\")\n</code></pre>"},{"location":"reference-docs/domain_models/overview/","title":"Overview","text":"<p>Domain models are one of the core concepts to understand in the workflow orchestrator. To understand the high-level concepts of a domain model, start by reading the domain models page in the Architecture section of the documentation. Additionally, for an example design of domain models that could be commonly implemented by NRENs, see this document created for G\u00c9ANT.</p>"},{"location":"reference-docs/domain_models/product_blocks/","title":"Product Blocks","text":""},{"location":"reference-docs/domain_models/product_blocks/#defining-a-product-block","title":"Defining a Product Block","text":"<p>Warning</p> <p>You should only have one root Product Block on your domain model!</p> <p>A Product Block is a reusable collection of product attributes that lives under a Product Type (which we covered in the Product Types section of these docs). A Product Block allows you to define the bulk of the attributes that you want to assign to your Product definition. At it's most basic, you will have a root product block that is stored on your Product Type, as shown in the Product Types documentation. Building off of that example, let's examine a basic product block by examining the <code>NodeBlock</code> product block from the example workflow orchestrator:</p> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\nfrom pydantic import computed_field\n\n\nclass NodeBlockInactive(ProductBlockModel, product_block_name=\"Node\"):\n    role_id: int | None = None\n    type_id: int | None = None\n    site_id: int | None = None\n    node_status: str | None = None  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str | None = None\n    node_description: str | None = None\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    ipv4_ipam_id: int | None = None\n    ipv6_ipam_id: int | None = None\n\n\nclass NodeBlockProvisioning(NodeBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    role_id: int\n    type_id: int\n    site_id: int\n    node_status: str  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str\n    node_description: str | None = None\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    ipv4_ipam_id: int | None = None\n    ipv6_ipam_id: int | None = None\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def title(self) -&gt; str:\n        return f\"node {self.node_name} status {self.node_status}\"\n\n\nclass NodeBlock(NodeBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    role_id: int\n    type_id: int\n    site_id: int\n    node_status: str  # should be NodeStatus, but strEnum is not supported (yet?)\n    node_name: str\n    node_description: str | None = None\n    ims_id: int\n    nrm_id: int\n    ipv4_ipam_id: int\n    ipv6_ipam_id: int\n</code></pre> <p>Breaking this product block down a bit more, we see 3 classes, <code>NodeBlockInactive</code>, <code>NodeBlockProvisioning</code>, and finally <code>NodeBlock</code>. These three classes are built off of each-other, with the lowest level class (<code>NodeBlockInactive</code>) based off of the <code>ProductBlockModel</code> base class. These classes have a number of attributes, referred to as \"Resource Types\", which you can read more about here. Looking at this <code>ProductBlockModel</code> base class tells us a lot about how to use it in our workflows:</p>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel","title":"orchestrator.domain.base.ProductBlockModel","text":"<p>               Bases: <code>orchestrator.domain.base.DomainModel</code></p> <p>This is the base class for all product block models.</p> <p>This class should have been called SubscriptionInstanceModel.</p> <p>ProductTable Blocks are represented as dataclasses with pydantic runtime validation.</p> <p>Different stages of a subscription lifecycle could require different product block definition.Mainly to support mandatory fields when a subscription is active. To support this a lifecycle specific product block definition can be created by subclassing the generic product block with keyword argument 'lifecycle' and overriding its fields.</p> <p>All product blocks are related to a database ProductBlockTable object through the <code>product_block_name</code> that is given as class keyword argument.</p> <p>Define a product block:</p> <pre><code>&gt;&gt;&gt; class BlockInactive(ProductBlockModel, product_block_name=\"Virtual Circuit\"):\n...    int_field: Optional[int] = None\n...    str_field: Optional[str] = None\n\n&gt;&gt;&gt; class Block(BlockInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n...    int_field: int\n...    str_field: str\n</code></pre> <p>This example defines a product_block with two different contraints based on lifecycle. <code>Block</code> is valid only for <code>ACTIVE</code> And <code>BlockInactive</code> for all other states. <code>product_block_name</code> must be defined on the base class and need not to be defined on the others</p> <p>Create a new empty product block:</p> <pre><code>&gt;&gt;&gt; example1 = BlockInactive()  # doctest: +SKIP\n</code></pre> <p>Create a new instance based on a dict in the state:</p> <pre><code>&gt;&gt;&gt; example2 = BlockInactive(**state)  # doctest:+SKIP\n</code></pre> <p>To retrieve a ProductBlockModel from the database:</p> <pre><code>&gt;&gt;&gt; BlockInactive.from_db(subscription_instance_id)  # doctest:+SKIP\n</code></pre> Source code in <code>orchestrator/domain/base.py</code> <pre><code>class ProductBlockModel(DomainModel):\n    r\"\"\"This is the base class for all product block models.\n\n    This class should have been called SubscriptionInstanceModel.\n\n    ProductTable Blocks are represented as dataclasses with pydantic runtime validation.\n\n    Different stages of a subscription lifecycle could require different product block\n    definition.Mainly to support mandatory fields when a subscription is active. To support\n    this a lifecycle specific product block definition can be created by subclassing the\n    generic product block with keyword argument 'lifecycle' and overriding its fields.\n\n    All product blocks are related to a database ProductBlockTable object through the `product_block_name`\n    that is given as class keyword argument.\n\n    Define a product block:\n\n        &gt;&gt;&gt; class BlockInactive(ProductBlockModel, product_block_name=\"Virtual Circuit\"):\n        ...    int_field: Optional[int] = None\n        ...    str_field: Optional[str] = None\n\n        &gt;&gt;&gt; class Block(BlockInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n        ...    int_field: int\n        ...    str_field: str\n\n    This example defines a product_block with two different contraints based on lifecycle. `Block` is valid only for `ACTIVE`\n    And `BlockInactive` for all other states.\n    `product_block_name` must be defined on the base class and need not to be defined on the others\n\n    Create a new empty product block:\n\n        &gt;&gt;&gt; example1 = BlockInactive()  # doctest: +SKIP\n\n    Create a new instance based on a dict in the state:\n\n        &gt;&gt;&gt; example2 = BlockInactive(**state)  # doctest:+SKIP\n\n    To retrieve a ProductBlockModel from the database:\n\n        &gt;&gt;&gt; BlockInactive.from_db(subscription_instance_id)  # doctest:+SKIP\n    \"\"\"\n\n    registry: ClassVar[dict[str, type[\"ProductBlockModel\"]]] = {}  # pragma: no mutate\n    __names__: ClassVar[set[str]] = set()\n    product_block_id: ClassVar[UUID]\n    description: ClassVar[str]\n    tag: ClassVar[str]\n    _db_model: SubscriptionInstanceTable | None = PrivateAttr(default=None)\n\n    # Product block name. This needs to be an instance var because its part of the API (we expose it to the frontend)\n    # Is actually optional since abstract classes don't have it.\n    # TODO #427 name is used as both a ClassVar and a pydantic Field, for which Pydantic 2.x raises\n    #  warnings (which may become errors)\n    name: str | None\n    subscription_instance_id: UUID\n    owner_subscription_id: UUID\n    label: str | None = None\n\n    @classmethod\n    def _fix_pb_data(cls) -&gt; None:\n        if not cls.name:\n            raise ValueError(f\"Cannot create instance of abstract class. Use one of {cls.__names__}\")\n\n        # Would have been nice to do this in __init_subclass__ but that runs outside the app context so we can't\n        # access the db. So now we do it just before we instantiate the instance\n        if not hasattr(cls, \"product_block_id\"):\n            product_block = db.session.scalars(\n                select(ProductBlockTable).filter(ProductBlockTable.name == cls.name)\n            ).one()\n            cls.product_block_id = product_block.product_block_id\n            cls.description = product_block.description\n            cls.tag = product_block.tag\n\n    @classmethod\n    def __pydantic_init_subclass__(  # type: ignore[override]\n        cls,\n        *,\n        product_block_name: str | None = None,\n        lifecycle: list[SubscriptionLifecycle] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__pydantic_init_subclass__(lifecycle=lifecycle, **kwargs)\n\n        if product_block_name is not None:\n            # This is a concrete product block base class (so not a abstract super class or a specific lifecycle version)\n            cls.name = product_block_name\n            cls.__base_type__ = cls\n            cls.__names__ = {cls.name}\n            cls.registry[cls.name] = cls\n        elif lifecycle is None:\n            # Abstract class, no product block name\n            cls.name = None\n            cls.__names__ = set()\n\n        # For everything except abstract classes\n        if cls.name is not None:\n            register_specialized_type(cls, lifecycle)\n\n            # Add ourselves to any super class. That way we can match a superclass to an instance when loading\n            for klass in cls.__mro__:\n                if issubclass(klass, ProductBlockModel):\n                    klass.__names__.add(cls.name)\n\n        cls.__doc__ = make_product_block_docstring(cls, lifecycle)\n\n    @classmethod\n    def diff_product_block_in_database(cls) -&gt; dict[str, set[str]]:\n        \"\"\"Return any differences between the attrs defined on the domain model and those on product blocks in the database.\n\n        This is only needed to check if the domain model and database models match which would be done during testing...\n        \"\"\"\n        if not cls.name:\n            # This is a superclass we can't check that\n            return {}\n\n        product_block_db = db.session.scalars(\n            select(ProductBlockTable).where(ProductBlockTable.name == cls.name)\n        ).one_or_none()\n        product_blocks_in_db = {pb.name for pb in product_block_db.depends_on} if product_block_db else set()\n\n        product_blocks_in_model = cls._get_depends_on_product_block_types()\n        product_blocks_types_in_model = get_depends_on_product_block_type_list(product_blocks_in_model)\n\n        product_blocks_in_model = set(flatten(map(attrgetter(\"__names__\"), product_blocks_types_in_model)))  # type: ignore\n\n        missing_product_blocks_in_db = product_blocks_in_model - product_blocks_in_db  # type: ignore\n        missing_product_blocks_in_model = product_blocks_in_db - product_blocks_in_model  # type: ignore\n\n        resource_types_model = set(cls._non_product_block_fields_)\n        resource_types_db = {rt.resource_type for rt in product_block_db.resource_types} if product_block_db else set()\n\n        missing_resource_types_in_db = resource_types_model - resource_types_db\n        missing_resource_types_in_model = resource_types_db - resource_types_model\n\n        logger.debug(\n            \"ProductBlockTable blocks diff\",\n            product_block_db=product_block_db.name if product_block_db else None,\n            product_blocks_in_db=product_blocks_in_db,\n            product_blocks_in_model=product_blocks_in_model,\n            resource_types_db=resource_types_db,\n            resource_types_model=resource_types_model,\n            missing_product_blocks_in_db=missing_product_blocks_in_db,\n            missing_product_blocks_in_model=missing_product_blocks_in_model,\n            missing_resource_types_in_db=missing_resource_types_in_db,\n            missing_resource_types_in_model=missing_resource_types_in_model,\n        )\n\n        missing_data: dict[str, Any] = {}\n        for product_block_model in product_blocks_types_in_model:\n            if product_block_model.name == cls.name or product_block_model.name in missing_data:\n                continue\n            missing_data.update(product_block_model.diff_product_block_in_database())\n\n        diff: dict[str, set[str]] = {\n            k: v\n            for k, v in {\n                \"missing_product_blocks_in_db\": missing_product_blocks_in_db,\n                \"missing_product_blocks_in_model\": missing_product_blocks_in_model,\n                \"missing_resource_types_in_db\": missing_resource_types_in_db,\n                \"missing_resource_types_in_model\": missing_resource_types_in_model,\n            }.items()\n            if v\n        }\n\n        if diff:\n            missing_data[cls.name] = diff\n\n        return missing_data\n\n    @classmethod\n    def new(cls: type[B], subscription_id: UUID, **kwargs: Any) -&gt; B:\n        \"\"\"Create a new empty product block.\n\n        We need to use this instead of the normal constructor because that assumes you pass in\n        all required values. That is cumbersome since that means creating a tree of product blocks.\n\n        This is similar to `from_product_id()`\n        \"\"\"\n        sub_instances = cls._init_instances(subscription_id, set(kwargs.keys()))\n\n        subscription_instance_id = uuid4()\n\n        # Make sure product block stuff is already set if new is the first usage of this class\n        cls._fix_pb_data()\n\n        db_model = SubscriptionInstanceTable(\n            product_block_id=cls.product_block_id,\n            subscription_instance_id=subscription_instance_id,\n            subscription_id=subscription_id,\n        )\n        db.session.enable_relationship_loading(db_model)\n\n        if kwargs_name := kwargs.pop(\"name\", None):\n            # Not allowed to change the product block model name at runtime. This is only possible through\n            # the `product_block_name=..` metaclass parameter\n            logger.warning(\"Ignoring `name` keyword to ProductBlockModel.new()\", rejected_name=kwargs_name)\n        model = cls(\n            name=cls.name,\n            subscription_instance_id=subscription_instance_id,\n            owner_subscription_id=subscription_id,\n            label=db_model.label,\n            **sub_instances,\n            **kwargs,\n        )\n        model.db_model = db_model\n        return model\n\n    @classmethod\n    def _load_instances_values(cls, instance_values: list[SubscriptionInstanceValueTable]) -&gt; dict[str, str]:\n        \"\"\"Load non product block fields (instance values).\n\n        Args:\n            instance_values: List of instance values from database\n\n        Returns:\n            Dict of fields to use for constructor\n\n        \"\"\"\n        instance_values_dict: State = {}\n        list_field_names = set()\n\n        # Set default values\n        for field_name, field_type in cls._non_product_block_fields_.items():\n            # Ensure that empty lists are handled OK\n            if is_list_type(field_type):\n                instance_values_dict[field_name] = []\n                list_field_names.add(field_name)\n            elif is_optional_type(field_type):\n                # Initialize \"optional required\" fields\n                instance_values_dict[field_name] = None\n\n        for siv in instance_values:\n            # check the type of the siv in the instance and act accordingly: only lists and scalar values supported\n            resource_type_name = siv.resource_type.resource_type\n            if resource_type_name in list_field_names:\n                instance_values_dict[resource_type_name].append(siv.value)\n            else:\n                instance_values_dict[resource_type_name] = siv.value\n\n        return instance_values_dict\n\n    @classmethod\n    def _from_other_lifecycle(\n        cls: type[B],\n        other: \"ProductBlockModel\",\n        status: SubscriptionLifecycle,\n        subscription_id: UUID,\n    ) -&gt; B:\n        \"\"\"Create new domain model from instance while changing the status.\n\n        This makes sure we always have a specific instance.\n        \"\"\"\n        if not cls.__base_type__:\n            cls = ProductBlockModel.registry.get(other.name, cls)  # type:ignore\n            cls = lookup_specialized_type(cls, status)\n\n        data = cls._data_from_lifecycle(other, status, subscription_id)\n\n        cls._fix_pb_data()\n        model = cls(**data)\n        model.db_model = other.db_model\n        return model\n\n    @classmethod\n    def from_db(\n        cls: type[B],\n        subscription_instance_id: UUID | None = None,\n        subscription_instance: SubscriptionInstanceTable | None = None,\n        status: SubscriptionLifecycle | None = None,\n    ) -&gt; B:\n        \"\"\"Create a product block based on a subscription instance from the database.\n\n        This function is similar to `from_subscription()`\n\n            &gt;&gt;&gt; subscription_instance_id = KNOWN_UUID_IN_DB  # doctest:+SKIP\n            &gt;&gt;&gt; si_from_db = db.SubscriptionInstanceTable.query.get(subscription_instance_id)  # doctest:+SKIP\n            &gt;&gt;&gt; example3 = ProductBlockModel.from_db(subscription_instance=si_from_db)  # doctest:+SKIP\n            &gt;&gt;&gt; example4 = ProductBlockModel.from_db(subscription_instance_id=subscription_instance_id)  # doctest:+SKIP\n        \"\"\"\n        # Fill values from actual subscription\n        if subscription_instance_id:\n            subscription_instance = db.session.get(SubscriptionInstanceTable, subscription_instance_id)\n        if subscription_instance:\n            subscription_instance_id = subscription_instance.subscription_instance_id\n        assert subscription_instance_id  # noqa: S101\n        assert subscription_instance  # noqa: S101\n\n        if not status:\n            status = SubscriptionLifecycle(subscription_instance.subscription.status)\n\n        if not cls.__base_type__:\n            cls = ProductBlockModel.registry.get(subscription_instance.product_block.name, cls)  # type:ignore\n            cls = lookup_specialized_type(cls, status)\n\n        elif not issubclass(cls, lookup_specialized_type(cls, status)):\n            raise ValueError(f\"{cls} is not valid for lifecycle {status}\")\n\n        label = subscription_instance.label\n\n        instance_values = cls._load_instances_values(subscription_instance.values)\n        sub_instances = cls._load_instances(\n            subscription_instance.depends_on,\n            status,\n            match_domain_attr=True,\n            in_use_by_id_boundary=subscription_instance_id,\n        )\n\n        cls._fix_pb_data()\n        try:\n            model = cls(\n                name=cls.name,\n                subscription_instance_id=subscription_instance_id,\n                owner_subscription_id=subscription_instance.subscription_id,\n                label=label,\n                subscription=subscription_instance.subscription,\n                **instance_values,  # type: ignore\n                **sub_instances,\n            )\n            model.db_model = subscription_instance\n\n            return model\n        except ValidationError:\n            logger.exception(\n                \"Subscription is not correct in database\",\n                loaded_instance_values=instance_values,\n                loaded_sub_instances=sub_instances,\n            )\n            raise\n\n    def _save_instance_values(\n        self, product_block: ProductBlockTable, current_values: list[SubscriptionInstanceValueTable]\n    ) -&gt; list[SubscriptionInstanceValueTable]:\n        \"\"\"Save non product block fields (instance values).\n\n        Returns:\n            List of database instances values to save\n\n        \"\"\"\n        resource_types = {rt.resource_type: rt for rt in product_block.resource_types}\n        current_values_dict: dict[str, list[SubscriptionInstanceValueTable]] = defaultdict(list)\n        for siv in current_values:\n            current_values_dict[siv.resource_type.resource_type].append(siv)\n\n        subscription_instance_values = []\n        for field_name, field_type in self._non_product_block_fields_.items():\n            assert (  # noqa: S101\n                field_name in resource_types\n            ), f\"Domain model {self.__class__} does not match the ProductBlockTable {product_block.name}, missing: {field_name} {resource_types}\"\n\n            resource_type = resource_types[field_name]\n            value = getattr(self, field_name)\n            if value is None:\n                continue\n            if is_list_type(field_type):\n                for val, siv in zip_longest(value, current_values_dict[field_name]):\n                    if val is not None:\n                        if siv:\n                            siv.value = str(val)\n                            subscription_instance_values.append(siv)\n                        else:\n                            subscription_instance_values.append(\n                                SubscriptionInstanceValueTable(resource_type=resource_type, value=str(val))\n                            )\n            else:\n                if field_name in current_values_dict:\n                    current_value = current_values_dict[field_name][0]\n                    current_value.value = str(value)\n                    subscription_instance_values.append(current_value)\n                else:\n                    subscription_instance_values.append(\n                        SubscriptionInstanceValueTable(resource_type=resource_type, value=str(value))\n                    )\n        return subscription_instance_values\n\n    def _set_instance_domain_model_attrs(\n        self,\n        subscription_instance: SubscriptionInstanceTable,\n        subscription_instance_mapping: dict[str, list[SubscriptionInstanceTable]],\n    ) -&gt; None:\n        \"\"\"Save the domain model attribute to the database.\n\n        This function iterates through the subscription instances and stores the domain model attribute in the\n        hierarchy relationship.\n\n        Args:\n            subscription_instance: The subscription instance object.\n            subscription_instance_mapping: a mapping of the domain model attribute a underlying instances\n\n        Returns:\n            None\n\n        \"\"\"\n        depends_on_block_relations = []\n        # Set the domain_model_attrs in the database\n        for domain_model_attr, instances in subscription_instance_mapping.items():\n            instance: SubscriptionInstanceTable\n            for index, instance in enumerate(instances):\n                relation = SubscriptionInstanceRelationTable(\n                    in_use_by_id=subscription_instance.subscription_instance_id,\n                    depends_on_id=instance.subscription_instance_id,\n                    order_id=index,\n                    domain_model_attr=domain_model_attr,\n                )\n                depends_on_block_relations.append(relation)\n        subscription_instance.depends_on_block_relations = depends_on_block_relations\n\n    def save(\n        self, *, subscription_id: UUID, status: SubscriptionLifecycle\n    ) -&gt; tuple[list[SubscriptionInstanceTable], SubscriptionInstanceTable]:\n        \"\"\"Save the current model instance to the database.\n\n        This means saving the whole tree of subscription instances and separately saving all instance\n        values for this instance. This is called automatically when you return a subscription to the state\n        in a workflow step.\n\n        Args:\n            status: current SubscriptionLifecycle to check if all constraints match\n            subscription_id: Optional subscription id needed if this is a new model\n\n        Returns:\n            List of saved instances\n\n        \"\"\"\n        if not self.name:\n            raise ValueError(f\"Cannot create instance of abstract class. Use one of {self.__names__}\")\n\n        # Make sure we have a valid subscription instance database model\n        subscription_instance: SubscriptionInstanceTable | None = db.session.get(\n            SubscriptionInstanceTable, self.subscription_instance_id\n        )\n        if subscription_instance:\n            # Make sure we do not use a mapped session.\n            db.session.refresh(subscription_instance)\n\n            # If this is a \"foreign\" instance we just stop saving and return it so only its relation is saved\n            # We should not touch these themselves\n            if self.owner_subscription_id != subscription_id:\n                return [], subscription_instance\n\n            self.db_model = subscription_instance\n        elif subscription_instance := self.db_model:\n            # We only need to add to the session if the subscription_instance does not exist.\n            db.session.add(subscription_instance)\n        else:\n            raise ValueError(\"Cannot save ProductBlockModel without a db_model\")\n\n        subscription_instance.subscription_id = subscription_id\n\n        db.session.flush()\n\n        # Everything is ok, make sure we are of the right class\n        specialized_type = lookup_specialized_type(self.__class__, status)\n        if specialized_type and not isinstance(self, specialized_type):\n            raise ValueError(\n                f\"Lifecycle status {status} requires specialized type {specialized_type!r}, was: {type(self)!r}\"\n            )\n\n        # Actually save stuff\n        subscription_instance.label = self.label\n        subscription_instance.values = self._save_instance_values(\n            subscription_instance.product_block, subscription_instance.values\n        )\n\n        sub_instances, depends_on_instances = self._save_instances(subscription_id, status)\n\n        # Save the subscription instances relations.\n        self._set_instance_domain_model_attrs(subscription_instance, depends_on_instances)\n\n        return sub_instances + [subscription_instance], subscription_instance\n\n    @property\n    def subscription(self) -&gt; SubscriptionTable | None:\n        return self.db_model.subscription if self.db_model else None\n\n    @property\n    def db_model(self) -&gt; SubscriptionInstanceTable | None:\n        if not self._db_model:\n            self._db_model = db.session.execute(\n                select(SubscriptionInstanceTable).where(\n                    SubscriptionInstanceTable.subscription_instance_id == self.subscription_instance_id\n                )\n            ).scalar_one_or_none()\n        return self._db_model\n\n    @db_model.setter\n    def db_model(self, value: SubscriptionInstanceTable) -&gt; None:\n        self._db_model = value\n\n    @property\n    def in_use_by(self) -&gt; list[SubscriptionInstanceTable]:  # TODO check where used, might need eagerloading\n        \"\"\"This provides a list of product blocks that depend on this product block.\"\"\"\n        return self.db_model.in_use_by if self.db_model else []\n\n    @property\n    def depends_on(self) -&gt; list[SubscriptionInstanceTable]:  # TODO check where used, might need eagerloading\n        \"\"\"This provides a list of product blocks that this product block depends on.\"\"\"\n        return self.db_model.depends_on if self.db_model else []\n</code></pre>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.depends_on","title":"depends_on  <code>property</code>","text":"<pre><code>depends_on: list[orchestrator.db.SubscriptionInstanceTable]\n</code></pre> <p>This provides a list of product blocks that this product block depends on.</p>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.in_use_by","title":"in_use_by  <code>property</code>","text":"<pre><code>in_use_by: list[orchestrator.db.SubscriptionInstanceTable]\n</code></pre> <p>This provides a list of product blocks that depend on this product block.</p>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.diff_product_block_in_database","title":"diff_product_block_in_database  <code>classmethod</code>","text":"<pre><code>diff_product_block_in_database() -&gt; dict[str, set[str]]\n</code></pre> <p>Return any differences between the attrs defined on the domain model and those on product blocks in the database.</p> <p>This is only needed to check if the domain model and database models match which would be done during testing...</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef diff_product_block_in_database(cls) -&gt; dict[str, set[str]]:\n    \"\"\"Return any differences between the attrs defined on the domain model and those on product blocks in the database.\n\n    This is only needed to check if the domain model and database models match which would be done during testing...\n    \"\"\"\n    if not cls.name:\n        # This is a superclass we can't check that\n        return {}\n\n    product_block_db = db.session.scalars(\n        select(ProductBlockTable).where(ProductBlockTable.name == cls.name)\n    ).one_or_none()\n    product_blocks_in_db = {pb.name for pb in product_block_db.depends_on} if product_block_db else set()\n\n    product_blocks_in_model = cls._get_depends_on_product_block_types()\n    product_blocks_types_in_model = get_depends_on_product_block_type_list(product_blocks_in_model)\n\n    product_blocks_in_model = set(flatten(map(attrgetter(\"__names__\"), product_blocks_types_in_model)))  # type: ignore\n\n    missing_product_blocks_in_db = product_blocks_in_model - product_blocks_in_db  # type: ignore\n    missing_product_blocks_in_model = product_blocks_in_db - product_blocks_in_model  # type: ignore\n\n    resource_types_model = set(cls._non_product_block_fields_)\n    resource_types_db = {rt.resource_type for rt in product_block_db.resource_types} if product_block_db else set()\n\n    missing_resource_types_in_db = resource_types_model - resource_types_db\n    missing_resource_types_in_model = resource_types_db - resource_types_model\n\n    logger.debug(\n        \"ProductBlockTable blocks diff\",\n        product_block_db=product_block_db.name if product_block_db else None,\n        product_blocks_in_db=product_blocks_in_db,\n        product_blocks_in_model=product_blocks_in_model,\n        resource_types_db=resource_types_db,\n        resource_types_model=resource_types_model,\n        missing_product_blocks_in_db=missing_product_blocks_in_db,\n        missing_product_blocks_in_model=missing_product_blocks_in_model,\n        missing_resource_types_in_db=missing_resource_types_in_db,\n        missing_resource_types_in_model=missing_resource_types_in_model,\n    )\n\n    missing_data: dict[str, Any] = {}\n    for product_block_model in product_blocks_types_in_model:\n        if product_block_model.name == cls.name or product_block_model.name in missing_data:\n            continue\n        missing_data.update(product_block_model.diff_product_block_in_database())\n\n    diff: dict[str, set[str]] = {\n        k: v\n        for k, v in {\n            \"missing_product_blocks_in_db\": missing_product_blocks_in_db,\n            \"missing_product_blocks_in_model\": missing_product_blocks_in_model,\n            \"missing_resource_types_in_db\": missing_resource_types_in_db,\n            \"missing_resource_types_in_model\": missing_resource_types_in_model,\n        }.items()\n        if v\n    }\n\n    if diff:\n        missing_data[cls.name] = diff\n\n    return missing_data\n</code></pre>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.from_db","title":"from_db  <code>classmethod</code>","text":"<pre><code>from_db(\n    subscription_instance_id: uuid.UUID | None = None,\n    subscription_instance: (\n        orchestrator.db.SubscriptionInstanceTable | None\n    ) = None,\n    status: orchestrator.types.SubscriptionLifecycle | None = None,\n) -&gt; B\n</code></pre> <p>Create a product block based on a subscription instance from the database.</p> <p>This function is similar to <code>from_subscription()</code></p> <pre><code>&gt;&gt;&gt; subscription_instance_id = KNOWN_UUID_IN_DB  # doctest:+SKIP\n&gt;&gt;&gt; si_from_db = db.SubscriptionInstanceTable.query.get(subscription_instance_id)  # doctest:+SKIP\n&gt;&gt;&gt; example3 = ProductBlockModel.from_db(subscription_instance=si_from_db)  # doctest:+SKIP\n&gt;&gt;&gt; example4 = ProductBlockModel.from_db(subscription_instance_id=subscription_instance_id)  # doctest:+SKIP\n</code></pre> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef from_db(\n    cls: type[B],\n    subscription_instance_id: UUID | None = None,\n    subscription_instance: SubscriptionInstanceTable | None = None,\n    status: SubscriptionLifecycle | None = None,\n) -&gt; B:\n    \"\"\"Create a product block based on a subscription instance from the database.\n\n    This function is similar to `from_subscription()`\n\n        &gt;&gt;&gt; subscription_instance_id = KNOWN_UUID_IN_DB  # doctest:+SKIP\n        &gt;&gt;&gt; si_from_db = db.SubscriptionInstanceTable.query.get(subscription_instance_id)  # doctest:+SKIP\n        &gt;&gt;&gt; example3 = ProductBlockModel.from_db(subscription_instance=si_from_db)  # doctest:+SKIP\n        &gt;&gt;&gt; example4 = ProductBlockModel.from_db(subscription_instance_id=subscription_instance_id)  # doctest:+SKIP\n    \"\"\"\n    # Fill values from actual subscription\n    if subscription_instance_id:\n        subscription_instance = db.session.get(SubscriptionInstanceTable, subscription_instance_id)\n    if subscription_instance:\n        subscription_instance_id = subscription_instance.subscription_instance_id\n    assert subscription_instance_id  # noqa: S101\n    assert subscription_instance  # noqa: S101\n\n    if not status:\n        status = SubscriptionLifecycle(subscription_instance.subscription.status)\n\n    if not cls.__base_type__:\n        cls = ProductBlockModel.registry.get(subscription_instance.product_block.name, cls)  # type:ignore\n        cls = lookup_specialized_type(cls, status)\n\n    elif not issubclass(cls, lookup_specialized_type(cls, status)):\n        raise ValueError(f\"{cls} is not valid for lifecycle {status}\")\n\n    label = subscription_instance.label\n\n    instance_values = cls._load_instances_values(subscription_instance.values)\n    sub_instances = cls._load_instances(\n        subscription_instance.depends_on,\n        status,\n        match_domain_attr=True,\n        in_use_by_id_boundary=subscription_instance_id,\n    )\n\n    cls._fix_pb_data()\n    try:\n        model = cls(\n            name=cls.name,\n            subscription_instance_id=subscription_instance_id,\n            owner_subscription_id=subscription_instance.subscription_id,\n            label=label,\n            subscription=subscription_instance.subscription,\n            **instance_values,  # type: ignore\n            **sub_instances,\n        )\n        model.db_model = subscription_instance\n\n        return model\n    except ValidationError:\n        logger.exception(\n            \"Subscription is not correct in database\",\n            loaded_instance_values=instance_values,\n            loaded_sub_instances=sub_instances,\n        )\n        raise\n</code></pre>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(subscription_id: uuid.UUID, **kwargs: typing.Any) -&gt; B\n</code></pre> <p>Create a new empty product block.</p> <p>We need to use this instead of the normal constructor because that assumes you pass in all required values. That is cumbersome since that means creating a tree of product blocks.</p> <p>This is similar to <code>from_product_id()</code></p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef new(cls: type[B], subscription_id: UUID, **kwargs: Any) -&gt; B:\n    \"\"\"Create a new empty product block.\n\n    We need to use this instead of the normal constructor because that assumes you pass in\n    all required values. That is cumbersome since that means creating a tree of product blocks.\n\n    This is similar to `from_product_id()`\n    \"\"\"\n    sub_instances = cls._init_instances(subscription_id, set(kwargs.keys()))\n\n    subscription_instance_id = uuid4()\n\n    # Make sure product block stuff is already set if new is the first usage of this class\n    cls._fix_pb_data()\n\n    db_model = SubscriptionInstanceTable(\n        product_block_id=cls.product_block_id,\n        subscription_instance_id=subscription_instance_id,\n        subscription_id=subscription_id,\n    )\n    db.session.enable_relationship_loading(db_model)\n\n    if kwargs_name := kwargs.pop(\"name\", None):\n        # Not allowed to change the product block model name at runtime. This is only possible through\n        # the `product_block_name=..` metaclass parameter\n        logger.warning(\"Ignoring `name` keyword to ProductBlockModel.new()\", rejected_name=kwargs_name)\n    model = cls(\n        name=cls.name,\n        subscription_instance_id=subscription_instance_id,\n        owner_subscription_id=subscription_id,\n        label=db_model.label,\n        **sub_instances,\n        **kwargs,\n    )\n    model.db_model = db_model\n    return model\n</code></pre>"},{"location":"reference-docs/domain_models/product_blocks/#orchestrator.domain.base.ProductBlockModel.save","title":"save","text":"<pre><code>save(\n    *,\n    subscription_id: uuid.UUID,\n    status: orchestrator.types.SubscriptionLifecycle\n) -&gt; tuple[list[SubscriptionInstanceTable], SubscriptionInstanceTable]\n</code></pre> <p>Save the current model instance to the database.</p> <p>This means saving the whole tree of subscription instances and separately saving all instance values for this instance. This is called automatically when you return a subscription to the state in a workflow step.</p> <p>Parameters:</p> <ul> <li> <code>status</code>               (<code>orchestrator.types.SubscriptionLifecycle</code>)           \u2013            <p>current SubscriptionLifecycle to check if all constraints match</p> </li> <li> <code>subscription_id</code>               (<code>uuid.UUID</code>)           \u2013            <p>Optional subscription id needed if this is a new model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[orchestrator.db.SubscriptionInstanceTable], orchestrator.db.SubscriptionInstanceTable]</code>           \u2013            <p>List of saved instances</p> </li> </ul> Source code in <code>orchestrator/domain/base.py</code> <pre><code>def save(\n    self, *, subscription_id: UUID, status: SubscriptionLifecycle\n) -&gt; tuple[list[SubscriptionInstanceTable], SubscriptionInstanceTable]:\n    \"\"\"Save the current model instance to the database.\n\n    This means saving the whole tree of subscription instances and separately saving all instance\n    values for this instance. This is called automatically when you return a subscription to the state\n    in a workflow step.\n\n    Args:\n        status: current SubscriptionLifecycle to check if all constraints match\n        subscription_id: Optional subscription id needed if this is a new model\n\n    Returns:\n        List of saved instances\n\n    \"\"\"\n    if not self.name:\n        raise ValueError(f\"Cannot create instance of abstract class. Use one of {self.__names__}\")\n\n    # Make sure we have a valid subscription instance database model\n    subscription_instance: SubscriptionInstanceTable | None = db.session.get(\n        SubscriptionInstanceTable, self.subscription_instance_id\n    )\n    if subscription_instance:\n        # Make sure we do not use a mapped session.\n        db.session.refresh(subscription_instance)\n\n        # If this is a \"foreign\" instance we just stop saving and return it so only its relation is saved\n        # We should not touch these themselves\n        if self.owner_subscription_id != subscription_id:\n            return [], subscription_instance\n\n        self.db_model = subscription_instance\n    elif subscription_instance := self.db_model:\n        # We only need to add to the session if the subscription_instance does not exist.\n        db.session.add(subscription_instance)\n    else:\n        raise ValueError(\"Cannot save ProductBlockModel without a db_model\")\n\n    subscription_instance.subscription_id = subscription_id\n\n    db.session.flush()\n\n    # Everything is ok, make sure we are of the right class\n    specialized_type = lookup_specialized_type(self.__class__, status)\n    if specialized_type and not isinstance(self, specialized_type):\n        raise ValueError(\n            f\"Lifecycle status {status} requires specialized type {specialized_type!r}, was: {type(self)!r}\"\n        )\n\n    # Actually save stuff\n    subscription_instance.label = self.label\n    subscription_instance.values = self._save_instance_values(\n        subscription_instance.product_block, subscription_instance.values\n    )\n\n    sub_instances, depends_on_instances = self._save_instances(subscription_id, status)\n\n    # Save the subscription instances relations.\n    self._set_instance_domain_model_attrs(subscription_instance, depends_on_instances)\n\n    return sub_instances + [subscription_instance], subscription_instance\n</code></pre>"},{"location":"reference-docs/domain_models/product_blocks/#nesting-product-blocks","title":"Nesting Product Blocks","text":"<p>When building a more complicated product, you will likely want to start nesting layers of product blocks. Some of these might just be used by one product, but some of these will be a reference to a product block in use by multiple product types. For an example of this, let's look at the <code>CorePortBlock</code> product block from the example workflow orchestrator:</p> Example: <code>example-orchestrator/products/product_blocks/core_port.py</code> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\nfrom pydantic import computed_field\n\nfrom products.product_blocks.node import NodeBlock, NodeBlockInactive, NodeBlockProvisioning\n\n\nclass CorePortBlockInactive(ProductBlockModel, product_block_name=\"CorePort\"):\n    port_name: str | None = None\n    enabled: bool | None = True\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    node: NodeBlockInactive | None = None\n    ipv6_ipam_id: int | None = None\n\n\nclass CorePortBlockProvisioning(CorePortBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    port_name: str | None = None\n    enabled: bool\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    node: NodeBlockProvisioning\n    ipv6_ipam_id: int | None = None\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def title(self) -&gt; str:\n        return f\"core port {self.port_name} on {self.node.node_name}\"\n\n\nclass CorePortBlock(CorePortBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    port_name: str | None = None\n    enabled: bool\n    ims_id: int\n    nrm_id: int\n    node: NodeBlock\n    ipv6_ipam_id: int\n</code></pre> <p>Note in this example how the attribute <code>node</code> is type-hinted with <code>NodeBlockInactive</code>. This tells the WFO to bring in the entire tree of product blocks that are referenced, in this case, <code>NodeBlockInactive</code> when you load the <code>CorePortBlockInactive</code> into your workflow step. To read more on this concept, read through the Product Modelling page in the architecture section of the docs.</p>"},{"location":"reference-docs/domain_models/product_blocks/#creating-database-migrations","title":"Creating Database Migrations","text":"<p>Just like Product types, you'll need to create a database migration to properly wire-up the product block in the orchestrator's database. A migration file for this example NodeBlock model looks like this:</p> Example: <code>example-orchestrator/migrations/versions/schema/2023-10-27_a84ca2e5e4db_add_node.py</code> <pre><code>\"\"\"Add node product.\n\nRevision ID: a84ca2e5e4db\nRevises: a77227fe5455\nCreate Date: 2023-10-27 11:25:40.994878\n\n\"\"\"\n\nfrom uuid import uuid4\n\nfrom alembic import op\nfrom orchestrator.migrations.helpers import (\n    create,\n    create_workflow,\n    delete,\n    delete_workflow,\n    ensure_default_workflows,\n)\nfrom orchestrator.targets import Target\n\n# revision identifiers, used by Alembic.\nrevision = \"a84ca2e5e4db\"\ndown_revision = \"a77227fe5455\"\nbranch_labels = None\ndepends_on = None\n\nnew_products = {\n    \"products\": {\n        \"node Cisco\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Cisco\",\n            },\n        },\n        \"node Nokia\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Nokia\",\n            },\n        },\n        \"node Cumulus\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"Cumulus\",\n            },\n        },\n        \"node FRR\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"FRR\",\n            },\n        },\n    },\n    \"product_blocks\": {\n        \"Node\": {\n            \"product_block_id\": uuid4(),\n            \"description\": \"node product block\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"resources\": {\n                \"role_id\": \"ID in CMDB of role of the node in the network\",\n                \"type_id\": \"ID in CMDB of type of the node\",\n                \"site_id\": \"ID in CMDB of site where the node is located\",\n                \"node_status\": \"Operational status of the node\",\n                \"node_name\": \"Unique name of the node\",\n                \"node_description\": \"Description of the node\",\n                \"ims_id\": \"ID of the node in the inventory management system\",\n                \"nrm_id\": \"ID of the node in the network resource manager\",\n                \"ipv4_ipam_id\": \"ID of the node\u2019s iPv4 loopback address in IPAM\",\n                \"ipv6_ipam_id\": \"ID of the node\u2019s iPv6 loopback address in IPAM\",\n            },\n            \"depends_on_block_relations\": [],\n        },\n    },\n    \"workflows\": {},\n}\n\nnew_workflows = [\n    {\n        \"name\": \"create_node\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_node\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Modify node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_sync_ports\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Update node interfaces\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"terminate_node\",\n        \"target\": Target.TERMINATE,\n        \"description\": \"Terminate node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"validate_node\",\n        \"target\": Target.SYSTEM,\n        \"description\": \"Validate node\",\n        \"product_type\": \"Node\",\n    },\n]\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    create(conn, new_products)\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n    ensure_default_workflows(conn)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n\n    delete(conn, new_products)\n</code></pre> <p>Thankfully, you don't have to write these database migrations by hand, you can simply use the <code>main.py db migrate-domain-models</code> command that is part of the orchestrator CLI, documented here.</p>"},{"location":"reference-docs/domain_models/product_blocks/#automatically-generating-product-types","title":"Automatically Generating Product Types","text":"<p>If all of this seems like too much work, then good news, as all clever engineers before us have done, we've fixed that with YAML! Using the WFO CLI, you can generate your product types directly from a YAML. For more information on how to do that, check out the CLI <code>generate</code> command documentation.</p>"},{"location":"reference-docs/domain_models/product_types/","title":"Product Types","text":""},{"location":"reference-docs/domain_models/product_types/#defining-a-product-type","title":"Defining a Product Type","text":"<p>A Product Type (often referred to simply as a product) is the top level object of a domain model. A product is effectively the template used for creating a subscription instance, and you can instantiate as many instances of these as you want. To see an example product model, you can see a very simple Node product type from the example workflow orchestrator:</p> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.node import NodeBlock, NodeBlockInactive, NodeBlockProvisioning\nfrom pydantic_forms.types import strEnum\n\n\nclass Node_Type(strEnum):\n    Cisco = \"Cisco\"\n    Nokia = \"Nokia\"\n    Cumulus = \"Cumulus\"\n    FRR = \"FRR\"\n\n\nclass NodeInactive(SubscriptionModel, is_base=True):\n    node_type: Node_Type\n    node: NodeBlockInactive\n\n\nclass NodeProvisioning(NodeInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    node_type: Node_Type\n    node: NodeBlockProvisioning\n\n\nclass Node(NodeProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    node_type: Node_Type\n    node: NodeBlock\n</code></pre> <p>Type Hints</p> <p>Notice how type hints are used on these classes\u2014The WFO uses these types for pydantic validations and for type safety when serializing data into and out of the database. If you're not familiar with type hinting, learn about the benefits from PEP 484!</p> <p>Fixed Inputs</p> <p>When a hard coded value is stored on product model, like <code>Node_Type</code> is here, it is called a Fixed Input. Read more about Fixed Inputs here</p> <p>Breaking this product down a bit more, we see 3 classes, <code>NodeInactive</code>, <code>NodeProvisioning</code>, and finally <code>Node</code>. These three classes are built off of each-other, with the lowest level class (<code>NodeInactive</code>) based off of the <code>SubscriptionModel</code> base class. Each class has two simple attributes, one is the Fixed Input of <code>Node_Type</code>, and the other is the root product block <code>node</code>. Each one of these classes represents the <code>Node</code> product in its various lifecycle states, which are defined here in the <code>SubscriptionLifecycle</code> enum:</p> <p>To fully understand the Subscription Model, it's best to look at the <code>SubscriptionModel</code> itself in the code. Here you can also see the various methods available for use on these Subscription instances when you are using them in your workflow code:</p> <p>It is also quite helpful to see how the Product Type is stored in the database\u2014To see this, look at the <code>ProductTable</code> model as it shows all of the attributes stored in the database to store your WFO products:</p>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.types.SubscriptionLifecycle","title":"orchestrator.types.SubscriptionLifecycle","text":"<p>               Bases: <code>pydantic_forms.types.strEnum</code></p> Source code in <code>orchestrator/types.py</code> <pre><code>@strawberry.enum\nclass SubscriptionLifecycle(strEnum):\n    INITIAL = \"initial\"\n    ACTIVE = \"active\"\n    MIGRATING = \"migrating\"\n    DISABLED = \"disabled\"\n    TERMINATED = \"terminated\"\n    PROVISIONING = \"provisioning\"\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel","title":"orchestrator.domain.base.SubscriptionModel","text":"<p>               Bases: <code>orchestrator.domain.base.DomainModel</code></p> <p>This is the base class for all product subscription models.</p> <p>To use this class, see the examples below:</p> <p>Definining a subscription model:</p> <pre><code>&gt;&gt;&gt; class SubscriptionInactive(SubscriptionModel, product_type=\"SP\"):  # doctest:+SKIP\n...    block: Optional[ProductBlockModelInactive] = None\n\n&gt;&gt;&gt; class Subscription(BlockInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):  # doctest:+SKIP\n...    block: ProductBlockModel\n</code></pre> <p>This example defines a subscription model with two different contraints based on lifecycle. <code>Subscription</code> is valid only for <code>ACTIVE</code> And <code>SubscriptionInactive</code> for all other states. <code>product_type</code> must be defined on the base class and need not to be defined on the others</p> <p>Create a new empty subscription:</p> <pre><code>&gt;&gt;&gt; example1 = SubscriptionInactive.from_product_id(product_id, customer_id)  # doctest:+SKIP\n</code></pre> <p>Create a new instance based on a dict in the state:</p> <pre><code>&gt;&gt;&gt; example2 = SubscriptionInactive(**state)  # doctest:+SKIP\n</code></pre> <p>To retrieve a ProductBlockModel from the database:</p> <pre><code>&gt;&gt;&gt; SubscriptionInactive.from_subscription(subscription_id)  # doctest:+SKIP\n</code></pre> Source code in <code>orchestrator/domain/base.py</code> <pre><code>class SubscriptionModel(DomainModel):\n    r\"\"\"This is the base class for all product subscription models.\n\n    To use this class, see the examples below:\n\n    Definining a subscription model:\n\n        &gt;&gt;&gt; class SubscriptionInactive(SubscriptionModel, product_type=\"SP\"):  # doctest:+SKIP\n        ...    block: Optional[ProductBlockModelInactive] = None\n\n        &gt;&gt;&gt; class Subscription(BlockInactive, lifecycle=[SubscriptionLifecycle.ACTIVE]):  # doctest:+SKIP\n        ...    block: ProductBlockModel\n\n    This example defines a subscription model with two different contraints based on lifecycle. `Subscription` is valid only for `ACTIVE`\n    And `SubscriptionInactive` for all other states.\n    `product_type` must be defined on the base class and need not to be defined on the others\n\n    Create a new empty subscription:\n\n        &gt;&gt;&gt; example1 = SubscriptionInactive.from_product_id(product_id, customer_id)  # doctest:+SKIP\n\n    Create a new instance based on a dict in the state:\n\n        &gt;&gt;&gt; example2 = SubscriptionInactive(**state)  # doctest:+SKIP\n\n    To retrieve a ProductBlockModel from the database:\n\n        &gt;&gt;&gt; SubscriptionInactive.from_subscription(subscription_id)  # doctest:+SKIP\n    \"\"\"\n\n    __model_dump_cache__: ClassVar[dict[UUID, \"SubscriptionModel\"] | None] = None\n\n    product: ProductModel\n    customer_id: str\n    _db_model: SubscriptionTable | None = PrivateAttr(default=None)\n    subscription_id: UUID = Field(default_factory=uuid4)  # pragma: no mutate\n    description: str = \"Initial subscription\"  # pragma: no mutate\n    status: SubscriptionLifecycle = SubscriptionLifecycle.INITIAL  # pragma: no mutate\n    insync: bool = False  # pragma: no mutate\n    start_date: datetime | None = None  # pragma: no mutate\n    end_date: datetime | None = None  # pragma: no mutate\n    note: str | None = None  # pragma: no mutate\n    version: int = 1  # pragma: no mutate\n\n    def __new__(cls, *args: Any, status: SubscriptionLifecycle | None = None, **kwargs: Any) -&gt; \"SubscriptionModel\":\n        # status can be none if created during change_lifecycle\n        if status and not issubclass(cls, lookup_specialized_type(cls, status)):\n            raise ValueError(f\"{cls} is not valid for status {status}\")\n\n        return super().__new__(cls)\n\n    @classmethod\n    def __pydantic_init_subclass__(  # type: ignore[override]\n        cls, is_base: bool = False, lifecycle: list[SubscriptionLifecycle] | None = None, **kwargs: Any\n    ) -&gt; None:\n        super().__pydantic_init_subclass__(lifecycle=lifecycle, **kwargs)\n\n        if is_base:\n            cls.__base_type__ = cls\n\n        if is_base or lifecycle:\n            register_specialized_type(cls, lifecycle)\n\n        cls.__doc__ = make_subscription_model_docstring(cls, lifecycle)\n\n    @classmethod\n    def diff_product_in_database(cls, product_id: UUID) -&gt; dict[str, dict[str, set[str] | dict[str, set[str]]]]:\n        \"\"\"Return any differences between the attrs defined on the domain model and those on product blocks in the database.\n\n        This is only needed to check if the domain model and database models match which would be done during testing...\n        \"\"\"\n        product_db = db.session.get(ProductTable, product_id)\n        product_blocks_in_db = {pb.name for pb in product_db.product_blocks} if product_db else set()\n\n        product_blocks_in_model = cls._get_depends_on_product_block_types()\n        product_blocks_types_in_model = get_depends_on_product_block_type_list(product_blocks_in_model)\n\n        product_blocks_in_model = set(flatten(map(attrgetter(\"__names__\"), product_blocks_types_in_model)))  # type: ignore\n\n        missing_product_blocks_in_db = product_blocks_in_model - product_blocks_in_db  # type: ignore\n        missing_product_blocks_in_model = product_blocks_in_db - product_blocks_in_model  # type: ignore\n\n        fixed_inputs_model = set(cls._non_product_block_fields_)\n        fixed_inputs_in_db = {fi.name for fi in product_db.fixed_inputs} if product_db else set()\n\n        missing_fixed_inputs_in_db = fixed_inputs_model - fixed_inputs_in_db\n        missing_fixed_inputs_in_model = fixed_inputs_in_db - fixed_inputs_model\n\n        logger.debug(\n            \"ProductTable blocks diff\",\n            product_block_db=product_db.name if product_db else None,\n            product_blocks_in_db=product_blocks_in_db,\n            product_blocks_in_model=product_blocks_in_model,\n            fixed_inputs_in_db=fixed_inputs_in_db,\n            fixed_inputs_model=fixed_inputs_model,\n            missing_product_blocks_in_db=missing_product_blocks_in_db,\n            missing_product_blocks_in_model=missing_product_blocks_in_model,\n            missing_fixed_inputs_in_db=missing_fixed_inputs_in_db,\n            missing_fixed_inputs_in_model=missing_fixed_inputs_in_model,\n        )\n\n        missing_data_depends_on_blocks: dict[str, set[str]] = {}\n        for product_block_in_model in product_blocks_types_in_model:\n            missing_data_depends_on_blocks.update(product_block_in_model.diff_product_block_in_database())\n\n        diff: dict[str, set[str] | dict[str, set[str]]] = {\n            k: v\n            for k, v in {\n                \"missing_product_blocks_in_db\": missing_product_blocks_in_db,\n                \"missing_product_blocks_in_model\": missing_product_blocks_in_model,\n                \"missing_fixed_inputs_in_db\": missing_fixed_inputs_in_db,\n                \"missing_fixed_inputs_in_model\": missing_fixed_inputs_in_model,\n                \"missing_in_depends_on_blocks\": missing_data_depends_on_blocks,\n            }.items()\n            if v\n        }\n\n        missing_data: dict[str, dict[str, set[str] | dict[str, set[str]]]] = {}\n        if diff and product_db:\n            missing_data[product_db.name] = diff\n\n        return missing_data\n\n    @classmethod\n    def _load_root_instances(\n        cls,\n        subscription_id: UUID | UUIDstr,\n    ) -&gt; dict[str, Optional[dict] | list[dict]]:\n        \"\"\"Load root subscription instance(s) for this subscription model.\n\n        When a new subscription model is loaded from an existing subscription, this function loads the entire root\n        subscription instance(s) from database using an optimized postgres function. The result of that function\n        is used to instantiate the root product block(s).\n\n        The \"old\" method DomainModel._load_instances() would recursively load subscription instances from the\n        database and individually instantiate nested blocks, more or less \"manually\" reconstructing the subscription.\n\n        The \"new\" method SubscriptionModel._load_root_instances() takes a different approach; since it has all\n        data for the root subscription instance, it can rely on Pydantic to instantiate the root block and all\n        nested blocks in one go. This is also why it does not have the params `status` and `match_domain_attr` because\n        this information is already encoded in the domain model of a product.\n        \"\"\"\n        root_block_instance_ids = get_root_blocks_to_instance_ids(subscription_id)\n\n        root_block_types = {\n            field_name: list(flatten_product_block_types(product_block_type).keys())\n            for field_name, product_block_type in cls._product_block_fields_.items()\n        }\n\n        def get_instances_by_block_names(block_names: list[str]) -&gt; Iterable[dict]:\n            for block_name in block_names:\n                for instance_id in root_block_instance_ids.get(block_name, []):\n                    yield get_subscription_instance_dict(instance_id)\n\n        # Map root product block fields to subscription instance(s) dicts\n        instances = {\n            field_name: list(get_instances_by_block_names(block_names))\n            for field_name, block_names in root_block_types.items()\n        }\n\n        # Transform values according to domain models (list[dict] -&gt; dict, add None as default for optionals)\n        rules = {\n            klass.name: field_transformation_rules(klass) for klass in ProductBlockModel.registry.values() if klass.name\n        }\n        for instance_list in instances.values():\n            for instance in instance_list:\n                transform_instance_fields(rules, instance)\n\n        # Support the (theoretical?) usecase of a list of root product blocks\n        def unpack_instance_list(field_name: str, instance_list: list[dict]) -&gt; list[dict] | dict | None:\n            field_type = cls._product_block_fields_[field_name]\n            if is_list_type(field_type):\n                return instance_list\n            return only(instance_list)\n\n        return {\n            field_name: unpack_instance_list(field_name, instance_list)\n            for field_name, instance_list in instances.items()\n        }\n\n    @classmethod\n    def from_product_id(\n        cls: type[S],\n        product_id: UUID | UUIDstr,\n        customer_id: str,\n        status: SubscriptionLifecycle = SubscriptionLifecycle.INITIAL,\n        description: str | None = None,\n        insync: bool = False,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        note: str | None = None,\n        version: int = 1,\n    ) -&gt; S:\n        \"\"\"Use product_id (and customer_id) to return required fields of a new empty subscription.\"\"\"\n        # Caller wants a new instance and provided a product_id and customer_id\n        product_db = db.session.get(ProductTable, product_id)\n        if not product_db:\n            raise KeyError(\"Could not find a product for the given product_id\")\n\n        product = ProductModel(\n            product_id=product_db.product_id,\n            name=product_db.name,\n            description=product_db.description,\n            product_type=product_db.product_type,\n            tag=product_db.tag,\n            status=product_db.status,\n            created_at=product_db.created_at,\n            end_date=product_db.end_date,\n        )\n\n        if description is None:\n            description = f\"Initial subscription of {product.description}\"\n\n        subscription_id = uuid4()\n        subscription = SubscriptionTable(\n            subscription_id=subscription_id,\n            product_id=product_id,\n            customer_id=customer_id,\n            description=description,\n            status=status.value,\n            insync=insync,\n            start_date=start_date,\n            end_date=end_date,\n            note=note,\n            version=version,\n        )\n        db.session.add(subscription)\n\n        fixed_inputs = {fi.name: fi.value for fi in product_db.fixed_inputs}\n        instances = cls._init_instances(subscription_id)\n\n        model = cls(\n            product=product,\n            customer_id=customer_id,\n            subscription_id=subscription_id,\n            description=description,\n            status=status,\n            insync=insync,\n            start_date=start_date,\n            end_date=end_date,\n            note=note,\n            version=version,\n            **fixed_inputs,\n            **instances,\n        )\n        model.db_model = subscription\n        return model\n\n    @classmethod\n    def from_other_lifecycle(\n        cls: type[S],\n        other: \"SubscriptionModel\",\n        status: SubscriptionLifecycle,\n        skip_validation: bool = False,\n    ) -&gt; S:\n        \"\"\"Create new domain model from instance while changing the status.\n\n        This makes sure we always have a specific instance.\n        \"\"\"\n        if not cls.__base_type__:\n            # Import here to prevent cyclic imports\n            from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\n            cls = SUBSCRIPTION_MODEL_REGISTRY.get(other.product.name, cls)  # type:ignore\n            cls = lookup_specialized_type(cls, status)\n\n        # this will raise ValueError when wrong lifecycle transitions are detected in the new domain model\n        if not skip_validation:\n            validate_lifecycle_change(other, status)\n\n        data = cls._data_from_lifecycle(other, status, other.subscription_id)\n        data[\"status\"] = status\n        if data[\"start_date\"] is None and status == SubscriptionLifecycle.ACTIVE:\n            data[\"start_date\"] = nowtz()\n        if data[\"end_date\"] is None and status == SubscriptionLifecycle.TERMINATED:\n            data[\"end_date\"] = nowtz()\n\n        model = cls(**data)\n        model.db_model = other._db_model\n\n        return model\n\n    # Some common functions shared by from_other_product and from_subscription\n    @classmethod\n    def _get_subscription(cls: type[S], subscription_id: UUID | UUIDstr) -&gt; SubscriptionTable | None:\n        if not isinstance(subscription_id, UUID | UUIDstr):\n            raise TypeError(f\"subscription_id is of type {type(subscription_id)} instead of UUID | UUIDstr\")\n\n        loaders = [\n            joinedload(SubscriptionTable.product).selectinload(ProductTable.fixed_inputs),\n        ]\n\n        return db.session.get(SubscriptionTable, subscription_id, options=loaders)\n\n    @classmethod\n    def _to_product_model(cls: type[S], product: ProductTable) -&gt; ProductModel:\n        return ProductModel(\n            product_id=product.product_id,\n            name=product.name,\n            description=product.description,\n            product_type=product.product_type,\n            tag=product.tag,\n            status=product.status,\n            created_at=product.created_at if product.created_at else None,\n            end_date=product.end_date if product.end_date else None,\n        )\n\n    @classmethod\n    def from_other_product(\n        cls: type[S],\n        old_instantiation: S,\n        new_product_id: UUID | str,\n        new_root: tuple[str, ProductBlockModel] | None = None,\n    ) -&gt; S:\n        db_product = get_product_by_id(new_product_id)\n        if not db_product:\n            raise KeyError(\"Could not find a product for the given product_id\")\n\n        old_subscription_id = old_instantiation.subscription_id\n        if not (subscription := cls._get_subscription(old_subscription_id)):\n            raise ValueError(f\"Subscription with id: {old_subscription_id}, does not exist\")\n        product = cls._to_product_model(db_product)\n\n        status = SubscriptionLifecycle(subscription.status)\n\n        if not cls.__base_type__:\n            # Import here to prevent cyclic imports\n            from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\n            cls = SUBSCRIPTION_MODEL_REGISTRY.get(subscription.product.name, cls)  # type:ignore\n            cls = lookup_specialized_type(cls, status)\n        elif not issubclass(cls, lookup_specialized_type(cls, status)):\n            raise ValueError(f\"{cls} is not valid for lifecycle {status}\")\n\n        fixed_inputs = {fi.name: fi.value for fi in db_product.fixed_inputs}\n\n        if new_root:\n            name, product_block = new_root\n            instances = {name: product_block}\n        else:\n            # TODO test using cls._load_root_instances() here as well\n            instances = cls._load_instances(subscription.instances, status, match_domain_attr=False)  # type:ignore\n\n        try:\n            model = cls(\n                product=product,\n                customer_id=subscription.customer_id,\n                subscription_id=subscription.subscription_id,\n                description=subscription.description,\n                status=status,\n                insync=subscription.insync,\n                start_date=subscription.start_date,\n                end_date=subscription.end_date,\n                note=subscription.note,\n                version=subscription.version,\n                **fixed_inputs,\n                **instances,\n            )\n            model.db_model = subscription\n            return model\n        except ValidationError:\n            logger.exception(\n                \"Subscription is not correct in database\", loaded_fixed_inputs=fixed_inputs, loaded_instances=instances\n            )\n            raise\n\n    @classmethod\n    def from_subscription(cls: type[S], subscription_id: UUID | UUIDstr) -&gt; S:\n        \"\"\"Use a subscription_id to return required fields of an existing subscription.\"\"\"\n        from orchestrator.domain.context_cache import get_from_cache, store_in_cache\n\n        if cached_model := get_from_cache(subscription_id):\n            return cast(S, cached_model)\n\n        if not (subscription := cls._get_subscription(subscription_id)):\n            raise ValueError(f\"Subscription with id: {subscription_id}, does not exist\")\n        product = cls._to_product_model(subscription.product)\n\n        status = SubscriptionLifecycle(subscription.status)\n\n        if not cls.__base_type__:\n            # Import here to prevent cyclic imports\n            from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\n            try:\n                cls = SUBSCRIPTION_MODEL_REGISTRY[subscription.product.name]  # type:ignore\n            except KeyError:\n                raise ProductNotInRegistryError(\n                    f\"'{subscription.product.name}' is not found within the SUBSCRIPTION_MODEL_REGISTRY\"\n                )\n            cls = lookup_specialized_type(cls, status)\n        elif not issubclass(cls, lookup_specialized_type(cls, status)):\n            raise ValueError(f\"{cls} is not valid for lifecycle {status}\")\n\n        fixed_inputs = {fi.name: fi.value for fi in subscription.product.fixed_inputs}\n\n        instances = cls._load_root_instances(subscription_id)\n\n        try:\n            model = cls(\n                product=product,\n                customer_id=subscription.customer_id,\n                subscription_id=subscription.subscription_id,\n                description=subscription.description,\n                status=status,\n                insync=subscription.insync,\n                start_date=subscription.start_date,\n                end_date=subscription.end_date,\n                note=subscription.note,\n                version=subscription.version,\n                **fixed_inputs,\n                **instances,\n            )\n            model.db_model = subscription\n\n            store_in_cache(model)\n\n            return model\n        except ValidationError:\n            logger.exception(\n                \"Subscription is not correct in database\", loaded_fixed_inputs=fixed_inputs, loaded_instances=instances\n            )\n            raise\n\n    def save(self) -&gt; None:\n        \"\"\"Save the subscription to the database.\"\"\"\n        specialized_type = lookup_specialized_type(self.__class__, self.status)\n        if specialized_type and not isinstance(self, specialized_type):\n            raise ValueError(\n                f\"Lifecycle status {self.status.value} requires specialized type {specialized_type!r}, was: {type(self)!r}\"\n            )\n\n        existing_sub = db.session.get(\n            SubscriptionTable,\n            self.subscription_id,\n            options=[\n                selectinload(SubscriptionTable.instances)\n                .joinedload(SubscriptionInstanceTable.product_block)\n                .selectinload(ProductBlockTable.resource_types),\n                selectinload(SubscriptionTable.instances).selectinload(SubscriptionInstanceTable.values),\n            ],\n        )\n        if not (sub := (existing_sub or self.db_model)):\n            raise ValueError(\"Cannot save SubscriptionModel without a db_model\")\n\n        # Make sure we refresh the object and not use an already mapped object\n        db.session.refresh(sub)\n\n        self.db_model = sub\n        sub.product_id = self.product.product_id\n        sub.customer_id = self.customer_id\n        sub.description = self.description\n        sub.status = self.status.value\n        sub.insync = self.insync\n        sub.start_date = self.start_date\n        sub.end_date = self.end_date\n        sub.note = self.note\n\n        db.session.add(sub)\n        db.session.flush()  # Sends INSERT and returns subscription_id without committing transaction\n\n        old_instances_dict = {instance.subscription_instance_id: instance for instance in sub.instances}\n\n        saved_instances, depends_on_instances = self._save_instances(self.subscription_id, self.status)\n\n        for instances in depends_on_instances.values():\n            for instance in instances:\n                if instance.subscription_id != self.subscription_id:\n                    raise ValueError(\n                        \"Attempting to save a Foreign `Subscription Instance` directly below a subscription. \"\n                        \"This is not allowed.\"\n                    )\n        sub.instances = saved_instances\n\n        # Calculate what to remove\n        instances_set = {instance.subscription_instance_id for instance in sub.instances}\n        for instance_id in instances_set:\n            old_instances_dict.pop(instance_id, None)\n\n        # What's left should be removed\n        for instance in old_instances_dict.values():\n            db.session.delete(instance)\n\n        db.session.flush()\n\n    @property\n    def db_model(self) -&gt; SubscriptionTable | None:\n        if not self._db_model:\n            self._db_model = self._get_subscription(self.subscription_id)\n        return self._db_model\n\n    @db_model.setter\n    def db_model(self, value: SubscriptionTable) -&gt; None:\n        self._db_model = value\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel.diff_product_in_database","title":"diff_product_in_database  <code>classmethod</code>","text":"<pre><code>diff_product_in_database(\n    product_id: uuid.UUID,\n) -&gt; dict[str, dict[str, set[str] | dict[str, set[str]]]]\n</code></pre> <p>Return any differences between the attrs defined on the domain model and those on product blocks in the database.</p> <p>This is only needed to check if the domain model and database models match which would be done during testing...</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef diff_product_in_database(cls, product_id: UUID) -&gt; dict[str, dict[str, set[str] | dict[str, set[str]]]]:\n    \"\"\"Return any differences between the attrs defined on the domain model and those on product blocks in the database.\n\n    This is only needed to check if the domain model and database models match which would be done during testing...\n    \"\"\"\n    product_db = db.session.get(ProductTable, product_id)\n    product_blocks_in_db = {pb.name for pb in product_db.product_blocks} if product_db else set()\n\n    product_blocks_in_model = cls._get_depends_on_product_block_types()\n    product_blocks_types_in_model = get_depends_on_product_block_type_list(product_blocks_in_model)\n\n    product_blocks_in_model = set(flatten(map(attrgetter(\"__names__\"), product_blocks_types_in_model)))  # type: ignore\n\n    missing_product_blocks_in_db = product_blocks_in_model - product_blocks_in_db  # type: ignore\n    missing_product_blocks_in_model = product_blocks_in_db - product_blocks_in_model  # type: ignore\n\n    fixed_inputs_model = set(cls._non_product_block_fields_)\n    fixed_inputs_in_db = {fi.name for fi in product_db.fixed_inputs} if product_db else set()\n\n    missing_fixed_inputs_in_db = fixed_inputs_model - fixed_inputs_in_db\n    missing_fixed_inputs_in_model = fixed_inputs_in_db - fixed_inputs_model\n\n    logger.debug(\n        \"ProductTable blocks diff\",\n        product_block_db=product_db.name if product_db else None,\n        product_blocks_in_db=product_blocks_in_db,\n        product_blocks_in_model=product_blocks_in_model,\n        fixed_inputs_in_db=fixed_inputs_in_db,\n        fixed_inputs_model=fixed_inputs_model,\n        missing_product_blocks_in_db=missing_product_blocks_in_db,\n        missing_product_blocks_in_model=missing_product_blocks_in_model,\n        missing_fixed_inputs_in_db=missing_fixed_inputs_in_db,\n        missing_fixed_inputs_in_model=missing_fixed_inputs_in_model,\n    )\n\n    missing_data_depends_on_blocks: dict[str, set[str]] = {}\n    for product_block_in_model in product_blocks_types_in_model:\n        missing_data_depends_on_blocks.update(product_block_in_model.diff_product_block_in_database())\n\n    diff: dict[str, set[str] | dict[str, set[str]]] = {\n        k: v\n        for k, v in {\n            \"missing_product_blocks_in_db\": missing_product_blocks_in_db,\n            \"missing_product_blocks_in_model\": missing_product_blocks_in_model,\n            \"missing_fixed_inputs_in_db\": missing_fixed_inputs_in_db,\n            \"missing_fixed_inputs_in_model\": missing_fixed_inputs_in_model,\n            \"missing_in_depends_on_blocks\": missing_data_depends_on_blocks,\n        }.items()\n        if v\n    }\n\n    missing_data: dict[str, dict[str, set[str] | dict[str, set[str]]]] = {}\n    if diff and product_db:\n        missing_data[product_db.name] = diff\n\n    return missing_data\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel.from_other_lifecycle","title":"from_other_lifecycle  <code>classmethod</code>","text":"<pre><code>from_other_lifecycle(\n    other: orchestrator.domain.base.SubscriptionModel,\n    status: orchestrator.types.SubscriptionLifecycle,\n    skip_validation: bool = False,\n) -&gt; S\n</code></pre> <p>Create new domain model from instance while changing the status.</p> <p>This makes sure we always have a specific instance.</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef from_other_lifecycle(\n    cls: type[S],\n    other: \"SubscriptionModel\",\n    status: SubscriptionLifecycle,\n    skip_validation: bool = False,\n) -&gt; S:\n    \"\"\"Create new domain model from instance while changing the status.\n\n    This makes sure we always have a specific instance.\n    \"\"\"\n    if not cls.__base_type__:\n        # Import here to prevent cyclic imports\n        from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\n        cls = SUBSCRIPTION_MODEL_REGISTRY.get(other.product.name, cls)  # type:ignore\n        cls = lookup_specialized_type(cls, status)\n\n    # this will raise ValueError when wrong lifecycle transitions are detected in the new domain model\n    if not skip_validation:\n        validate_lifecycle_change(other, status)\n\n    data = cls._data_from_lifecycle(other, status, other.subscription_id)\n    data[\"status\"] = status\n    if data[\"start_date\"] is None and status == SubscriptionLifecycle.ACTIVE:\n        data[\"start_date\"] = nowtz()\n    if data[\"end_date\"] is None and status == SubscriptionLifecycle.TERMINATED:\n        data[\"end_date\"] = nowtz()\n\n    model = cls(**data)\n    model.db_model = other._db_model\n\n    return model\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel.from_product_id","title":"from_product_id  <code>classmethod</code>","text":"<pre><code>from_product_id(\n    product_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    customer_id: str,\n    status: orchestrator.types.SubscriptionLifecycle = SubscriptionLifecycle.INITIAL,\n    description: str | None = None,\n    insync: bool = False,\n    start_date: datetime.datetime | None = None,\n    end_date: datetime.datetime | None = None,\n    note: str | None = None,\n    version: int = 1,\n) -&gt; S\n</code></pre> <p>Use product_id (and customer_id) to return required fields of a new empty subscription.</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef from_product_id(\n    cls: type[S],\n    product_id: UUID | UUIDstr,\n    customer_id: str,\n    status: SubscriptionLifecycle = SubscriptionLifecycle.INITIAL,\n    description: str | None = None,\n    insync: bool = False,\n    start_date: datetime | None = None,\n    end_date: datetime | None = None,\n    note: str | None = None,\n    version: int = 1,\n) -&gt; S:\n    \"\"\"Use product_id (and customer_id) to return required fields of a new empty subscription.\"\"\"\n    # Caller wants a new instance and provided a product_id and customer_id\n    product_db = db.session.get(ProductTable, product_id)\n    if not product_db:\n        raise KeyError(\"Could not find a product for the given product_id\")\n\n    product = ProductModel(\n        product_id=product_db.product_id,\n        name=product_db.name,\n        description=product_db.description,\n        product_type=product_db.product_type,\n        tag=product_db.tag,\n        status=product_db.status,\n        created_at=product_db.created_at,\n        end_date=product_db.end_date,\n    )\n\n    if description is None:\n        description = f\"Initial subscription of {product.description}\"\n\n    subscription_id = uuid4()\n    subscription = SubscriptionTable(\n        subscription_id=subscription_id,\n        product_id=product_id,\n        customer_id=customer_id,\n        description=description,\n        status=status.value,\n        insync=insync,\n        start_date=start_date,\n        end_date=end_date,\n        note=note,\n        version=version,\n    )\n    db.session.add(subscription)\n\n    fixed_inputs = {fi.name: fi.value for fi in product_db.fixed_inputs}\n    instances = cls._init_instances(subscription_id)\n\n    model = cls(\n        product=product,\n        customer_id=customer_id,\n        subscription_id=subscription_id,\n        description=description,\n        status=status,\n        insync=insync,\n        start_date=start_date,\n        end_date=end_date,\n        note=note,\n        version=version,\n        **fixed_inputs,\n        **instances,\n    )\n    model.db_model = subscription\n    return model\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel.from_subscription","title":"from_subscription  <code>classmethod</code>","text":"<pre><code>from_subscription(\n    subscription_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n) -&gt; S\n</code></pre> <p>Use a subscription_id to return required fields of an existing subscription.</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>@classmethod\ndef from_subscription(cls: type[S], subscription_id: UUID | UUIDstr) -&gt; S:\n    \"\"\"Use a subscription_id to return required fields of an existing subscription.\"\"\"\n    from orchestrator.domain.context_cache import get_from_cache, store_in_cache\n\n    if cached_model := get_from_cache(subscription_id):\n        return cast(S, cached_model)\n\n    if not (subscription := cls._get_subscription(subscription_id)):\n        raise ValueError(f\"Subscription with id: {subscription_id}, does not exist\")\n    product = cls._to_product_model(subscription.product)\n\n    status = SubscriptionLifecycle(subscription.status)\n\n    if not cls.__base_type__:\n        # Import here to prevent cyclic imports\n        from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\n        try:\n            cls = SUBSCRIPTION_MODEL_REGISTRY[subscription.product.name]  # type:ignore\n        except KeyError:\n            raise ProductNotInRegistryError(\n                f\"'{subscription.product.name}' is not found within the SUBSCRIPTION_MODEL_REGISTRY\"\n            )\n        cls = lookup_specialized_type(cls, status)\n    elif not issubclass(cls, lookup_specialized_type(cls, status)):\n        raise ValueError(f\"{cls} is not valid for lifecycle {status}\")\n\n    fixed_inputs = {fi.name: fi.value for fi in subscription.product.fixed_inputs}\n\n    instances = cls._load_root_instances(subscription_id)\n\n    try:\n        model = cls(\n            product=product,\n            customer_id=subscription.customer_id,\n            subscription_id=subscription.subscription_id,\n            description=subscription.description,\n            status=status,\n            insync=subscription.insync,\n            start_date=subscription.start_date,\n            end_date=subscription.end_date,\n            note=subscription.note,\n            version=subscription.version,\n            **fixed_inputs,\n            **instances,\n        )\n        model.db_model = subscription\n\n        store_in_cache(model)\n\n        return model\n    except ValidationError:\n        logger.exception(\n            \"Subscription is not correct in database\", loaded_fixed_inputs=fixed_inputs, loaded_instances=instances\n        )\n        raise\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.domain.base.SubscriptionModel.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Save the subscription to the database.</p> Source code in <code>orchestrator/domain/base.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save the subscription to the database.\"\"\"\n    specialized_type = lookup_specialized_type(self.__class__, self.status)\n    if specialized_type and not isinstance(self, specialized_type):\n        raise ValueError(\n            f\"Lifecycle status {self.status.value} requires specialized type {specialized_type!r}, was: {type(self)!r}\"\n        )\n\n    existing_sub = db.session.get(\n        SubscriptionTable,\n        self.subscription_id,\n        options=[\n            selectinload(SubscriptionTable.instances)\n            .joinedload(SubscriptionInstanceTable.product_block)\n            .selectinload(ProductBlockTable.resource_types),\n            selectinload(SubscriptionTable.instances).selectinload(SubscriptionInstanceTable.values),\n        ],\n    )\n    if not (sub := (existing_sub or self.db_model)):\n        raise ValueError(\"Cannot save SubscriptionModel without a db_model\")\n\n    # Make sure we refresh the object and not use an already mapped object\n    db.session.refresh(sub)\n\n    self.db_model = sub\n    sub.product_id = self.product.product_id\n    sub.customer_id = self.customer_id\n    sub.description = self.description\n    sub.status = self.status.value\n    sub.insync = self.insync\n    sub.start_date = self.start_date\n    sub.end_date = self.end_date\n    sub.note = self.note\n\n    db.session.add(sub)\n    db.session.flush()  # Sends INSERT and returns subscription_id without committing transaction\n\n    old_instances_dict = {instance.subscription_instance_id: instance for instance in sub.instances}\n\n    saved_instances, depends_on_instances = self._save_instances(self.subscription_id, self.status)\n\n    for instances in depends_on_instances.values():\n        for instance in instances:\n            if instance.subscription_id != self.subscription_id:\n                raise ValueError(\n                    \"Attempting to save a Foreign `Subscription Instance` directly below a subscription. \"\n                    \"This is not allowed.\"\n                )\n    sub.instances = saved_instances\n\n    # Calculate what to remove\n    instances_set = {instance.subscription_instance_id for instance in sub.instances}\n    for instance_id in instances_set:\n        old_instances_dict.pop(instance_id, None)\n\n    # What's left should be removed\n    for instance in old_instances_dict.values():\n        db.session.delete(instance)\n\n    db.session.flush()\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#orchestrator.db.models.ProductTable","title":"orchestrator.db.models.ProductTable","text":"<p>               Bases: <code>orchestrator.db.database.BaseModel</code></p> Source code in <code>orchestrator/db/models.py</code> <pre><code>class ProductTable(BaseModel):\n    __tablename__ = \"products\"\n    __table_args__ = {\"extend_existing\": True}\n\n    __allow_unmapped__ = True\n\n    product_id = mapped_column(UUIDType, server_default=text(\"uuid_generate_v4()\"), primary_key=True)\n    name = mapped_column(String(), nullable=False, unique=True)\n    description = mapped_column(String(DESCRIPTION_LENGTH), nullable=False)\n    product_type = mapped_column(String(255), nullable=False)\n    tag = mapped_column(String(TAG_LENGTH), nullable=False, index=True)\n    status = mapped_column(String(STATUS_LENGTH), nullable=False)\n    created_at = mapped_column(UtcTimestamp, nullable=False, server_default=text(\"current_timestamp()\"))\n    end_date = mapped_column(UtcTimestamp)\n\n    product_blocks = relationship(\n        \"ProductBlockTable\",\n        secondary=product_product_block_association,\n        back_populates=\"products\",\n        passive_deletes=True,\n    )\n    workflows = relationship(\n        \"WorkflowTable\",\n        secondary=product_workflows_association,\n        secondaryjoin=\"and_(products_workflows.c.workflow_id == WorkflowTable.workflow_id, \"\n        \"WorkflowTable.deleted_at == None)\",\n        back_populates=\"products\",\n        passive_deletes=True,\n    )\n    fixed_inputs = relationship(\n        \"FixedInputTable\", cascade=\"all, delete-orphan\", back_populates=\"product\", passive_deletes=True\n    )\n\n    def find_block_by_name(self, name: str) -&gt; ProductBlockTable:\n        if session := object_session(self):\n            return session.query(ProductBlockTable).with_parent(self).filter(ProductBlockTable.name == name).one()\n        raise AssertionError(\"Session should not be None\")\n\n    def fixed_input_value(self, name: str) -&gt; str:\n        if session := object_session(self):\n            return (\n                session.query(FixedInputTable)\n                .with_parent(self)\n                .filter(FixedInputTable.name == name)\n                .with_entities(FixedInputTable.value)\n                .scalar()\n            )\n        raise AssertionError(\"Session should not be None\")\n\n    def _subscription_workflow_key(self, target: Target) -&gt; str | None:\n        wfs = list(filter(lambda w: w.target == target, self.workflows))\n        return wfs[0].name if len(wfs) &gt; 0 else None\n\n    def create_subscription_workflow_key(self) -&gt; str | None:\n        return self._subscription_workflow_key(Target.CREATE)\n\n    def terminate_subscription_workflow_key(self) -&gt; str | None:\n        return self._subscription_workflow_key(Target.TERMINATE)\n\n    def modify_subscription_workflow_key(self, name: str) -&gt; str | None:\n        wfs = list(filter(lambda w: w.target == Target.MODIFY and w.name == name, self.workflows))\n        return wfs[0].name if len(wfs) &gt; 0 else None\n\n    def workflow_by_key(self, name: str) -&gt; WorkflowTable | None:\n        return first_true(self.workflows, None, lambda wf: wf.name == name)  # type: ignore\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#subscription-model-registry","title":"Subscription Model Registry","text":"<p>When you define a Product Type as a domain model in python, you also need to register it in the subscription model registry, by using the <code>SUBSCRIPTION_MODEL_REGISTRY</code> dictionary, like is shown here in the example workflow orchestrator:</p> <pre><code># Copyright 2019-2023 SURF.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\nfrom products.product_types.core_link import CoreLink\nfrom products.product_types.l2vpn import L2vpn\nfrom products.product_types.node import Node\nfrom products.product_types.nsip2p import Nsip2p\nfrom products.product_types.nsistp import Nsistp\nfrom products.product_types.port import Port\n\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"node Cisco\": Node,\n        \"node Nokia\": Node,\n        \"node Cumulus\": Node,\n        \"node FRR\": Node,\n        \"port 10G\": Port,\n        \"port 100G\": Port,\n        \"core link 10G\": CoreLink,\n        \"core link 100G\": CoreLink,\n        \"l2vpn\": L2vpn,\n        \"nsistp\": Nsistp,\n        \"nsip2p\": Nsip2p,\n    }\n)\n</code></pre>"},{"location":"reference-docs/domain_models/product_types/#automatically-generating-product-types","title":"Automatically Generating Product Types","text":"<p>If all of this seems like too much work, then good news, as all clever engineers before us have done, we've fixed that with YAML! Using the WFO CLI, you can generate your product types directly from a YAML. For more information on how to do that, check out the CLI <code>generate</code> command documentation.</p>"},{"location":"reference-docs/domain_models/product_types/#creating-database-migrations","title":"Creating Database Migrations","text":"<p>After defining all of the components of a Product type, you'll also need to create a database migration to properly wire-up the product in the orchestrator's database. A migration file for this example Node model looks like this:</p> Example: <code>example-orchestrator/migrations/versions/schema/2023-10-27_a84ca2e5e4db_add_node.py</code> <pre><code>\"\"\"Add node product.\n\nRevision ID: a84ca2e5e4db\nRevises: a77227fe5455\nCreate Date: 2023-10-27 11:25:40.994878\n\n\"\"\"\n\nfrom uuid import uuid4\n\nfrom alembic import op\nfrom orchestrator.migrations.helpers import (\n    create,\n    create_workflow,\n    delete,\n    delete_workflow,\n    ensure_default_workflows,\n)\nfrom orchestrator.targets import Target\n\n# revision identifiers, used by Alembic.\nrevision = \"a84ca2e5e4db\"\ndown_revision = \"a77227fe5455\"\nbranch_labels = None\ndepends_on = None\n\nnew_products = {\n    \"products\": {\n        \"node Cisco\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Cisco\",\n            },\n        },\n        \"node Nokia\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"root_product_block\": \"Node\",\n            \"fixed_inputs\": {\n                \"node_type\": \"Nokia\",\n            },\n        },\n        \"node Cumulus\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"Cumulus\",\n            },\n        },\n        \"node FRR\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"Node\",\n            \"description\": \"Network node\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"Node\",\n            ],\n            \"fixed_inputs\": {\n                \"node_type\": \"FRR\",\n            },\n        },\n    },\n    \"product_blocks\": {\n        \"Node\": {\n            \"product_block_id\": uuid4(),\n            \"description\": \"node product block\",\n            \"tag\": \"NODE\",\n            \"status\": \"active\",\n            \"resources\": {\n                \"role_id\": \"ID in CMDB of role of the node in the network\",\n                \"type_id\": \"ID in CMDB of type of the node\",\n                \"site_id\": \"ID in CMDB of site where the node is located\",\n                \"node_status\": \"Operational status of the node\",\n                \"node_name\": \"Unique name of the node\",\n                \"node_description\": \"Description of the node\",\n                \"ims_id\": \"ID of the node in the inventory management system\",\n                \"nrm_id\": \"ID of the node in the network resource manager\",\n                \"ipv4_ipam_id\": \"ID of the node\u2019s iPv4 loopback address in IPAM\",\n                \"ipv6_ipam_id\": \"ID of the node\u2019s iPv6 loopback address in IPAM\",\n            },\n            \"depends_on_block_relations\": [],\n        },\n    },\n    \"workflows\": {},\n}\n\nnew_workflows = [\n    {\n        \"name\": \"create_node\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_node\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Modify node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"modify_sync_ports\",\n        \"target\": Target.MODIFY,\n        \"description\": \"Update node interfaces\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"terminate_node\",\n        \"target\": Target.TERMINATE,\n        \"description\": \"Terminate node\",\n        \"product_type\": \"Node\",\n    },\n    {\n        \"name\": \"validate_node\",\n        \"target\": Target.SYSTEM,\n        \"description\": \"Validate node\",\n        \"product_type\": \"Node\",\n    },\n]\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    create(conn, new_products)\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n    ensure_default_workflows(conn)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n\n    delete(conn, new_products)\n</code></pre> <p>Thankfully, you don't have to write these database migrations by hand, you can simply use the <code>main.py db migrate-domain-models</code> command that is part of the orchestrator CLI, documented here.</p>"},{"location":"reference-docs/monitoring/error-tracking/","title":"Error tracking","text":"<p>The <code>orchestrator-core</code> supports Sentry for error tracking and performance monitoring. Sentry is an application monitoring platform that helps developers identify, debug, and resolve issues in their applications by providing real-time error tracking, performance monitoring, and distributed tracing capabilities.</p> <p>In order to initialize Sentry (assuming you have already set up a Sentry project), perform the following steps:</p> <p>1. Update your own <code>Settings</code> class</p> <p>Add the following attributes to the <code>Settings</code> object in your own orchestrator's <code>settings.py</code>:</p> <pre><code>TRACING_ENABLED: bool = False\nSENTRY_DSN: str = \"\"\nTRACE_SAMPLE_RATE: float = 0.1\n</code></pre> <pre><code># settings.py\nfrom pydantic_settings import BaseSettings\n\nclass MySettings(BaseSettings):\n    TRACING_ENABLED: bool = False\n    SENTRY_DSN: str = \"\"\n    TRACE_SAMPLE_RATE: float = 0.1\n\nmy_settings = MySettings()\n</code></pre> <p>2. Set environment variables <pre><code>TRACING_ENABLED=True\nSENTRY_DSN = \"your_sentry_dsn\" # should be obtained from Sentry\nTRACE_SAMPLE_RATE = 0.1 # should be a float between 0 and 1\n</code></pre> Setting <code>TRACING_ENABLED</code> to <code>True</code> will enable tracing for the application, allowing you to monitor performance and errors more effectively.</p> <p>Note</p> <ul> <li>SENTRY_DSN: The Data Source Name (DSN) is a unique URL provided by Sentry. It connects your application to your Sentry project so errors and performance data are sent to the correct place.</li> <li>TRACE_SAMPLE_RATE: A float between 0 and 1 that controls what percentage of transactions are sent to Sentry for performance monitoring (e.g., 0.5 means 50% of traces are sampled). See Sentry documentation for more details on sampling.</li> </ul> <p>3. Update <code>main.py</code> file</p> <p>Add the following code to the <code>main.py</code> file of the <code>orchestrator-core</code> application: <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\nfrom my_orchestrator.settings import my_settings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif app.base_settings.TRACING_ENABLED and app.base_settings.ENVIRONMENT != \"local\":\n    from orchestrator.app import sentry_integrations\n    from sentry_sdk.integrations.httpx import HttpxIntegration\n\n    sentry_integrations.append(HttpxIntegration())\n\n    app.add_sentry(\n        my_settings.SENTRY_DSN,\n        my_settings.TRACE_SAMPLE_RATE,\n        app.base_settings.SERVICE_NAME,\n        app.base_settings.ENVIRONMENT,\n    )\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre></p> <p>Note</p> <ul> <li>It's recommended to use separate Sentry projects for different environments (production, staging, etc.) to avoid cluttering production error tracking with development errors. Many teams  skip Sentry integration in local development environments.</li> <li>Make sure to adjust the <code>TRACE_SAMPLE_RATE</code> according to your needs and test the integration in a development environment before deploying it to production.</li> </ul> <p>4. Test the Sentry integration</p> <p>You can test the Sentry integration by triggering an error in your application. For example, you can  create a route that raises an exception when the endpoint is triggered:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/debug-sentry\")\ndef debug_sentry():\n    division_by_zero = 1 / 0\n</code></pre> <p>Warning</p> <p>Make sure your environment variable <code>ENVIRONMENT</code> is NOT set to <code>local</code> when testing the Sentry integration.</p> <p>See also</p> <ul> <li>Running the app</li> <li>General info on app settings</li> </ul>"},{"location":"reference-docs/monitoring/metrics/","title":"Collecting Metrics","text":"<p>The <code>orchestrator-core</code> is capable of exporting metrics on an API endpoint that is compatible with Prometheus. Prometheus is a time-series database that can be used to collect metrics over time, to give insight in the usage and performance of your orchestrator, the running processes, and its subscriptions.</p> <p>By default, <code>orchestrator-core</code> exports metrics for: subscriptions, processes, and the workflow engine. These can be enabled by enabling the corresponding app setting <code>ENABLE_PROMETHEUS_METRICS_ENDPOINT</code>. An API response on <code>/api/metrics</code> with the default metrics enabled looks as follows:</p> <pre><code># HELP wfo_subscriptions_count Number of subscriptions per product, lifecycle state, customer, and in sync state.\n# TYPE wfo_subscriptions_count gauge\nwfo_subscriptions_count{customer_id=\"00000000-0000-0000-0000-000000000000\",insync=\"True\",lifecycle_state=\"active\",product_name=\"Router\"} 53.0\nwfo_subscriptions_count{customer_id=\"00000000-0000-0000-0000-000000000000\",insync=\"True\",lifecycle_state=\"active\",product_name=\"IP trunk\"} 52.0\nwfo_subscriptions_count{customer_id=\"00000000-0000-0000-0000-000000000000\",insync=\"True\",lifecycle_state=\"active\",product_name=\"Site\"} 36.0\nwfo_subscriptions_count{customer_id=\"00000000-0000-0000-0000-000000000000\",insync=\"True\",lifecycle_state=\"terminated\",product_name=\"Router\"} 22.0\n# HELP wfo_process_count Number of processes per status, creator, task, product, workflow, customer, and target.\n# TYPE wfo_process_count gauge\nwfo_process_count{created_by=\"SYSTEM\",customer_id=\"00000000-0000-0000-0000-000000000000\",is_task=\"True\",last_status=\"completed\",product_name=\"IP trunk\",workflow_name=\"validate_iptrunk\",workflow_target=\"SYSTEM\"} 5755.0\nwfo_process_count{created_by=\"SYSTEM\",customer_id=\"00000000-0000-0000-0000-000000000000\",is_task=\"True\",last_status=\"completed\",product_name=\"Router\",workflow_name=\"validate_router\",workflow_target=\"SYSTEM\"} 4066.0\nwfo_process_count{created_by=\"SYSTEM\",customer_id=\"12345678-1234-abcd-deff-123456789012\",is_task=\"True\",last_status=\"completed\",product_name=\"Edge Port\",workflow_name=\"validate_edge_port\",workflow_target=\"SYSTEM\"} 133.0\n# HELP wfo_process_seconds_total_count Total time spent on processes in seconds.\n# TYPE wfo_process_seconds_total_count gauge\nwfo_process_seconds_total_count{created_by=\"SYSTEM\",customer_id=\"00000000-0000-0000-0000-000000000000\",is_task=\"True\",last_status=\"completed\",product_name=\"IP trunk\",workflow_name=\"validate_iptrunk\",workflow_target=\"SYSTEM\"} 3.11e+06\nwfo_process_seconds_total_count{created_by=\"SYSTEM\",customer_id=\"00000000-0000-0000-0000-000000000000\",is_task=\"True\",last_status=\"completed\",product_name=\"Router\",workflow_name=\"validate_router\",workflow_target=\"SYSTEM\"} 6.72e+06\nwfo_process_seconds_total_count{created_by=\"SYSTEM\",customer_id=\"12345678-1234-abcd-deff-123456789012\",is_task=\"True\",last_status=\"completed\",product_name=\"Edge Port\",workflow_name=\"validate_edge_port\",workflow_target=\"SYSTEM\"} 6514.921\n# HELP wfo_engine_status Current workflow engine status.\n# TYPE wfo_engine_status gauge\nwfo_engine_status{wfo_engine_status=\"PAUSED\"} 0.0\nwfo_engine_status{wfo_engine_status=\"PAUSING\"} 0.0\nwfo_engine_status{wfo_engine_status=\"RUNNING\"} 1.0\n# HELP wfo_active_process_count Number of currently running processes in the workflow engine.\n# TYPE wfo_active_process_count gauge\nwfo_active_process_count 5.0\n</code></pre> <p>An example Grafana dashboard that uses these metrics is given in the code repository in <code>grafana-example.json</code>.</p>"},{"location":"reference-docs/monitoring/metrics/#adding-custom-metrics","title":"Adding custom metrics","text":"<p>It's possible to add more metric collectors to your orchestrator, if there are organisation-specific metrics you want to keep track of. This is done by implementing extra metrics from the <code>prometheus_client</code> library, documentation on how to achieve this is available here.</p> <p>When your new collector is implemented, register it in the orchestrator metrics registry when initialising your orchestrator with <code>ORCHESTRATOR_METRICS_REGISTRY.register(MyNewCollector())</code>.</p>"},{"location":"reference-docs/workflows/callbacks/","title":"Callbacks","text":"<p>In most workflow steps, when an operation is initiated, it completes and returns a result within a few seconds. However, if you have (for example) a longer-lived operation that is triggered through a HTTP request, then holding the HTTP connection open for the duration is fragile, and could result in missing the response without the ability to recover.</p> <p>To handle this scenario, Workflow Orchestrator supports callbacks.</p>"},{"location":"reference-docs/workflows/callbacks/#using-callbacks","title":"Using callbacks","text":"<p>When you wish to use a callback, Orchestrator supplies a callback URL that should be passed to the remote service. When the operation is triggered, the step immediately completes.</p> <p>The workflow then pauses until the remote service performs the callback. At that point, the callback URL is disabled (so further triggers will not result in a repeat), a validation step is performed, and the result is provided to the UI.</p> <p>If the validation step fails, then the step fails and execution of the workflow is stopped. If the validation step succeeds, then execution of the workflow is paused, and the operator is prompted for confirmation before it continues.</p>"},{"location":"reference-docs/workflows/callbacks/#writing-a-callback","title":"Writing a callback","text":"<p>The only immediate difference between a regular step and one that uses a callback is the addition of the <code>callback_route</code> parameter. This will be populated by orchestrator.</p> <p>Here is an example that makes a HTTP request to a service that executes an Ansible playbook:</p> <pre><code>from orchestrator import step\n\n\n@step(\"Execute an ansible playbook\")\ndef call_ansible_playbook(\n    subscription: L2vpnProvisioning,\n    callback_route: str,\n    *,\n    dry_run: bool,\n) -&gt; None:\n    inventory = f\"{port_A.node.node_name}\\n{port_B.node.node_name}\"\n    port_A = subscription.virtual_circuit.saps[0].port\n    port_B = subscription.virtual_circuit.saps[1].port\n\n    extra_vars = {\n        \"vlan\": subscription.virtual_circuit.saps[0].vlan,\n        \"SiteA\": f\"{port_A.node.node_name}\",\n        \"interfaceA\": port_A.port_name,\n        \"SiteB\": f\"{port_B.node.node_name}\",\n        \"interfaceB\": port_B.port_name,\n    }\n\n    callback_url = f\"http://orchestrator{callback_route}\"\n\n    parameters = {\n        \"playbook_name\": \"playbook.yml\",\n        \"inventory\": inventory,\n        \"extra_vars\": extra_vars,\n        \"callback\": callback_url,\n    }\n\n    url = f\"http://ansible-proxy/api/playbook/\"\n    request = requests.post(url, json=parameters, timeout=10)\n    request.raise_for_status()\n</code></pre> <p>However, this step is not included directly in the step list. To supplement it, we will need an additional function which also provides the validation step.</p> <pre><code>from orchestrator.workflow import Step, callback_step\n\n\ndef callback_interaction(provisioning_step: Step) -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; callback_step(\n            name=provisioning_step.name,\n            action_step=provisioning_step,\n            validate_step=_evaluate_callback_results,\n        )\n        &gt;&gt; _show_callback_results\n    )\n</code></pre> <p>We also have to provide the evaluation function, and the confirmation step.</p> <p>The remote service, when performing the callback, should provide a JSON payload containing the fields that will be evaluated and output in the UI. In the above example, the service responds with this payload:</p> <pre><code>{\n    \"job_id\": \"example\", // UUID\n    \"output\": \"example\", // From ansible_playbook_run.stdout.readlines()\n    \"return_code\": 0,    // From int(ansible_playbook_run.rc)\n    \"status\": \"example\"  // From ansible_playbook_run.status\n}\n</code></pre> <p>This allows us, in this example, to use the return code from Ansible to make the pass/fail decision on the step:</p> <pre><code>from orchestrator import step\nfrom orchestrator.utils.errors import ProcessFailureError\n\n\n@step(\"Evaluate callback result\")\ndef _evaluate_callback_results(callback_result: dict) -&gt; State:\n    if callback_result[\"return_code\"] != 0:\n        raise ProcessFailureError(message=\"Callback failure\", details=callback_result)\n\n    return {\"callback_result\": callback_result}\n</code></pre> <p>Then we also need the step that presents the results to the operator and requests confirmation before proceeding:</p> <pre><code>from orchestrator.config.assignee import Assignee\nfrom orchestrator.forms import FormPage\nfrom orchestrator.workflow import Step, inputstep\nfrom pydantic_forms.validators import LongText\n\n\n@inputstep(\"Confirm provisioning proxy results\", assignee=Assignee(\"SYSTEM\"))\ndef _show_callback_results(state: State) -&gt; FormGenerator:\n    if \"callback_result\" not in state:\n        return state\n\n    class ConfirmRunPage(FormPage):\n        class Config:\n            title: str = (\n                f\"Execution for {state['subscription']['product']['name']} completed.\"\n            )\n\n        run_status: str = state[\"callback_result\"][\"status\"]\n        run_results: LongText = json.dumps(state[\"callback_result\"], indent=4)\n\n    yield ConfirmRunPage\n    state.pop(\"run_results\")\n    return state\n</code></pre> <p>Finally, we wire this all up in our StepList. Instead of including the step directly, provide the step as a parameter to the interaction function:</p> <pre><code>from orchestrator.types import SubscriptionLifecycle\nfrom orchestrator.workflows.utils import create_workflow\n\n\n@create_workflow(\"Example workflow\", initial_input_form=initial_input_form_generator)\ndef create_l2vpn() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; construct_model\n        &gt;&gt; store_process_subscription()\n        &gt;&gt; callback_interaction(call_ansible_playbook)\n        &gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n    )\n</code></pre>"},{"location":"reference-docs/workflows/callbacks/#callback-progress-during-execution","title":"Callback progress during execution","text":"<p>For long-running jobs, such as executing Terraform or Ansible playbooks, the Orchestrator allows callback jobs to send real-time progress updates which can be used to provide operators with feedback on the progress of the running task.</p> <p>Progress updates should be delivered to <code>{callback_route}/progress</code>, where <code>\"/progress\"</code> is a fixed endpoint appended to the callback URL.</p> <p>The remote service should send a JSON or plain string payload with each progress callback, this replaces the previous progress update and refreshes the UI. Once the final callback is triggered and the job completes, progress updates are removed, leaving only the callback result. Therefore, all troubleshooting or diagnostic information must be included in the final callback payload, as progress updates are not retained for debugging.</p>"},{"location":"reference-docs/workflows/workflow-steps/","title":"Workflow Steps","text":"<p>Workflows are what actually takes a product definition and populates your domain models. To read more about the architectural design of workflows check out the architecture page on workflows. To see more details about the types of steps that are available for use, read on to the next section.</p>"},{"location":"reference-docs/workflows/workflow-steps/#step-types","title":"Step Types","text":""},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.workflow","title":"orchestrator.workflow","text":""},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.workflow.step","title":"step","text":"<pre><code>step(\n    name: str,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[StepFunc], Step]\n</code></pre> <p>Mark a function as a workflow step.</p> Source code in <code>orchestrator/workflow.py</code> <pre><code>def step(\n    name: str,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[StepFunc], Step]:\n    \"\"\"Mark a function as a workflow step.\"\"\"\n\n    def decorator(func: StepFunc) -&gt; Step:\n        @functools.wraps(func)\n        def wrapper(state: State) -&gt; Process:\n            with bound_contextvars(\n                func=func.__qualname__,\n                workflow_name=state.get(\"workflow_name\"),\n                process_id=state.get(\"process_id\"),\n            ):\n                step_in_inject_args = inject_args(func)\n                try:\n                    with transactional(db, logger):\n                        result = step_in_inject_args(state)\n                        return Success(result)\n                except Exception as ex:\n                    logger.warning(\"Step failed\", exc_info=ex)\n                    return Failed(ex)\n\n        return make_step_function(\n            wrapper,\n            name,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.workflow.retrystep","title":"retrystep","text":"<pre><code>retrystep(\n    name: str,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[StepFunc], Step]\n</code></pre> <p>Mark a function as a retryable workflow step.</p> <p>If this step fails it goes to <code>Waiting</code> were it will be retried periodically. If it <code>Success</code> it acts as a normal step.</p> Source code in <code>orchestrator/workflow.py</code> <pre><code>def retrystep(\n    name: str,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[StepFunc], Step]:\n    \"\"\"Mark a function as a retryable workflow step.\n\n    If this step fails it goes to `Waiting` were it will be retried periodically. If it `Success` it acts as a normal\n    step.\n    \"\"\"\n\n    def decorator(func: StepFunc) -&gt; Step:\n        @functools.wraps(func)\n        def wrapper(state: State) -&gt; Process:\n            with bound_contextvars(\n                func=func.__qualname__,\n                workflow_name=state.get(\"workflow_name\"),\n                process_id=state.get(\"process_id\"),\n            ):\n                step_in_inject_args = inject_args(func)\n                try:\n                    with transactional(db, logger):\n                        result = step_in_inject_args(state)\n                        return Success(result)\n                except Exception as ex:\n                    return Waiting(ex)\n\n        return make_step_function(\n            wrapper,\n            name,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.workflow.inputstep","title":"inputstep","text":"<pre><code>inputstep(\n    name: str,\n    assignee: orchestrator.config.assignee.Assignee,\n    resume_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[InputStepFunc], Step]\n</code></pre> <p>Add user input step to workflow.</p> <p>Any authorization callbacks will be attached to the resulting Step.</p> <p>IMPORTANT: In contrast to other workflow steps, the <code>@inputstep</code> wrapped function will not run in the workflow engine! This means that it must be free of side effects!</p> <p>Example::</p> <pre><code>@inputstep(\"User step\", assignee=Assignee.NOC)\ndef user_step(state: State) -&gt; FormGenerator:\n    class Form(FormPage):\n        name: str\n    user_input = yield Form\n    return {**user_input.model_dump(), \"some extra key\": True}\n</code></pre> Source code in <code>orchestrator/workflow.py</code> <pre><code>def inputstep(\n    name: str,\n    assignee: Assignee,\n    resume_auth_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[InputStepFunc], Step]:\n    \"\"\"Add user input step to workflow.\n\n    Any authorization callbacks will be attached to the resulting Step.\n\n    IMPORTANT: In contrast to other workflow steps, the `@inputstep` wrapped function will not run in the\n    workflow engine! This means that it must be free of side effects!\n\n    Example::\n\n        @inputstep(\"User step\", assignee=Assignee.NOC)\n        def user_step(state: State) -&gt; FormGenerator:\n            class Form(FormPage):\n                name: str\n            user_input = yield Form\n            return {**user_input.model_dump(), \"some extra key\": True}\n\n    \"\"\"\n\n    def decorator(func: InputStepFunc) -&gt; Step:\n        def wrapper(state: State) -&gt; FormGenerator:\n            form_generator_in_form_inject_args = form_inject_args(func)\n\n            form_generator = _handle_simple_input_form_generator(form_generator_in_form_inject_args)\n\n            return form_generator(state)\n\n        @functools.wraps(func)\n        def suspend(state: State) -&gt; Process:\n            return Suspend(state)\n\n        return make_step_function(\n            suspend,\n            name,\n            wrapper,\n            assignee,\n            resume_auth_callback=resume_auth_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.workflow.conditional","title":"conditional","text":"<pre><code>conditional(\n    p: collections.abc.Callable[[pydantic_forms.types.State], bool],\n) -&gt; Callable[..., StepList]\n</code></pre> <p>Use a predicate to control whether a step is run.</p> Source code in <code>orchestrator/workflow.py</code> <pre><code>def conditional(p: Callable[[State], bool]) -&gt; Callable[..., StepList]:\n    \"\"\"Use a predicate to control whether a step is run.\"\"\"\n\n    def _conditional(steps_or_func: StepList | Step) -&gt; StepList:\n        if isinstance(steps_or_func, Step):\n            steps = StepList([steps_or_func])\n        else:\n            steps = steps_or_func\n\n        def wrap(step: Step) -&gt; Step:\n            @functools.wraps(step)\n            def wrapper(state: State) -&gt; Process:\n                return step(state) if p(state) else Skipped(state)\n\n            return make_step_function(wrapper, step.name, step.form, step.assignee)\n\n        return steps.map(wrap)\n\n    return _conditional\n</code></pre>"},{"location":"reference-docs/workflows/workflow-steps/#database-implications","title":"Database Implications","text":"<p>Because a workflow is tied to a product type, this means that you will need a new database migration when adding new workflows. Thankfully, the CLI tool will help you with this! Check out the CLI docs for <code>db migrate-workflows</code> for more information.</p>"},{"location":"reference-docs/workflows/workflow-steps/#building-a-step-function-signature","title":"Building a Step Function Signature","text":"<p>One important detail that is very helpful to understand is how your Step Function's python function signature is used to deserialize objects from the database into something you can use in your workflow python code. The WFO achieves this by introspecting the step function signature for the type hints you've defined and then tries to populate the appropriate objects for you. To understand the details of how this works, look at this method:</p>"},{"location":"reference-docs/workflows/workflow-steps/#orchestrator.utils.state.inject_args","title":"orchestrator.utils.state.inject_args","text":"<pre><code>inject_args(func: orchestrator.types.StepFunc) -&gt; Callable[[State], State]\n</code></pre> <p>Allow functions to specify values from the state dict as parameters named after the state keys.</p> <p>Note</p> <p>Domain models are subject to special processing (see: :ref:<code>domain models processing &lt;domain-models-processing&gt;</code>)</p> <p>What this decorator does is better explained with an example than lots of text. So normally we do this::</p> <pre><code>def load_initial_state_for_modify(state: State) -&gt; State:\n    customer_id = state[\"customer_id\"]\n    subscription_id = state[\"subscription_id\"]\n    ....\n    # build new_state\n    ...\n    return {**state, **new_state}\n</code></pre> <p>With this decorator we can do::</p> <pre><code>@inject_args\ndef load_initial_state_for_modify(customer_id: UUID, subscription_id: UUID) -&gt; State:\n    ....\n    # build new_state\n    ...\n    return new_state\n</code></pre> <p>So any parameters specified to the step function are looked up in the <code>state</code> dict supplied by the <code>step</code> decorator and passed as values to the step function. The dict <code>new_state</code> returned by the step function will be merged with that of the original <code>state</code> dict and returned as the final result.</p> <p>It knows how to deal with parameters that have a default. Eg, given::</p> <pre><code>@inject_args\ndef do_stuff_with_saps(subscription_id: UUID, sap1: Dict, sap2: Optional[Dict] = None) -&gt; State:\n    ....\n    # build new_state\n    ...\n    return new_state\n</code></pre> <p>Both <code>subscription_id</code> and <code>sap1</code> need to be present in the state. However, <code>sap2</code> can be present but does not need to be. If it is not present in the state it will get the value <code>None</code></p> <p>.. _domain-models-processing:</p> <p>Domain models as parameters are subject to special processing. Eg, given::</p> <pre><code>@inject_args\ndef do_stuff(light_path: Sn8LightPath) -&gt; State:\n    ...\n    return {'light_path': light_path}  # &lt;- required for any changes to be saved to the DB\n</code></pre> <p>Then the key 'light_path' is looked up in the state. If it is present, it is expected to be either:</p> <ul> <li>a UUID (or str representation of a UUID)</li> <li>a dictionary with at least a key 'subscription_id', representing a domain model.</li> </ul> <p>It will use the UUID found to retrieve the domain model from the DB and inject it into the step function. None of the other data from the domain model (in case of it being a dict representation) will be used! At the end of the step function any domain models explicitly returned will be automatically saved to the DB; this includes any new domain models that might be created in the step and returned by the step. Hence, the automatic save is not limited to domain models requested as part of the step parameter list.</p> <p>If the key <code>light_path</code> was not found in the state, the parameter is interpreted as a request to create a domain model of the given type. For that to work correctly the keys <code>product</code> and <code>customer_id</code> need to be present in the state. This will not work for more than one domain model. E.g. you can't request two domain models to be created as we will not know to which of the two domain models <code>product</code> is applicable to.</p> <p>Also supported is wrapping a domain model in <code>Optional</code> or <code>List</code>. Other types are not supported.</p> <p>Args:     func: a step function with parameters (that should be keys into the state dict, except for optional ones)</p> <p>Returns:     The original state dict merged with the state that step function returned.</p> Source code in <code>orchestrator/utils/state.py</code> <pre><code>def inject_args(func: StepFunc) -&gt; Callable[[State], State]:\n    \"\"\"Allow functions to specify values from the state dict as parameters named after the state keys.\n\n    !!! note\n        Domain models are subject to special processing (see: :ref:`domain models processing\n        &lt;domain-models-processing&gt;`)\n\n    What this decorator does is better explained with an example than lots of text. So normally we do this::\n\n        def load_initial_state_for_modify(state: State) -&gt; State:\n            customer_id = state[\"customer_id\"]\n            subscription_id = state[\"subscription_id\"]\n            ....\n            # build new_state\n            ...\n            return {**state, **new_state}\n\n    With this decorator we can do::\n\n        @inject_args\n        def load_initial_state_for_modify(customer_id: UUID, subscription_id: UUID) -&gt; State:\n            ....\n            # build new_state\n            ...\n            return new_state\n\n    So any parameters specified to the step function are looked up in the `state` dict supplied by the `step` decorator\n    and passed as values to the step function. The dict `new_state` returned by the step function will be merged with\n    that of the original `state` dict and returned as the final result.\n\n    It knows how to deal with parameters that have a default. Eg, given::\n\n        @inject_args\n        def do_stuff_with_saps(subscription_id: UUID, sap1: Dict, sap2: Optional[Dict] = None) -&gt; State:\n            ....\n            # build new_state\n            ...\n            return new_state\n\n    Both `subscription_id` and `sap1` need to be present in the state. However, `sap2` can be present but does not need\n    to be. If it is not present in the state it will get the value `None`\n\n    .. _domain-models-processing:\n\n    Domain models as parameters are subject to special processing. Eg, given::\n\n        @inject_args\n        def do_stuff(light_path: Sn8LightPath) -&gt; State:\n            ...\n            return {'light_path': light_path}  # &lt;- required for any changes to be saved to the DB\n\n    Then the key 'light_path' is looked up in the state. If it is present, it is expected to be either:\n\n    - a UUID (or str representation of a UUID)\n    - a dictionary with at least a key 'subscription_id', representing a domain model.\n\n    It will use the UUID found to retrieve the domain model from the DB and inject it into the step function. None of\n    the other data from the domain model (in case of it being a dict representation) will be used! At the end of the\n    step function any domain models explicitly returned will be automatically saved to the DB; this includes any new\n    domain models that might be created in the step and returned by the step. Hence, the automatic save is not limited\n    to domain models requested as part of the step parameter list.\n\n    If the key `light_path` was not found in the state, the parameter is interpreted as a request to create a\n    domain model of the given type. For that to work correctly the keys `product` and `customer_id` need to be\n    present in the state. This will not work for more than one domain model. E.g. you can't request two domain\n    models to be created as we will not know to which of the two domain models `product` is applicable to.\n\n    Also supported is wrapping a domain model in ``Optional`` or ``List``. Other types are not supported.\n\n    Args:\n        func: a step function with parameters (that should be keys into the state dict, except for optional ones)\n\n    Returns:\n        The original state dict merged with the state that step function returned.\n\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(state: State) -&gt; State:\n        args = _build_arguments(func, state)\n        new_state = func(*args)\n\n        # Support step functions that don't return anything\n        if new_state is None:\n            new_state = {}\n\n        _save_models(new_state)\n\n        return {**state, **new_state}\n\n    return wrapper\n</code></pre>"},{"location":"reference-docs/workflows/workflow-steps/#reusable-workflow-steps","title":"Reusable Workflow Steps","text":"<p>When designing workflows it is good practice to make steps reusable. With a single product definition, you can build reusable steps for both <code>CREATE</code> and <code>MODIFY</code> workflows. For example, you may want to:</p> <ul> <li>Update a product's description using subscription properties.</li> <li>Push data to an external system via an upsert operation.</li> </ul> <p>To take reusability further, Python's <code>@singledispatch</code> decorator can help abstract product-specific logic behind a common interface. This makes your steps cleaner, easier to reuse across multiple workflows and more maintainable.</p>"},{"location":"reference-docs/workflows/workflow-steps/#generic-workflow-steps","title":"Generic Workflow Steps","text":"<p>You can define a generic step when the logic should be shared across multiple workflows or products. Here's an example of a reusable workflow step that updates an external system based on the product type:</p> <pre><code>@step(\"Update external system\")\ndef update_external_system(subscription: SubscriptionModel):\n    match type(subscription):\n        case ProductTypeOne:\n            payload = create_product_type_one_payload(subscription.some_block)\n        case ProductTypeTwo:\n            payload = create_product_type_two_payload(subscription.some_other_block)\n        case _:\n            raise TypeError(f\"Unsupported subscription type: {type(subscription)}\")\n\n    response = external_system_request(payload)\n    return {\"response\": response}\n</code></pre> <p>While this approach works, the switch logic (via <code>match</code> or <code>if isinstance()</code>) can become unwieldy as more product types are introduced. This is where <code>@singledispatch</code> can help.</p>"},{"location":"reference-docs/workflows/workflow-steps/#using-singledispatch-for-cleaner-reusability","title":"Using <code>@singledispatch</code> for Cleaner Reusability","text":"<p>In the example above, each product requires slightly different logic for building the payload. Rather than branching on type manually, you can delegate this responsibility to Python's <code>@singledispatch</code>.</p> <p>With <code>@singledispatch</code>, you define a generic function and register specific implementations based on the type of the input model.</p> <p>Benefits:</p> <ul> <li>Simplifies logic: No need for <code>match</code> or if <code>isinstance</code> checks.</li> <li>Improves maintainability: Logic is cleanly separated per product.</li> <li>Enhances extensibility: Easily add new product support with a new @register.</li> </ul> <p>Note: When using <code>@singledispatch</code> with Orchestrator models like <code>SubscriptionModel</code>, be sure to register specific lifecycle types (e.g., <code>ProductTypeOneProvisioning</code>).</p> <p>Example: Single Dispatch for External System Updates (default function)</p> <pre><code>from functools import singledispatch\nfrom surf.utils.singledispatch import single_dispatch_base\n\n@singledispatch\ndef update_external_system(model: SubscriptionModel) -&gt; str:\n    \"\"\"\n    Generic function to update an external system based on a subscription model.\n    Specific implementations must be registered for each product type.\n\n    Args:\n        model: The subscription lifecycle model.\n\n    Returns:\n        A response json from the external system.\n\n    Raises:\n        TypeError: If no registered implementation is found for the model.\n    \"\"\"\n    return single_dispatch_base(update_external_system, model)\n</code></pre> <p>Registering Implementations:</p> <pre><code>@update_external_system.register\ndef product_one_update_external_system(model: ProductTypeOneProvisioning | ProductTypeOne) -&gt; str:\n    payload = {}  # add payload logic...\n    return external_system_request(payload)\n\n\n@update_external_system.register\ndef product_two__active_update_external_system(model: ProductTypeTwo) -&gt; str:\n    payload = {}  # add payload logic...\n    return external_system_request(payload)\n\n\n@update_external_system.register\ndef product_two_provisioning_update_external_system(model: ProductTypeTwoProvisioning) -&gt; str:\n    payload = {}  # add payload logic...\n    return external_system_request(payload)\n</code></pre> <p>Now you can call <code>update_external_system(model)</code> without worrying about branching logic. The correct function will be called based on the model's type.</p>"},{"location":"reference-docs/workflows/workflows/","title":"Workflows","text":"<p>Workflows are what actually takes a product definition and populates your domain models. To read more about the architectural design of workflows check out the architecture page on workflows. To see more details about the workflow lifecycle states and functions, read on to the next section.</p>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflow.ProcessStatus","title":"orchestrator.workflow.ProcessStatus","text":"<p>               Bases: <code>pydantic_forms.types.strEnum</code></p> Source code in <code>orchestrator/workflow.py</code> <pre><code>@strawberry.enum\nclass ProcessStatus(strEnum):\n    CREATED = \"created\"\n    RUNNING = \"running\"\n    SUSPENDED = \"suspended\"\n    WAITING = \"waiting\"\n    AWAITING_CALLBACK = \"awaiting_callback\"\n    ABORTED = \"aborted\"\n    FAILED = \"failed\"\n    API_UNAVAILABLE = \"api_unavailable\"\n    INCONSISTENT_DATA = \"inconsistent_data\"\n    COMPLETED = \"completed\"\n    RESUMED = \"resumed\"\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils","title":"orchestrator.workflows.utils","text":""},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils.create_workflow","title":"create_workflow","text":"<pre><code>create_workflow(\n    description: str,\n    initial_input_form: pydantic_forms.types.InputStepFunc | None = None,\n    status: orchestrator.types.SubscriptionLifecycle = SubscriptionLifecycle.ACTIVE,\n    additional_steps: orchestrator.workflow.StepList | None = None,\n    authorize_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Transform an initial_input_form and a step list into a workflow with a target=Target.CREATE.</p> <p>Use this for create workflows only.</p> <p>Example::</p> <pre><code>@create_workflow(\"create service port\")\ndef create_service_port() -&gt; StepList:\n    do_something\n    &gt;&gt; do_something_else\n</code></pre> Source code in <code>orchestrator/workflows/utils.py</code> <pre><code>def create_workflow(\n    description: str,\n    initial_input_form: InputStepFunc | None = None,\n    status: SubscriptionLifecycle = SubscriptionLifecycle.ACTIVE,\n    additional_steps: StepList | None = None,\n    authorize_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Transform an initial_input_form and a step list into a workflow with a target=Target.CREATE.\n\n    Use this for create workflows only.\n\n    Example::\n\n        @create_workflow(\"create service port\")\n        def create_service_port() -&gt; StepList:\n            do_something\n            &gt;&gt; do_something_else\n    \"\"\"\n    create_initial_input_form_generator = wrap_create_initial_input_form(initial_input_form)\n\n    def _create_workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        steplist = (\n            init\n            &gt;&gt; f()\n            &gt;&gt; (additional_steps or StepList())\n            &gt;&gt; set_status(status)\n            &gt;&gt; resync\n            &gt;&gt; refresh_subscription_search_index\n            &gt;&gt; refresh_process_search_index\n            &gt;&gt; done\n        )\n\n        return make_workflow(\n            f,\n            description,\n            create_initial_input_form_generator,\n            Target.CREATE,\n            steplist,\n            authorize_callback=authorize_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return _create_workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils.modify_workflow","title":"modify_workflow","text":"<pre><code>modify_workflow(\n    description: str,\n    initial_input_form: pydantic_forms.types.InputStepFunc | None = None,\n    additional_steps: orchestrator.workflow.StepList | None = None,\n    authorize_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Transform an initial_input_form and a step list into a workflow.</p> <p>Use this for modify workflows.</p> <p>Example::</p> <pre><code>@modify_workflow(\"modify service port\") -&gt; StepList:\ndef modify_service_port():\n    do_something\n    &gt;&gt; do_something_else\n</code></pre> Source code in <code>orchestrator/workflows/utils.py</code> <pre><code>def modify_workflow(\n    description: str,\n    initial_input_form: InputStepFunc | None = None,\n    additional_steps: StepList | None = None,\n    authorize_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Transform an initial_input_form and a step list into a workflow.\n\n    Use this for modify workflows.\n\n    Example::\n\n        @modify_workflow(\"modify service port\") -&gt; StepList:\n        def modify_service_port():\n            do_something\n            &gt;&gt; do_something_else\n    \"\"\"\n\n    wrapped_modify_initial_input_form_generator = wrap_modify_initial_input_form(initial_input_form)\n\n    def _modify_workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        steplist = (\n            init\n            &gt;&gt; store_process_subscription()\n            &gt;&gt; unsync\n            &gt;&gt; f()\n            &gt;&gt; (additional_steps or StepList())\n            &gt;&gt; resync\n            &gt;&gt; refresh_subscription_search_index\n            &gt;&gt; refresh_process_search_index\n            &gt;&gt; done\n        )\n\n        return make_workflow(\n            f,\n            description,\n            wrapped_modify_initial_input_form_generator,\n            Target.MODIFY,\n            steplist,\n            authorize_callback=authorize_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return _modify_workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils.terminate_workflow","title":"terminate_workflow","text":"<pre><code>terminate_workflow(\n    description: str,\n    initial_input_form: pydantic_forms.types.InputStepFunc | None = None,\n    additional_steps: orchestrator.workflow.StepList | None = None,\n    authorize_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Transform an initial_input_form and a step list into a workflow.</p> <p>Use this for terminate workflows.</p> <p>Example::</p> <pre><code>@terminate_workflow(\"terminate service port\") -&gt; StepList:\ndef terminate_service_port():\n    do_something\n    &gt;&gt; do_something_else\n</code></pre> Source code in <code>orchestrator/workflows/utils.py</code> <pre><code>def terminate_workflow(\n    description: str,\n    initial_input_form: InputStepFunc | None = None,\n    additional_steps: StepList | None = None,\n    authorize_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Transform an initial_input_form and a step list into a workflow.\n\n    Use this for terminate workflows.\n\n    Example::\n\n        @terminate_workflow(\"terminate service port\") -&gt; StepList:\n        def terminate_service_port():\n            do_something\n            &gt;&gt; do_something_else\n    \"\"\"\n\n    wrapped_terminate_initial_input_form_generator = wrap_modify_initial_input_form(initial_input_form)\n\n    def _terminate_workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        steplist = (\n            init\n            &gt;&gt; store_process_subscription()\n            &gt;&gt; unsync\n            &gt;&gt; f()\n            &gt;&gt; (additional_steps or StepList())\n            &gt;&gt; set_status(SubscriptionLifecycle.TERMINATED)\n            &gt;&gt; resync\n            &gt;&gt; refresh_subscription_search_index\n            &gt;&gt; refresh_process_search_index\n            &gt;&gt; done\n        )\n\n        return make_workflow(\n            f,\n            description,\n            wrapped_terminate_initial_input_form_generator,\n            Target.TERMINATE,\n            steplist,\n            authorize_callback=authorize_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return _terminate_workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils.validate_workflow","title":"validate_workflow","text":"<pre><code>validate_workflow(\n    description: str,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Transform an initial_input_form and a step list into a workflow.</p> <p>Use this for subscription validate workflows.</p> <p>Example::</p> <pre><code>@validate_workflow(\"create service port\")\ndef create_service_port():\n    do_something\n    &gt;&gt; do_something_else\n</code></pre> Source code in <code>orchestrator/workflows/utils.py</code> <pre><code>def validate_workflow(description: str) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Transform an initial_input_form and a step list into a workflow.\n\n    Use this for subscription validate workflows.\n\n    Example::\n\n        @validate_workflow(\"create service port\")\n        def create_service_port():\n            do_something\n            &gt;&gt; do_something_else\n    \"\"\"\n\n    def _validate_workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        steplist = init &gt;&gt; store_process_subscription() &gt;&gt; unsync_unchecked &gt;&gt; f() &gt;&gt; resync &gt;&gt; done\n\n        return make_workflow(f, description, validate_initial_input_form_generator, Target.VALIDATE, steplist)\n\n    return _validate_workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflows.utils.reconcile_workflow","title":"reconcile_workflow","text":"<pre><code>reconcile_workflow(\n    description: str,\n    additional_steps: orchestrator.workflow.StepList | None = None,\n    authorize_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Similar to a modify_workflow but without required input user input to perform a sync with external systems based on the subscriptions existing configuration.</p> <p>Use this for subscription reconcile workflows.</p> <p>Example::</p> <pre><code>@reconcile_workflow(\"Reconcile l2vpn\")\ndef reconcile_l2vpn() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; update_l2vpn_in_external_systems\n    )\n</code></pre> Source code in <code>orchestrator/workflows/utils.py</code> <pre><code>def reconcile_workflow(\n    description: str,\n    additional_steps: StepList | None = None,\n    authorize_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Similar to a modify_workflow but without required input user input to perform a sync with external systems based on the subscriptions existing configuration.\n\n    Use this for subscription reconcile workflows.\n\n    Example::\n\n        @reconcile_workflow(\"Reconcile l2vpn\")\n        def reconcile_l2vpn() -&gt; StepList:\n            return (\n                begin\n                &gt;&gt; update_l2vpn_in_external_systems\n            )\n    \"\"\"\n\n    wrapped_reconcile_initial_input_form_generator = wrap_modify_initial_input_form(None)\n\n    def _reconcile_workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        steplist = (\n            init\n            &gt;&gt; store_process_subscription()\n            &gt;&gt; unsync\n            &gt;&gt; f()\n            &gt;&gt; (additional_steps or StepList())\n            &gt;&gt; resync\n            &gt;&gt; refresh_subscription_search_index\n            &gt;&gt; refresh_process_search_index\n            &gt;&gt; done\n        )\n\n        return make_workflow(\n            f,\n            description,\n            wrapped_reconcile_initial_input_form_generator,\n            Target.RECONCILE,\n            steplist,\n            authorize_callback=authorize_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return _reconcile_workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.workflow.workflow","title":"orchestrator.workflow.workflow","text":"<pre><code>workflow(\n    description: str,\n    initial_input_form: pydantic_forms.types.InputStepFunc | None = None,\n    target: orchestrator.targets.Target = Target.SYSTEM,\n    authorize_callback: orchestrator.utils.auth.Authorizer | None = None,\n    retry_auth_callback: orchestrator.utils.auth.Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]\n</code></pre> <p>Transform an initial_input_form and a step list into a workflow.</p> <p>Use this for other workflows. For create workflows use :func:<code>create_workflow</code></p> <p>Example::</p> <pre><code>@workflow(\"create service port\")\ndef create_service_port():\n    init\n    &lt;&lt; do_something\n    &lt;&lt; done\n</code></pre> Source code in <code>orchestrator/workflow.py</code> <pre><code>def workflow(\n    description: str,\n    initial_input_form: InputStepFunc | None = None,\n    target: Target = Target.SYSTEM,\n    authorize_callback: Authorizer | None = None,\n    retry_auth_callback: Authorizer | None = None,\n) -&gt; Callable[[Callable[[], StepList]], Workflow]:\n    \"\"\"Transform an initial_input_form and a step list into a workflow.\n\n    Use this for other workflows. For create workflows use :func:`create_workflow`\n\n    Example::\n\n        @workflow(\"create service port\")\n        def create_service_port():\n            init\n            &lt;&lt; do_something\n            &lt;&lt; done\n    \"\"\"\n    if initial_input_form is None:\n        initial_input_form_in_form_inject_args = None\n    else:\n        initial_input_form_in_form_inject_args = form_inject_args(initial_input_form)\n\n    def _workflow(f: Callable[[], StepList]) -&gt; Workflow:\n        return make_workflow(\n            f,\n            description,\n            initial_input_form_in_form_inject_args,\n            target,\n            f(),\n            authorize_callback=authorize_callback,\n            retry_auth_callback=retry_auth_callback,\n        )\n\n    return _workflow\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#workflow-helpers-to-register-them-in-db","title":"Workflow helpers to register them in DB","text":""},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers","title":"orchestrator.migrations.helpers","text":""},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.add_product_block_relation_between_products_by_id","title":"add_product_block_relation_between_products_by_id","text":"<pre><code>add_product_block_relation_between_products_by_id(\n    conn: sqlalchemy.engine.Connection,\n    in_use_by_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    depends_on_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n) -&gt; None\n</code></pre> <p>Add product block relation by product block id.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>in_use_by_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the product block that uses another product block.</p> </li> <li> <code>depends_on_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the product block that is used as dependency.</p> </li> </ul> <p>Usage: <pre><code>in_use_by_id = \"in_use_by_id\"\ndepends_on_id = \"depends_on_id\"\nadd_product_block_relation_between_products_by_id(conn, in_use_by_id, depends_on_id)\n</code></pre></p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def add_product_block_relation_between_products_by_id(\n    conn: sa.engine.Connection, in_use_by_id: UUID | UUIDstr, depends_on_id: UUID | UUIDstr\n) -&gt; None:\n    \"\"\"Add product block relation by product block id.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        in_use_by_id: ID of the product block that uses another product block.\n        depends_on_id: ID of the product block that is used as dependency.\n\n    Usage:\n    ```python\n    in_use_by_id = \"in_use_by_id\"\n    depends_on_id = \"depends_on_id\"\n    add_product_block_relation_between_products_by_id(conn, in_use_by_id, depends_on_id)\n    ```\n    \"\"\"\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            INSERT INTO product_block_relations (in_use_by_id, depends_on_id)\n            VALUES (:in_use_by_id, :depends_on_id)\n            \"\"\"\n        ),\n        {\n            \"in_use_by_id\": in_use_by_id,\n            \"depends_on_id\": depends_on_id,\n        },\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.add_products_to_workflow_by_product_tag","title":"add_products_to_workflow_by_product_tag","text":"<pre><code>add_products_to_workflow_by_product_tag(\n    conn: sqlalchemy.engine.Connection,\n    workflow_name: str,\n    product_tag: str,\n    product_name_like: str = \"%%\",\n) -&gt; None\n</code></pre> <p>Add products to a workflow by product tag.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>workflow_name</code>               (<code>str</code>)           \u2013            <p>Name of the workflow to add the products to.</p> </li> <li> <code>product_tag</code>               (<code>str</code>)           \u2013            <p>Tag of the product to add to the workflow.</p> </li> <li> <code>product_name_like</code>               (<code>optional</code>, default:                   <code>'%%'</code> )           \u2013            <p>Part of the product name to get more specific products (necessary for fw v2)</p> </li> </ul> <p>Usage: <pre><code>product_tag = \"product_tag\"\nworkflow_name = \"workflow_name\"\nadd_products_to_workflow_by_product_tag(conn, product_tag, workflow_name)\n</code></pre></p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def add_products_to_workflow_by_product_tag(\n    conn: sa.engine.Connection, workflow_name: str, product_tag: str, product_name_like: str = \"%%\"\n) -&gt; None:\n    \"\"\"Add products to a workflow by product tag.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        workflow_name: Name of the workflow to add the products to.\n        product_tag: Tag of the product to add to the workflow.\n        product_name_like (optional): Part of the product name to get more specific products (necessary for fw v2)\n\n    Usage:\n    ```python\n    product_tag = \"product_tag\"\n    workflow_name = \"workflow_name\"\n    add_products_to_workflow_by_product_tag(conn, product_tag, workflow_name)\n    ```\n    \"\"\"\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            WITH workflow AS (SELECT workflow_id\n                              FROM workflows\n                              WHERE name = :workflow_name)\n            INSERT\n            INTO products_workflows (product_id, workflow_id)\n            SELECT p.product_id,\n                   nw.workflow_id\n            FROM products AS p\n                     CROSS JOIN workflow AS nw\n            WHERE p.tag = :product_tag\n              AND name LIKE :product_name_like\n            \"\"\"\n        ),\n        {\n            \"workflow_name\": workflow_name,\n            \"product_tag\": product_tag,\n            \"product_name_like\": product_name_like,\n        },\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.convert_instance_relations_to_resource_type_relations_by_domain_model_attr","title":"convert_instance_relations_to_resource_type_relations_by_domain_model_attr","text":"<pre><code>convert_instance_relations_to_resource_type_relations_by_domain_model_attr(\n    conn: sqlalchemy.engine.Connection,\n    domain_model_attr: str,\n    resource_type_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    cleanup: bool = True,\n) -&gt; None\n</code></pre> <p>Move instance type relations to resouce type relations by domain model attribute.</p> <p>Note: It removes the instance relations after moving! Use the <code>cleanup</code> argument if you don't want this</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>domain_model_attr</code>               (<code>str</code>)           \u2013            <p>Name of the domain model attribute that connects the product blocks together.</p> </li> <li> <code>resource_type_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the resource type that you want to move the instance relations to.</p> </li> <li> <code>cleanup</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>remove old instance relations after the migrate?</p> </li> </ul> Usage <p>domain_model_attr = \"domain_model_attr\" resource_type_id = \"id\" convert_instance_relations_to_resource_type_relations_by_domain_model_attr(     conn, domain_model_attr, resource_type_id )</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def convert_instance_relations_to_resource_type_relations_by_domain_model_attr(\n    conn: sa.engine.Connection, domain_model_attr: str, resource_type_id: UUID | UUIDstr, cleanup: bool = True\n) -&gt; None:\n    \"\"\"Move instance type relations to resouce type relations by domain model attribute.\n\n    Note: It removes the instance relations after moving! Use the `cleanup` argument if you don't want this\n\n    Args:\n        conn: DB connection as available in migration main file.\n        domain_model_attr: Name of the domain model attribute that connects the product blocks together.\n        resource_type_id: ID of the resource type that you want to move the instance relations to.\n        cleanup: remove old instance relations after the migrate?\n\n    Usage:\n        &gt;&gt;&gt; domain_model_attr = \"domain_model_attr\"\n        &gt;&gt;&gt; resource_type_id = \"id\"\n        &gt;&gt;&gt; convert_instance_relations_to_resource_type_relations_by_domain_model_attr(\n            conn, domain_model_attr, resource_type_id\n        )\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"\n            INSERT INTO subscription_instance_values (subscription_instance_id, resource_type_id, value)\n            WITH instance_relations AS (SELECT in_use_by_id, depends_on_id\n                                        FROM subscription_instance_relations\n                                        WHERE domain_model_attr = :domain_model_attr)\n            SELECT ir.in_use_by_id    AS subscription_instance_id,\n                   :resource_type_id  AS resource_type_id,\n                   si.subscription_id AS value\n            from subscription_instances as si\n                     inner join instance_relations as ir on si.subscription_instance_id = ir.depends_on_id\n            \"\"\"\n        ),\n        {\n            \"domain_model_attr\": domain_model_attr,\n            \"resource_type_id\": resource_type_id,\n        },\n    )\n\n    if cleanup:\n        conn.execute(\n            sa.text(\"DELETE FROM subscription_instance_relations WHERE domain_model_attr=:attr\"),\n            {\"attr\": domain_model_attr},\n        )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.convert_resource_type_relations_to_instance_relations","title":"convert_resource_type_relations_to_instance_relations","text":"<pre><code>convert_resource_type_relations_to_instance_relations(\n    conn: sqlalchemy.engine.Connection,\n    resource_type_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    domain_model_attr: str,\n    cleanup: bool = True,\n) -&gt; None\n</code></pre> <p>Move resouce type relations to instance type relations using resource type id.</p> <p>Note: It removes the resource type relations after moving! (comment out the second execute to try without the removal)</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>resource_type_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the resource type that you want to move to instance relations.</p> </li> <li> <code>domain_model_attr</code>               (<code>str</code>)           \u2013            <p>Name of the domain model attribute that connects the product blocks together.</p> </li> <li> <code>cleanup</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>remove old resource type relations after the migrate?</p> </li> </ul> Usage <p>resource_type_id = \"id\" domain_model_attr = \"domain_model_attr\" convert_resource_type_relation_to_instance_relations(     conn, resource_type_id, domain_model_attr )</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def convert_resource_type_relations_to_instance_relations(\n    conn: sa.engine.Connection, resource_type_id: UUID | UUIDstr, domain_model_attr: str, cleanup: bool = True\n) -&gt; None:\n    \"\"\"Move resouce type relations to instance type relations using resource type id.\n\n    Note: It removes the resource type relations after moving! (comment out the second execute to try without the removal)\n\n    Args:\n        conn: DB connection as available in migration main file.\n        resource_type_id: ID of the resource type that you want to move to instance relations.\n        domain_model_attr: Name of the domain model attribute that connects the product blocks together.\n        cleanup: remove old resource type relations after the migrate?\n\n    Usage:\n        &gt;&gt;&gt; resource_type_id = \"id\"\n        &gt;&gt;&gt; domain_model_attr = \"domain_model_attr\"\n        &gt;&gt;&gt; convert_resource_type_relation_to_instance_relations(\n            conn, resource_type_id, domain_model_attr\n        )\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"\n            INSERT INTO subscription_instance_relations (in_use_by_id, depends_on_id, order_id, domain_model_attr)\n            WITH dependencies AS (SELECT siv.value                    AS subscription_id,\n                                         siv.subscription_instance_id AS in_use_by_instance_id,\n                                         si.product_block_id\n                                  FROM subscription_instance_values AS siv\n                                           LEFT JOIN subscription_instances AS si\n                                                     on siv.subscription_instance_id = si.subscription_instance_id\n                                  WHERE siv.resource_type_id = :resource_type_id)\n            SELECT in_use_by_instance_id                                        AS in_use_by_id,\n                   sii.subscription_instance_id                                 AS depends_on_id,\n                   (row_number() OVER (PARTITION BY in_use_by_instance_id) - 1) AS order_id,\n                   :domain_model_attr                                           AS domain_model_attr\n            FROM subscription_instances AS sii\n                     INNER JOIN dependencies AS dep ON sii.subscription_id = uuid(dep.subscription_id)\n            ON CONFLICT DO NOTHING\n            \"\"\"\n        ),\n        {\n            \"resource_type_id\": resource_type_id,\n            \"domain_model_attr\": domain_model_attr,\n        },\n    )\n\n    if cleanup:\n        conn.execute(\n            sa.text(\n                \"\"\"\n                DELETE\n                FROM subscription_instance_values\n                WHERE resource_type_id = :resource_type_id\n                \"\"\"\n            ),\n            {\"resource_type_id\": resource_type_id},\n        )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create","title":"create","text":"<pre><code>create(conn: sqlalchemy.engine.Connection, new: dict) -&gt; None\n</code></pre> <p>Call other functions in this file based on the schema.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>a dict with everything you want to make and link</p> </li> </ul> Example <p>new_stuff = {         \"products\": {             \"Example Product\": {                 \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",                 \"product_type\": \"ProductType1\",                 \"description\": \"Product description\",                 \"tag\": \"ProductType\",                 \"status\": \"active\",                 \"product_blocks\": [                     \"Example Product Block\"                 ],                 \"fixed_inputs\": {                     \"fixed_input_1\": (\"value\", \"f6a4f529-ad17-4ad8-b8ba-45684e2354ba\"),                     \"fixed_input_2\": (\"value\", \"5a67321d-45d5-4921-aa93-b8708b5d74c6\")                 }             },             \"Example Product 2\": {                 \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",                 \"product_type\": \"ProductType1\",                 \"description\": \"Product description\",                 \"tag\": \"ProductType\",                 \"status\": \"active\",                 \"product_block_ids\": [                     \"37afe017-5a04-4d87-96b0-b8f88a328d7a\"                 ]             }         },         \"product_blocks\": {             \"Example Product Block\": {                 \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",                 \"description\": \"Product description\",                 \"tag\": \"ProductType\",                 \"status\": \"active\",                 \"resources\": {                     \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),                     \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")                 }             },             \"Generated UUID Product Block\": {                 \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",                 \"description\": \"Product description\",                 \"tag\": \"ProductType\",                 \"status\": \"active\",                 \"resources\": {                     \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),                     \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")                 }             }         },         \"resources\": {             \"Existing Product\": {                 \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),                 \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")             }         },         \"workflows\": {             \"workflow_name\": {                 \"workflow_id\": \"f2702074-3203-454c-b298-6dfa7675423d\",                 \"target\": \"CREATE\",                 \"description\": \"Workflow description\",                 \"tag\": \"ProductType1\",                 \"search_phrase\": \"Search Phrase%\",             }         }     }</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create(conn: sa.engine.Connection, new: dict) -&gt; None:  # noqa: C901\n    \"\"\"Call other functions in this file based on the schema.\n\n    Args:\n        conn: DB connection as available in migration main file\n        new: a dict with everything you want to make and link\n\n    Example:\n        &gt;&gt;&gt; new_stuff = {\n                \"products\": {\n                    \"Example Product\": {\n                        \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",\n                        \"product_type\": \"ProductType1\",\n                        \"description\": \"Product description\",\n                        \"tag\": \"ProductType\",\n                        \"status\": \"active\",\n                        \"product_blocks\": [\n                            \"Example Product Block\"\n                        ],\n                        \"fixed_inputs\": {\n                            \"fixed_input_1\": (\"value\", \"f6a4f529-ad17-4ad8-b8ba-45684e2354ba\"),\n                            \"fixed_input_2\": (\"value\", \"5a67321d-45d5-4921-aa93-b8708b5d74c6\")\n                        }\n                    },\n                    \"Example Product 2\": {\n                        \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",\n                        \"product_type\": \"ProductType1\",\n                        \"description\": \"Product description\",\n                        \"tag\": \"ProductType\",\n                        \"status\": \"active\",\n                        \"product_block_ids\": [\n                            \"37afe017-5a04-4d87-96b0-b8f88a328d7a\"\n                        ]\n                    }\n                },\n                \"product_blocks\": {\n                    \"Example Product Block\": {\n                        \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",\n                        \"description\": \"Product description\",\n                        \"tag\": \"ProductType\",\n                        \"status\": \"active\",\n                        \"resources\": {\n                            \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),\n                            \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")\n                        }\n                    },\n                    \"Generated UUID Product Block\": {\n                        \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",\n                        \"description\": \"Product description\",\n                        \"tag\": \"ProductType\",\n                        \"status\": \"active\",\n                        \"resources\": {\n                            \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),\n                            \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")\n                        }\n                    }\n                },\n                \"resources\": {\n                    \"Existing Product\": {\n                        \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),\n                        \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")\n                    }\n                },\n                \"workflows\": {\n                    \"workflow_name\": {\n                        \"workflow_id\": \"f2702074-3203-454c-b298-6dfa7675423d\",\n                        \"target\": \"CREATE\",\n                        \"description\": \"Workflow description\",\n                        \"tag\": \"ProductType1\",\n                        \"search_phrase\": \"Search Phrase%\",\n                    }\n                }\n            }\n    \"\"\"\n    resources = new.get(\"resources\", {})\n    product_block_uuids = {}\n\n    # Create defined product blocks\n    if \"product_blocks\" in new:\n        for product_block_name, product_block in new.get(\"product_blocks\", {}).items():\n            # Move resources into one dict\n            if \"resources\" in product_block:\n                res_dict = {product_block_name: product_block[\"resources\"]}\n                resources.update(res_dict)\n                del product_block[\"resources\"]\n        product_block_uuids = create_product_blocks(conn, new[\"product_blocks\"])\n\n    def get_product_block_id(product_block_name: str) -&gt; str:\n        if product_block_id := product_block_uuids.get(product_block_name):\n            return product_block_id\n\n        try:\n            return str(get_product_block_id_by_name(conn, product_block_name))\n        except Exception:\n            raise ValueError(f\"{product_block_name} is not a valid product block.\")\n\n    # Create defined products\n    if \"products\" in new:\n        for _, product in new[\"products\"].items():\n            # The best practice is for a product to have only 1 root product block.\n            # Migrations created through the generator adhere to this practice.\n            product_block_ids = product.get(\"product_block_ids\", [])\n            if root_product_block := product.get(\"root_product_block\"):\n                root_product_block_id = get_product_block_id(root_product_block)\n                if product_block_ids and product_block_ids != [root_product_block_id]:\n                    logger.warning(\"Overriding hardcoded product_block_ids with root product block id\")\n                product[\"product_block_ids\"] = [root_product_block_id]\n                continue\n\n            # To avoid forcing users to re-write old migrations or their products, this function will still\n            # allow inserting multiple root product blocks.\n            if \"product_blocks\" in product:\n                product.setdefault(\"product_block_ids\", [])\n                for product_block_name in product[\"product_blocks\"]:\n                    product[\"product_block_ids\"].append(get_product_block_id(product_block_name))\n                del product[\"product_blocks\"]\n        create_products(conn, new[\"products\"])\n\n    # Create defined resource types\n    if resources:\n        create_resource_types_for_product_blocks(conn, resources)\n\n    # Create defined workflows\n    if \"workflows\" in new:\n        create_workflows(conn, new[\"workflows\"])\n\n    # Ensure default workflows exist for all products\n    ensure_default_workflows(conn)\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_fixed_inputs","title":"create_fixed_inputs","text":"<pre><code>create_fixed_inputs(\n    conn: sqlalchemy.engine.Connection,\n    product_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    new: dict,\n) -&gt; dict[str, str]\n</code></pre> <p>Create fixed inputs for a given product.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>product_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>UUID of the product to link to</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>an dict of your workflow data</p> </li> </ul> Example <p>product_id = \"id\" new = {         \"fixed_input_1\": (\"value\", \"f6a4f529-ad17-4ad8-b8ba-45684e2354ba\"),         \"fixed_input_2\": (\"value\", \"5a67321d-45d5-4921-aa93-b8708b5d74c6\")     } create_resource_types(conn, product_id, new)</p> <p>without extra ID's you don't need the tuple:</p> <pre><code>&gt;&gt;&gt; product_id = \"id\"\n&gt;&gt;&gt; new = {\n    \"fixed_input_1\": \"value\",\n    \"fixed_input_2\": \"value\",\n}\n&gt;&gt;&gt; create_fixed_inputs(conn, product_id, new)\n</code></pre> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_fixed_inputs(conn: sa.engine.Connection, product_id: UUID | UUIDstr, new: dict) -&gt; dict[str, str]:\n    \"\"\"Create fixed inputs for a given product.\n\n    Args:\n        conn: DB connection as available in migration main file\n        product_id: UUID of the product to link to\n        new: an dict of your workflow data\n\n    Example:\n        &gt;&gt;&gt; product_id = \"id\"\n        &gt;&gt;&gt; new = {\n                \"fixed_input_1\": (\"value\", \"f6a4f529-ad17-4ad8-b8ba-45684e2354ba\"),\n                \"fixed_input_2\": (\"value\", \"5a67321d-45d5-4921-aa93-b8708b5d74c6\")\n            }\n        &gt;&gt;&gt; create_resource_types(conn, product_id, new)\n\n    without extra ID's you don't need the tuple:\n\n        &gt;&gt;&gt; product_id = \"id\"\n        &gt;&gt;&gt; new = {\n            \"fixed_input_1\": \"value\",\n            \"fixed_input_2\": \"value\",\n        }\n        &gt;&gt;&gt; create_fixed_inputs(conn, product_id, new)\n\n    \"\"\"\n    insert_fixed_input_with_id = sa.text(\n        \"\"\"INSERT INTO fixed_inputs (fixed_input_id, name, value, created_at, product_id)\n           VALUES (:fixed_input_id, :key, :value, now(), :product_id)\n           ON CONFLICT DO NOTHING;\"\"\"\n    )\n    insert_fixed_input_without_id = sa.text(\n        \"\"\"INSERT INTO fixed_inputs (name, value, created_at, product_id)\n           VALUES (:key, :value, now(), :product_id)\n           ON CONFLICT DO NOTHING;\"\"\"\n    )\n\n    uuids = {}\n    for key, values in new.items():\n        if isinstance(values, tuple):\n            value, fixed_input_id = values\n            uuids[key] = fixed_input_id\n            conn.execute(\n                insert_fixed_input_with_id,\n                {\"fixed_input_id\": fixed_input_id, \"key\": key, \"value\": value, \"product_id\": product_id},\n            )\n        else:\n            conn.execute(insert_fixed_input_without_id, {\"key\": key, \"value\": values, \"product_id\": product_id})\n            uuids[key] = get_fixed_input_id_by_name(conn, key)\n\n    return uuids\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_product_blocks","title":"create_product_blocks","text":"<pre><code>create_product_blocks(\n    conn: sqlalchemy.engine.Connection, new: dict\n) -&gt; dict[str, UUIDstr]\n</code></pre> <p>Create new product blocks.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>an dict of your workflow data</p> </li> <li> <code>products</code>           \u2013            <p>list of product block ids to link these product blocks to</p> </li> </ul> Example <p>new = {         \"Example Product Block\": {             \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",             \"description\": \"Product description\",             \"tag\": \"ProductType\",             \"status\": \"active\",         },         \"Example Product Block Two\": {             \"product_block_id\": \"e8312243-f5cc-4560-adf0-63be4cefeccd\",             \"description\": \"Product description\",             \"tag\": \"ProductType\",             \"status\": \"active\",         }     }</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_product_blocks(conn: sa.engine.Connection, new: dict) -&gt; dict[str, UUIDstr]:\n    \"\"\"Create new product blocks.\n\n    Args:\n        conn: DB connection as available in migration main file\n        new: an dict of your workflow data\n        products: list of product block ids to link these product blocks to\n\n    Example:\n        &gt;&gt;&gt; new = {\n                \"Example Product Block\": {\n                    \"product_block_id\": \"37afe017-5a04-4d87-96b0-b8f88a328d7a\",\n                    \"description\": \"Product description\",\n                    \"tag\": \"ProductType\",\n                    \"status\": \"active\",\n                },\n                \"Example Product Block Two\": {\n                    \"product_block_id\": \"e8312243-f5cc-4560-adf0-63be4cefeccd\",\n                    \"description\": \"Product description\",\n                    \"tag\": \"ProductType\",\n                    \"status\": \"active\",\n                }\n            }\n    \"\"\"\n    uuids = {}\n    for name, product_block in new.items():\n        product_block[\"name\"] = name\n        uuids[name] = product_block[\"product_block_id\"]\n        conn.execute(\n            sa.text(\n                \"\"\"\n                INSERT INTO product_blocks (product_block_id, name, description, tag, status, created_at)\n                VALUES (:product_block_id, :name, :description, :tag, :status, now())\n                ON CONFLICT DO NOTHING;\n                \"\"\"\n            ),\n            product_block,\n        )\n        if \"resource_types\" in product_block:\n            block_resource_types = {name: product_block[\"resource_types\"]}\n            create_resource_types_for_product_blocks(conn, block_resource_types)\n\n        if \"in_use_by_block_relations\" in product_block:\n            for in_use_by_block_name in product_block[\"in_use_by_block_relations\"]:\n                in_use_by_block_id = get_product_block_id_by_name(conn, in_use_by_block_name)\n                add_product_block_relation_between_products_by_id(\n                    conn, str(in_use_by_block_id), str(product_block[\"product_block_id\"])\n                )\n\n        if \"depends_on_block_relations\" in product_block:\n            for depends_on_block_name in product_block[\"depends_on_block_relations\"]:\n                depends_on_block_id = get_product_block_id_by_name(conn, depends_on_block_name)\n                add_product_block_relation_between_products_by_id(\n                    conn, str(product_block[\"product_block_id\"]), str(depends_on_block_id)\n                )\n\n    return uuids\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_products","title":"create_products","text":"<pre><code>create_products(\n    conn: sqlalchemy.engine.Connection, new: dict\n) -&gt; dict[str, UUIDstr]\n</code></pre> <p>Create new products with their fixed inputs.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>an dict of your workflow data</p> </li> </ul> Example <p>new = {         \"Example Product\": {             \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",             \"product_type\": \"ProductType1\",             \"description\": \"Product description\",             \"tag\": \"ProductType\",             \"status\": \"active\",             \"fixed_inputs\": {                 \"fixed_input_1\": \"value\",                 \"fixed_input_2\": \"value2\"             }         },         \"Example Product 2\": {             \"product_type\": \"ProductType1\",             \"description\": \"Product description\",             \"tag\": \"ProductType\",             \"status\": \"active\",             \"product_block_ids\": [                 \"37afe017-5a04-4d87-96b0-b8f88a328d7a\"             ]         }     }</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_products(conn: sa.engine.Connection, new: dict) -&gt; dict[str, UUIDstr]:\n    \"\"\"Create new products with their fixed inputs.\n\n    Args:\n        conn: DB connection as available in migration main file\n        new: an dict of your workflow data\n\n    Example:\n        &gt;&gt;&gt; new = {\n                \"Example Product\": {\n                    \"product_id\": \"c9dc2374-514c-11eb-b685-acde48001122\",\n                    \"product_type\": \"ProductType1\",\n                    \"description\": \"Product description\",\n                    \"tag\": \"ProductType\",\n                    \"status\": \"active\",\n                    \"fixed_inputs\": {\n                        \"fixed_input_1\": \"value\",\n                        \"fixed_input_2\": \"value2\"\n                    }\n                },\n                \"Example Product 2\": {\n                    \"product_type\": \"ProductType1\",\n                    \"description\": \"Product description\",\n                    \"tag\": \"ProductType\",\n                    \"status\": \"active\",\n                    \"product_block_ids\": [\n                        \"37afe017-5a04-4d87-96b0-b8f88a328d7a\"\n                    ]\n                }\n            }\n    \"\"\"\n    uuids = {}\n    for name, product in new.items():\n        product[\"name\"] = name\n        current_uuid = product[\"product_id\"]\n        uuids[name] = current_uuid\n        conn.execute(\n            sa.text(\n                \"\"\"\n                INSERT INTO products (product_id, name, description, product_type, tag, status, created_at)\n                VALUES (:product_id, :name, :description, :product_type, :tag, :status, now())\n                ON CONFLICT DO NOTHING;\n                \"\"\"\n            ),\n            product,\n        )\n        for product_block_uuid in product.get(\"product_block_ids\", []):\n            conn.execute(\n                sa.text(\"INSERT INTO product_product_blocks VALUES (:product_id, :product_block_id)\"),\n                {\n                    \"product_id\": current_uuid,\n                    \"product_block_id\": product_block_uuid,\n                },\n            )\n        if \"fixed_inputs\" in product:\n            create_fixed_inputs(conn, current_uuid, product[\"fixed_inputs\"])\n    return uuids\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_resource_types_for_product_blocks","title":"create_resource_types_for_product_blocks","text":"<pre><code>create_resource_types_for_product_blocks(\n    conn: sqlalchemy.engine.Connection, new: dict\n) -&gt; list[UUID]\n</code></pre> <p>Create new resource types and link them to existing product_blocks by product_block name.</p> <p>Note: If the resource type already exists it will still add the resource type to the provided Product Blocks.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>a dict with your product blocks and resource types</p> </li> </ul> <p>Returns:     List of resource type ids in order of insertion</p> Usage examples <p>new_stuff = {     \"ProductBlockName1\": {         \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\")     },     \"ProductBlockName2\": {         \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),         \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")     } } create_resource_types(conn, new_stuff)</p> <p>without extra ID's you don't need the tuple:</p> <pre><code>&gt;&gt;&gt; new_stuff = {\n    \"ProductBlockName1\": {\n        \"resource_type1\": \"Resource description\"\n    },\n    \"ProductBlockName2\": {\n        \"resource_type1\": \"Resource description\",\n        \"resource_type1\": \"Resource description\"\n    }\n}\n\n&gt;&gt;&gt; create_resource_types_for_product_blocks(conn, new_stuff)\n</code></pre> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_resource_types_for_product_blocks(conn: sa.engine.Connection, new: dict) -&gt; list[UUID]:\n    \"\"\"Create new resource types and link them to existing product_blocks by product_block name.\n\n    Note: If the resource type already exists it will still add the resource type to the provided Product Blocks.\n\n    Args:\n        conn: DB connection as available in migration main file\n        new: a dict with your product blocks and resource types\n    Returns:\n        List of resource type ids in order of insertion\n\n    Usage examples:\n        &gt;&gt;&gt; new_stuff = {\n            \"ProductBlockName1\": {\n                \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\")\n            },\n            \"ProductBlockName2\": {\n                \"resource_type1\": (\"Resource description\", \"a47a3f96-c32f-4e4d-8e8c-11596451e878\"),\n                \"resource_type2\": (\"Resource description\", \"dffe1890-e0f8-4ed5-8d0b-e769c3f726cc\")\n            }\n        }\n        &gt;&gt;&gt; create_resource_types(conn, new_stuff)\n\n    without extra ID's you don't need the tuple:\n\n        &gt;&gt;&gt; new_stuff = {\n            \"ProductBlockName1\": {\n                \"resource_type1\": \"Resource description\"\n            },\n            \"ProductBlockName2\": {\n                \"resource_type1\": \"Resource description\",\n                \"resource_type1\": \"Resource description\"\n            }\n        }\n\n        &gt;&gt;&gt; create_resource_types_for_product_blocks(conn, new_stuff)\n\n    \"\"\"\n    insert_resource_type_with_id = sa.text(\n        \"\"\"INSERT INTO resource_types (resource_type_id, resource_type, description)\n           VALUES (:resource_type_id, :resource_type, :description)\n           ON CONFLICT DO NOTHING;\"\"\"\n    )\n    insert_resource_type_without_id = sa.text(\n        \"\"\"INSERT INTO resource_types (resource_type, description)\n           VALUES (:resource_type, :description)\n           ON CONFLICT DO NOTHING;\"\"\"\n    )\n\n    resource_type_ids = []\n    for resource_types in new.values():\n        for resource_type, values in resource_types.items():\n            if isinstance(values, tuple):\n                description, resource_type_id = values\n                conn.execute(\n                    insert_resource_type_with_id,\n                    {\n                        \"resource_type_id\": resource_type_id,\n                        \"resource_type\": resource_type,\n                        \"description\": description,\n                    },\n                )\n            else:\n                conn.execute(insert_resource_type_without_id, {\"resource_type\": resource_type, \"description\": values})\n            resource_type_ids.append(get_resource_type_id_by_name(conn, resource_type))\n\n    for product_block, resource_types in new.items():\n        conn.execute(\n            sa.text(\n                \"\"\"\n                WITH resource_type_ids AS (SELECT resource_types.resource_type_id\n                                           FROM resource_types\n                                           WHERE resource_types.resource_type = ANY (:new_resource_types)),\n                     service_attach_point AS (SELECT product_blocks.product_block_id\n                                              FROM product_blocks\n                                              WHERE product_blocks.name = :product_block_name)\n\n                INSERT\n                INTO product_block_resource_types (product_block_id, resource_type_id)\n                SELECT service_attach_point.product_block_id,\n                       resource_type_ids.resource_type_id\n                FROM service_attach_point\n                         CROSS JOIN\n                     resource_type_ids\n                \"\"\"\n            ),\n            {\n                \"product_block_name\": product_block,\n                \"new_resource_types\": list(resource_types.keys()),\n            },\n        )\n    return resource_type_ids\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_task","title":"create_task","text":"<pre><code>create_task(conn: sqlalchemy.engine.Connection, task: dict) -&gt; None\n</code></pre> <p>Create a new task.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>task</code>               (<code>dict</code>)           \u2013            <p>Dict with data for a new workflow. name: Name of the task. description: Description of the workflow.</p> </li> </ul> Usage <p>task = {     \"name\": \"task_name\",     \"description\": \"task description\", } create_task(conn, task)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_task(conn: sa.engine.Connection, task: dict) -&gt; None:\n    \"\"\"Create a new task.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        task: Dict with data for a new workflow.\n            name: Name of the task.\n            description: Description of the workflow.\n\n    Usage:\n        &gt;&gt;&gt; task = {\n            \"name\": \"task_name\",\n            \"description\": \"task description\",\n        }\n        &gt;&gt;&gt; create_task(conn, task)\n    \"\"\"\n    if has_table_column(table_name=\"workflows\", column_name=\"is_task\", conn=conn):\n        query = \"\"\"\n                INSERT INTO workflows(name, target, is_task, description)\n                VALUES (:name, 'SYSTEM', TRUE, :description)\n                ON CONFLICT DO NOTHING\n                RETURNING workflow_id\n                \"\"\"\n    else:\n        query = \"\"\"\n                INSERT INTO workflows(name, target, description)\n                VALUES (:name, 'SYSTEM', :description)\n                ON CONFLICT DO NOTHING\n                RETURNING workflow_id\n                \"\"\"\n\n    conn.execute(sa.text(query), task)\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_workflow","title":"create_workflow","text":"<pre><code>create_workflow(conn: sqlalchemy.engine.Connection, workflow: dict) -&gt; None\n</code></pre> <p>Create a new workflow for a product.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>workflow</code>               (<code>dict</code>)           \u2013            <p>Dict with data for a new workflow. name: Name of the workflow. target: Target of the workflow (\"CREATE\", \"MODIFY\", \"RECONCILE\", \"TERMINATE\", \"SYSTEM\") description: Description of the workflow. product_type: Product type to add the workflow to.</p> </li> </ul> Usage <p>workflow = {     \"name\": \"workflow_name\",     \"target\": \"SYSTEM\",     \"is_task\": False,     \"description\": \"workflow description\",     \"product_type\": \"product_type\",     \"product_tag\": \"product_tag\", } create_workflow(conn, workflow)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_workflow(conn: sa.engine.Connection, workflow: dict) -&gt; None:\n    \"\"\"Create a new workflow for a product.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        workflow: Dict with data for a new workflow.\n            name: Name of the workflow.\n            target: Target of the workflow (\"CREATE\", \"MODIFY\", \"RECONCILE\", \"TERMINATE\", \"SYSTEM\")\n            description: Description of the workflow.\n            product_type: Product type to add the workflow to.\n\n    Usage:\n        &gt;&gt;&gt; workflow = {\n            \"name\": \"workflow_name\",\n            \"target\": \"SYSTEM\",\n            \"is_task\": False,\n            \"description\": \"workflow description\",\n            \"product_type\": \"product_type\",\n            \"product_tag\": \"product_tag\",\n        }\n        &gt;&gt;&gt; create_workflow(conn, workflow)\n    \"\"\"\n    params = workflow.copy()\n    params.setdefault(\"is_task\", False)\n    params.setdefault(\"product_tag\", None)\n\n    query_parts = []\n\n    if has_table_column(table_name=\"workflows\", column_name=\"is_task\", conn=conn):\n        query_parts.append(\n            \"\"\"\n            WITH new_workflow AS (\n                INSERT INTO workflows (name, target, is_task, description)\n                    VALUES (:name, :target, :is_task, :description)\n                    ON CONFLICT DO NOTHING\n                    RETURNING workflow_id\n            )\n            \"\"\"\n        )\n    else:\n        params.pop(\"is_task\", None)\n        query_parts.append(\n            \"\"\"\n            WITH new_workflow AS (\n                INSERT INTO workflows (name, target, description)\n                    VALUES (:name, :target, :description)\n                    ON CONFLICT DO NOTHING\n                    RETURNING workflow_id\n            )\n            \"\"\"\n        )\n\n    query_parts.append(\n        \"\"\"\n        INSERT INTO products_workflows (product_id, workflow_id)\n        SELECT p.product_id, nw.workflow_id\n        FROM products AS p\n                 CROSS JOIN new_workflow AS nw\n        \"\"\"\n    )\n\n    query_parts.append(\"WHERE p.product_type = :product_type\")\n\n    if params.get(\"product_tag\") is not None:\n        query_parts.append(\"AND p.tag = :product_tag\")\n    else:\n        params.pop(\"product_tag\", None)\n\n    query_parts.append(\"ON CONFLICT DO NOTHING\")\n\n    query = \"\\n\".join(query_parts)\n\n    conn.execute(sa.text(query), params)\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.create_workflows","title":"create_workflows","text":"<pre><code>create_workflows(conn: sqlalchemy.engine.Connection, new: dict) -&gt; None\n</code></pre> <p>Create new workflows.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>new</code>               (<code>dict</code>)           \u2013            <p>an dict of your workflow data</p> </li> </ul> Example <p>new_workflows = {         \"workflow_name\": {             \"workflow_id\": \"f2702074-3203-454c-b298-6dfa7675423d\",             \"target\": \"CREATE\",             \"is_task\": False,             \"description\": \"Workflow description\",             \"tag\": \"ProductBlockName1\",             \"search_phrase\": \"Search Phrase%\",         }     }</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def create_workflows(conn: sa.engine.Connection, new: dict) -&gt; None:\n    \"\"\"Create new workflows.\n\n    Args:\n        conn: DB connection as available in migration main file\n        new: an dict of your workflow data\n\n    Example:\n        &gt;&gt;&gt; new_workflows = {\n                \"workflow_name\": {\n                    \"workflow_id\": \"f2702074-3203-454c-b298-6dfa7675423d\",\n                    \"target\": \"CREATE\",\n                    \"is_task\": False,\n                    \"description\": \"Workflow description\",\n                    \"tag\": \"ProductBlockName1\",\n                    \"search_phrase\": \"Search Phrase%\",\n                }\n            }\n    \"\"\"\n    for name, workflow in new.items():\n        workflow[\"name\"] = name\n\n        if not workflow.get(\"is_task\", False):\n            workflow[\"is_task\"] = False\n\n        if has_table_column(table_name=\"workflows\", column_name=\"is_task\", conn=conn):\n            query = \"\"\"\n                    WITH new_workflow AS (\n                        INSERT INTO workflows (workflow_id, name, target, is_task, description)\n                            VALUES (:workflow_id, :name, :target, :is_task, :description)\n                            RETURNING workflow_id)\n                    INSERT\n                    INTO products_workflows (product_id, workflow_id)\n                    SELECT p.product_id,\n                           nw.workflow_id\n                    FROM products AS p\n                             CROSS JOIN new_workflow AS nw\n                    WHERE p.tag = :tag\n                      AND p.name LIKE :search_phrase\n                    \"\"\"\n        else:\n            # Remove is_task from workflow dict and insert SQL\n            workflow = {k: v for k, v in workflow.items() if k != \"is_task\"}\n            query = \"\"\"\n                    WITH new_workflow AS (\n                        INSERT INTO workflows (workflow_id, name, target, description)\n                            VALUES (:workflow_id, :name, :target, :description)\n                            RETURNING workflow_id)\n                    INSERT\n                    INTO products_workflows (product_id, workflow_id)\n                    SELECT p.product_id,\n                           nw.workflow_id\n                    FROM products AS p\n                             CROSS JOIN new_workflow AS nw\n                    WHERE p.tag = :tag\n                      AND p.name LIKE :search_phrase\n                    \"\"\"\n\n        conn.execute(sa.text(query), workflow)\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete","title":"delete","text":"<pre><code>delete(conn: sqlalchemy.engine.Connection, obsolete: dict) -&gt; None\n</code></pre> <p>Delete multiple products, product_blocks, resources, and workflows.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>obsolete</code>               (<code>dict</code>)           \u2013            <p>a dict with everything you want to delete</p> </li> </ul> Example <p>obsolete = [         \"products\": [             \"Example Product\",             \"Example Product 2\"         ],         \"product_blocks\": [             \"Example Product Block\",             \"Example Product Block 2\"         ],         \"resources\": [                 \"resource_type4,                 \"resource_type5\"         ],         \"workflows\": [             \"workflow_name_a\",             \"workflow_name_b\",         ]     ]</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete(conn: sa.engine.Connection, obsolete: dict) -&gt; None:\n    \"\"\"Delete multiple products, product_blocks, resources, and workflows.\n\n    Args:\n        conn: DB connection as available in migration main file\n        obsolete: a dict with everything you want to delete\n\n    Example:\n        &gt;&gt;&gt; obsolete = [\n                \"products\": [\n                    \"Example Product\",\n                    \"Example Product 2\"\n                ],\n                \"product_blocks\": [\n                    \"Example Product Block\",\n                    \"Example Product Block 2\"\n                ],\n                \"resources\": [\n                        \"resource_type4,\n                        \"resource_type5\"\n                ],\n                \"workflows\": [\n                    \"workflow_name_a\",\n                    \"workflow_name_b\",\n                ]\n            ]\n    \"\"\"\n    if \"resource_types\" in obsolete:\n        for res_type in obsolete[\"resource_types\"]:\n            delete_resource_type(conn, res_type)\n\n    if \"product_blocks\" in obsolete:\n        for product_block_name in obsolete[\"product_blocks\"]:\n            delete_product_block(conn, product_block_name)\n\n    if \"products\" in obsolete:\n        for product in obsolete[\"products\"]:\n            delete_product(conn, product)\n\n    if \"workflows\" in obsolete:\n        for workflow in obsolete[\"workflows\"]:\n            delete_workflow(conn, workflow)\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_product","title":"delete_product","text":"<pre><code>delete_product(conn: sqlalchemy.engine.Connection, name: str) -&gt; None\n</code></pre> <p>Delete a product and it's occurrences in workflows and product_blocks.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>a product name you want to delete</p> </li> </ul> Example <p>obsolete_stuff = \"name_1\" delete_product(conn, obsolete_stuff)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_product(conn: sa.engine.Connection, name: str) -&gt; None:\n    \"\"\"Delete a product and it's occurrences in workflows and product_blocks.\n\n    Args:\n        conn: DB connection as available in migration main file\n        name: a product name you want to delete\n\n    Example:\n        &gt;&gt;&gt; obsolete_stuff = \"name_1\"\n        &gt;&gt;&gt; delete_product(conn, obsolete_stuff)\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"\n            WITH deleted_p AS (\n                DELETE FROM products WHERE name = :name\n                    RETURNING product_id),\n                 deleted_p_pb AS (\n                     DELETE FROM product_product_blocks WHERE product_id IN (SELECT product_id FROM deleted_p)),\n                 deleted_pb_rt AS (\n                     DELETE FROM products_workflows WHERE product_id IN (SELECT product_id FROM deleted_p))\n            SELECT *\n            from deleted_p;\n            \"\"\"\n        ),\n        {\"name\": name},\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_product_block","title":"delete_product_block","text":"<pre><code>delete_product_block(conn: sqlalchemy.engine.Connection, name: str) -&gt; None\n</code></pre> <p>Delete a product block and it's occurrences in resource types and products.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>a product_block name you want to delete</p> </li> </ul> Example <p>obsolete_stuff = \"name_1\" delete_product_block(conn, obsolete_stuff)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_product_block(conn: sa.engine.Connection, name: str) -&gt; None:\n    \"\"\"Delete a product block and it's occurrences in resource types and products.\n\n    Args:\n        conn: DB connection as available in migration main file\n        name: a product_block name you want to delete\n\n    Example:\n        &gt;&gt;&gt; obsolete_stuff = \"name_1\"\n        &gt;&gt;&gt; delete_product_block(conn, obsolete_stuff)\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"\n            WITH deleted_pb AS (\n                DELETE FROM product_blocks WHERE name = :name\n                    RETURNING product_block_id),\n                 deleted_p_pb AS (\n                     DELETE FROM product_product_blocks WHERE product_block_id IN (SELECT product_block_id FROM deleted_pb)),\n                 deleted_pb_rt AS (\n                     DELETE FROM product_block_resource_types WHERE product_block_id IN (SELECT product_block_id FROM deleted_pb))\n            SELECT *\n            from deleted_pb;\n            \"\"\"\n        ),\n        {\"name\": name},\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_resource_type","title":"delete_resource_type","text":"<pre><code>delete_resource_type(\n    conn: sqlalchemy.engine.Connection, resource_type: str\n) -&gt; None\n</code></pre> <p>Delete a resource type and it's occurrences in product blocks and products.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>resource_type</code>               (<code>str</code>)           \u2013            <p>a resource_type name you want to delete</p> </li> </ul> Example <p>resource_type = \"name_1\" delete_product_block(conn, resource_type)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_resource_type(conn: sa.engine.Connection, resource_type: str) -&gt; None:\n    \"\"\"Delete a resource type and it's occurrences in product blocks and products.\n\n    Args:\n        conn: DB connection as available in migration main file\n        resource_type: a resource_type name you want to delete\n\n    Example:\n        &gt;&gt;&gt; resource_type = \"name_1\"\n        &gt;&gt;&gt; delete_product_block(conn, resource_type)\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"\n            WITH deleted_pb AS (\n                DELETE FROM resource_types WHERE resource_type = :resource_type\n                    RETURNING resource_type_id),\n                 deleted_pb_rt AS (\n                     DELETE FROM product_block_resource_types WHERE resource_type_id IN (SELECT resource_type_id FROM deleted_pb))\n            SELECT *\n            from deleted_pb;\n            \"\"\"\n        ),\n        {\"resource_type\": resource_type},\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_resource_type_by_id","title":"delete_resource_type_by_id","text":"<pre><code>delete_resource_type_by_id(\n    conn: sqlalchemy.engine.Connection,\n    id: uuid.UUID | pydantic_forms.types.UUIDstr,\n) -&gt; None\n</code></pre> <p>Delete resource type by resource type id.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the resource type to delete.</p> </li> </ul> <p>Usage: <pre><code>resource_type_id = \"id\"\ndelete_resource_type_by_id(conn, resource_type_id)\n</code></pre></p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_resource_type_by_id(conn: sa.engine.Connection, id: UUID | UUIDstr) -&gt; None:\n    \"\"\"Delete resource type by resource type id.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        id: ID of the resource type to delete.\n\n    Usage:\n    ```python\n    resource_type_id = \"id\"\n    delete_resource_type_by_id(conn, resource_type_id)\n    ```\n    \"\"\"\n    conn.execute(sa.text(\"DELETE FROM resource_types WHERE resource_type_id=:id\"), {\"id\": id})\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_resource_types","title":"delete_resource_types","text":"<pre><code>delete_resource_types(\n    conn: sqlalchemy.engine.Connection, delete: collections.abc.Iterable\n) -&gt; None\n</code></pre> <p>Delete a resource type and it's occurrences in product blocks.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>delete</code>               (<code>collections.abc.Iterable</code>)           \u2013            <p>list of resource_type names you want to delete</p> </li> </ul> Example <p>obsolete_stuff = [\"name_1\", \"name_2\"] delete_resource_types(conn, obsolete_stuff)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_resource_types(conn: sa.engine.Connection, delete: Iterable) -&gt; None:\n    \"\"\"Delete a resource type and it's occurrences in product blocks.\n\n    Args:\n        conn: DB connection as available in migration main file\n        delete: list of resource_type names you want to delete\n\n    Example:\n        &gt;&gt;&gt; obsolete_stuff = [\"name_1\", \"name_2\"]\n        &gt;&gt;&gt; delete_resource_types(conn, obsolete_stuff)\n    \"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"DELETE\n               FROM product_block_resource_types\n                   USING resource_types\n               WHERE resource_types.resource_type_id = product_block_resource_types.resource_type_id\n                 AND resource_types.resource_type = ANY (:obsolete)\"\"\"\n        ),\n        {\"obsolete\": tuple(delete)},\n    )\n    conn.execute(sa.text(\"DELETE FROM resource_types WHERE resource_type in :obsolete;\"), {\"obsolete\": list(delete)})\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_resource_types_from_product_blocks","title":"delete_resource_types_from_product_blocks","text":"<pre><code>delete_resource_types_from_product_blocks(\n    conn: sqlalchemy.engine.Connection, delete: dict\n) -&gt; None\n</code></pre> <p>Delete resource type from product blocks.</p> <p>Note: the resource_type itself will not be deleted.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>delete</code>               (<code>dict</code>)           \u2013            <p>dict of product_blocks and resource_types names that you want to unlink</p> </li> </ul> Example <p>obsolete_stuff = {     \"ProductBlockName1\": {         \"resource_type1\": \"Resource description\"     },     \"ProductBlockName2\": {         \"resource_type1\": \"Resource description\",         \"resource_type1\": \"Resource description\"     } } delete_resource_types_from_product_blocks(conn, obsolete_stuff)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_resource_types_from_product_blocks(conn: sa.engine.Connection, delete: dict) -&gt; None:\n    \"\"\"Delete resource type from product blocks.\n\n    Note: the resource_type itself will not be deleted.\n\n    Args:\n        conn: DB connection as available in migration main file\n        delete: dict of product_blocks and resource_types names that you want to unlink\n\n    Example:\n        &gt;&gt;&gt; obsolete_stuff = {\n            \"ProductBlockName1\": {\n                \"resource_type1\": \"Resource description\"\n            },\n            \"ProductBlockName2\": {\n                \"resource_type1\": \"Resource description\",\n                \"resource_type1\": \"Resource description\"\n            }\n        }\n        &gt;&gt;&gt; delete_resource_types_from_product_blocks(conn, obsolete_stuff)\n    \"\"\"\n    for product_block_name, resource_types in delete.items():\n        conn.execute(\n            sa.text(\n                \"\"\"DELETE\n                   FROM product_block_resource_types\n                       USING resource_types\n                   WHERE\n                       product_block_id = (SELECT product_block_id FROM product_blocks WHERE name = :product_block_name)\n                     AND resource_types.resource_type_id = product_block_resource_types.resource_type_id\n                     AND resource_types.resource_type = ANY (:obsolete_resource_types)\"\"\"\n            ),\n            {\n                \"product_block_name\": product_block_name,\n                \"obsolete_resource_types\": list(resource_types.keys()),\n            },\n        )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.delete_workflow","title":"delete_workflow","text":"<pre><code>delete_workflow(conn: sqlalchemy.engine.Connection, name: str) -&gt; None\n</code></pre> <p>Delete a workflow and its occurrences in products.</p> <p>Note: the cascading delete rules in postgres will also ensure removal from <code>products_workflows</code>.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>a workflow name you want to delete</p> </li> </ul> Example <p>obsolete_stuff = \"name_1\" delete_workflow(conn, obsolete_stuff)</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def delete_workflow(conn: sa.engine.Connection, name: str) -&gt; None:\n    \"\"\"Delete a workflow and its occurrences in products.\n\n    Note: the cascading delete rules in postgres will also ensure removal from `products_workflows`.\n\n    Args:\n        conn: DB connection as available in migration main file\n        name: a workflow name you want to delete\n\n    Example:\n        &gt;&gt;&gt; obsolete_stuff = \"name_1\"\n        &gt;&gt;&gt; delete_workflow(conn, obsolete_stuff)\n    \"\"\"\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            DELETE\n            FROM workflows\n            WHERE name = :name;\n            \"\"\"\n        ),\n        {\"name\": name},\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.ensure_default_workflows","title":"ensure_default_workflows","text":"<pre><code>ensure_default_workflows(conn: sqlalchemy.engine.Connection) -&gt; None\n</code></pre> <p>Ensure products_workflows table contains a link between all 'active' workflows and the set of workflows identified in the DEFAULT_PRODUCT_WORKFLOWS app_setting.</p> <p>Note that the 0th element of the uuids are taken when generating product_workflow_table_rows because sqlalchemy returns a row tuple even if selecting for a single column.</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def ensure_default_workflows(conn: sa.engine.Connection) -&gt; None:\n    \"\"\"Ensure products_workflows table contains a link between all 'active' workflows and the set of workflows identified in the DEFAULT_PRODUCT_WORKFLOWS app_setting.\n\n    Note that the 0th element of the uuids are taken when generating product_workflow_table_rows because sqlalchemy returns a row tuple even if selecting for a single column.\n    \"\"\"\n    products = sa.Table(\"products\", sa.MetaData(), autoload_with=conn)\n    workflows = sa.Table(\"workflows\", sa.MetaData(), autoload_with=conn)\n    product_workflows_table = sa.Table(\"products_workflows\", sa.MetaData(), autoload_with=conn)\n\n    all_product_uuids = conn.execute(sa.select(products.c.product_id)).fetchall()\n    default_workflow_ids = conn.execute(\n        sa.select(workflows.c.workflow_id).where(workflows.c.name.in_(app_settings.DEFAULT_PRODUCT_WORKFLOWS))\n    ).fetchall()\n    product_workflow_table_rows = [\n        (product_uuid[0], workflow_uuid[0])\n        for product_uuid in all_product_uuids\n        for workflow_uuid in default_workflow_ids\n    ]\n    conn.execute(\n        sa.dialects.postgresql.insert(product_workflows_table)\n        .values(product_workflow_table_rows)\n        .on_conflict_do_nothing(index_elements=(\"product_id\", \"workflow_id\"))\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.get_all_active_products_and_ids","title":"get_all_active_products_and_ids","text":"<pre><code>get_all_active_products_and_ids(\n    conn: sqlalchemy.engine.Connection,\n) -&gt; list[dict[str, UUID | UUIDstr | str]]\n</code></pre> <p>Return a list, with dicts containing keys <code>product_id</code> and <code>name</code> of active products.</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def get_all_active_products_and_ids(conn: sa.engine.Connection) -&gt; list[dict[str, UUID | UUIDstr | str]]:\n    \"\"\"Return a list, with dicts containing keys `product_id` and `name` of active products.\"\"\"\n    result = conn.execute(sa.text(\"SELECT product_id, name  FROM products WHERE status='active'\"))\n    return [{\"product_id\": row[0], \"name\": row[1]} for row in result.fetchall()]\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.has_table_column","title":"has_table_column","text":"<pre><code>has_table_column(\n    table_name: str, column_name: str, conn: sqlalchemy.engine.Connection\n) -&gt; bool\n</code></pre> <p>Checks if the specified column exists in a given table.</p> <p>inspector.get_columns raises an exception if the table does not exist, so we catch that exception and return False. This is useful for migrations where you want to ensure that a column exists before performing operations on it.</p> <p>:param table_name: Name of the database table :param column_name: Name of the column to check :param conn: SQLAlchemy database Connection :return: True if the column exists, False otherwise</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def has_table_column(table_name: str, column_name: str, conn: sa.engine.Connection) -&gt; bool:\n    \"\"\"Checks if the specified column exists in a given table.\n\n    inspector.get_columns raises an exception if the table does not exist, so we catch that exception and return False.\n    This is useful for migrations where you want to ensure that a column exists before performing operations on it.\n\n    :param table_name: Name of the database table\n    :param column_name: Name of the column to check\n    :param conn: SQLAlchemy database Connection\n    :return: True if the column exists, False otherwise\n    \"\"\"\n    result = conn.execute(\n        sa.text(\n            \"\"\"\n            SELECT column_name\n            FROM information_schema.columns\n            WHERE table_name = :table_name and column_name = :column_name\n            \"\"\"\n        ),\n        {\n            \"table_name\": table_name,\n            \"column_name\": column_name,\n        },\n    )\n    return result.first() is not None\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.insert_resource_type","title":"insert_resource_type","text":"<pre><code>insert_resource_type(\n    conn: sqlalchemy.engine.Connection, resource_type: str, description: str\n) -&gt; None\n</code></pre> <p>Create a new resource types.</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def insert_resource_type(conn: sa.engine.Connection, resource_type: str, description: str) -&gt; None:\n    \"\"\"Create a new resource types.\"\"\"\n    conn.execute(\n        sa.text(\n            \"\"\"INSERT INTO resource_types (resource_type, description)\n               VALUES (:resource_type, :description)\n               ON CONFLICT DO NOTHING;\"\"\"\n        ),\n        {\n            \"resource_type\": resource_type,\n            \"description\": description,\n        },\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.remove_product_block_relation_between_products_by_id","title":"remove_product_block_relation_between_products_by_id","text":"<pre><code>remove_product_block_relation_between_products_by_id(\n    conn: sqlalchemy.engine.Connection,\n    in_use_by_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n    depends_on_id: uuid.UUID | pydantic_forms.types.UUIDstr,\n) -&gt; None\n</code></pre> <p>Remove product block relation by id.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>in_use_by_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the product block that uses another product block.</p> </li> <li> <code>depends_on_id</code>               (<code>uuid.UUID | pydantic_forms.types.UUIDstr</code>)           \u2013            <p>ID of the product block that is used as dependency.</p> </li> </ul> Usage <p>in_use_by_id = \"in_use_by_id\" depends_on_id = \"depends_on_id\" remove_product_block_relation_between_products_by_id(     conn, in_use_by_id, depends_on_id )</p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def remove_product_block_relation_between_products_by_id(\n    conn: sa.engine.Connection, in_use_by_id: UUID | UUIDstr, depends_on_id: UUID | UUIDstr\n) -&gt; None:\n    \"\"\"Remove product block relation by id.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        in_use_by_id: ID of the product block that uses another product block.\n        depends_on_id: ID of the product block that is used as dependency.\n\n    Usage:\n        &gt;&gt;&gt; in_use_by_id = \"in_use_by_id\"\n        &gt;&gt;&gt; depends_on_id = \"depends_on_id\"\n        &gt;&gt;&gt; remove_product_block_relation_between_products_by_id(\n            conn, in_use_by_id, depends_on_id\n        )\n    \"\"\"\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            DELETE\n            FROM product_block_relations\n            WHERE in_use_by_id = :in_use_by_id\n              AND depends_on_id = :depends_on_id\n            \"\"\"\n        ),\n        {\n            \"in_use_by_id\": in_use_by_id,\n            \"depends_on_id\": depends_on_id,\n        },\n    )\n</code></pre>"},{"location":"reference-docs/workflows/workflows/#orchestrator.migrations.helpers.remove_products_from_workflow_by_product_tag","title":"remove_products_from_workflow_by_product_tag","text":"<pre><code>remove_products_from_workflow_by_product_tag(\n    conn: sqlalchemy.engine.Connection,\n    workflow_name: str,\n    product_tag: str,\n    product_name_like: str = \"%%\",\n) -&gt; None\n</code></pre> <p>Delete products from a workflow by product tag.</p> <p>Parameters:</p> <ul> <li> <code>conn</code>               (<code>sqlalchemy.engine.Connection</code>)           \u2013            <p>DB connection as available in migration main file.</p> </li> <li> <code>workflow_name</code>               (<code>str</code>)           \u2013            <p>Name of the workflow that the products need to be removed from.</p> </li> <li> <code>product_tag</code>               (<code>str</code>)           \u2013            <p>Tag of the product to remove from the workflow.</p> </li> <li> <code>product_name_like</code>               (<code>optional</code>, default:                   <code>'%%'</code> )           \u2013            <p>Part of the product name to get more specific products (necessary for fw v2)</p> </li> </ul> <p>Usage: <pre><code>product_tag = \"product_tag\"\nworkflow_name = \"workflow_name\"\nremove_products_from_workflow_by_product_tag(conn, product_tag, workflow_name)\n</code></pre></p> Source code in <code>orchestrator/migrations/helpers.py</code> <pre><code>def remove_products_from_workflow_by_product_tag(\n    conn: sa.engine.Connection, workflow_name: str, product_tag: str, product_name_like: str = \"%%\"\n) -&gt; None:\n    \"\"\"Delete products from a workflow by product tag.\n\n    Args:\n        conn: DB connection as available in migration main file.\n        workflow_name: Name of the workflow that the products need to be removed from.\n        product_tag: Tag of the product to remove from the workflow.\n        product_name_like (optional): Part of the product name to get more specific products (necessary for fw v2)\n\n    Usage:\n    ```python\n    product_tag = \"product_tag\"\n    workflow_name = \"workflow_name\"\n    remove_products_from_workflow_by_product_tag(conn, product_tag, workflow_name)\n    ```\n    \"\"\"\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            DELETE\n            FROM products_workflows\n            WHERE workflow_id = (SELECT workflow_id\n                                 FROM workflows\n                                 where name = :workflow_name)\n              AND product_id IN (SELECT product_id\n                                 FROM products\n                                 WHERE tag = :product_tag\n                                   AND name LIKE :product_name_like)\n            \"\"\"\n        ),\n        {\n            \"workflow_name\": workflow_name,\n            \"product_tag\": product_tag,\n            \"product_name_like\": product_name_like,\n        },\n    )\n</code></pre>"},{"location":"workshops/advanced/bootstrap/","title":"Bootstrapping the environment","text":"<p>Here is how you can run the orchestrator-core, orchestrator-ui, and netbox with Docker Compose. We have this all setup in our docker-compose.yml file so that you don't have to think about how to start the applications required for this workshop! If you want to read more about how to manually install the Workflow Orchestrator, please refer to the beginner workshop here. The following Docker images are used in this workshop:</p> <ul> <li>orchestrator-core: The workflow orchestrator step engine.</li> <li>orchestrator-ui: The   GUI for the orchestrator-core.</li> <li>netbox: A free IPAM and SoT system.</li> <li>postgres: The PostgreSQL object-relational database system.</li> <li>redis: An open source, in-memory data store used by netbox</li> <li>Optional: containerlab: A free network topology simulator that uses containerized   network operating systems.</li> </ul> <p>Danger</p> <p>To run the workshop with container lab, the host architecture must be x86_64 with virtualization enabled</p>"},{"location":"workshops/advanced/bootstrap/#step-1-cloning-the-repo","title":"Step 1 - Cloning the repo","text":"<p>The fist step is to clone the Example orchestrator repository using: <pre><code>git clone https://github.com/workfloworchestrator/example-orchestrator.git\n</code></pre> At this point, you have a functional environment to start play with. This includes:</p> <ul> <li>The orchestrator (core and GUI)</li> <li>Netbox (the entire stack including database, workers, etc...)</li> <li>LSO (to run ansible playbooks)</li> <li>An example containerlab topology based on Nokia SRlinux.</li> <li>Some examples of Ansible playbooks</li> </ul> <p>The directory structure looks like this: <pre><code>~/example-orchestrator# ls -la\ntotal 144\ndrwxr-xr-x 14 root root  4096 May 10 10:29 .\ndrwx------ 16 root root  4096 May 15 06:42 ..\n-rw-r--r--  1 root root   147 May  8 10:35 .env.example\ndrwxr-xr-x  8 root root  4096 May 15 10:15 .git\n-rw-r--r--  1 root root    82 May  8 10:56 .gitignore\ndrwxr-xr-x  2 root root  4096 May  8 10:35 .pictures\n-rw-r--r--  1 root root 50000 May  8 10:56 README.md\n-rw-r--r--  1 root root   884 May  8 10:35 alembic.ini\ndrwxr-xr-x  4 root root  4096 May  9 19:42 ansible &lt;&lt;&lt;&lt; Ansible playbooks\ndrwxr-xr-x  3 root root  4096 May 15 10:16 clab &lt;&lt;&lt;&lt; Containerlab topology\ndrwxr-xr-x  9 root root  4096 May  8 10:56 docker &lt;&lt;&lt;&lt; Docker folder for config etc..\n-rw-r--r--  1 root root  4469 May 10 10:29 docker-compose.yml &lt;&lt;&lt;&lt; The docker compoose file to spin up orchestrator stack\n-rw-r--r--  1 root root   920 May  8 10:35 main.py\ndrwxr-xr-x  3 root root  4096 May  8 10:35 migrations\ndrwxr-xr-x  5 root root  4096 May  8 10:56 products\n-rw-r--r--  1 root root   100 May  8 10:35 pyproject.toml\n-rw-r--r--  1 root root    41 May  8 10:35 requirements.txt\ndrwxr-xr-x  2 root root  4096 May 10 10:46 services\n-rw-r--r--  1 root root   942 May  8 10:35 settings.py\ndrwxr-xr-x  2 root root  4096 May  8 10:35 templates\ndrwxr-xr-x  2 root root  4096 May  8 10:35 translations\ndrwxr-xr-x  2 root root  4096 May  8 10:35 utils\ndrwxr-xr-x  7 root root  4096 May  8 10:56 workflows\n</code></pre></p>"},{"location":"workshops/advanced/bootstrap/#step-2-editing-the-environment","title":"Step 2 - Editing the environment","text":"<p>Before starting up the stacks, we have to check the file:</p> <pre><code>docker/orchestrator-ui/orchestrator-ui.env\n</code></pre> <p>and adjust the LISTENING_IP value:</p> <p><pre><code>ENVIRONMENT_NAME=\"Example Orchestrator\"\n\nORCHESTRATOR_API_HOST=http://&lt;$LISTENING_IP&gt;:8080\nORCHESTRATOR_API_PATH=/api\nORCHESTRATOR_GRAPHQL_HOST=http://&lt;$LISTENING_IP&gt;:8080\nORCHESTRATOR_GRAPHQL_PATH=/api/graphql\nNEXTAUTH_SECRET=ToDo\n</code></pre> * if you are running the orchestrator locally, for example on your laptop, use \"localhost\" * if you are running the orchestrator on a remote machine, use the IP of that machine, for example 1.2.3.4</p>"},{"location":"workshops/advanced/bootstrap/#step-3-starting-the-environment","title":"Step 3 - Starting the environment","text":"<p>Once you edited the file, you can start the docker environment with:</p>"},{"location":"workshops/advanced/bootstrap/#remote-lab-environment-with-lso-and-containerlab","title":"Remote lab environment with LSO and Containerlab","text":"<p><pre><code>COMPOSE_PROFILES=lso docker compose up\n</code></pre> This will also enable LSO, so network devices will be actually configured. If you don't want this, just use:</p>"},{"location":"workshops/advanced/bootstrap/#local-environment","title":"Local environment","text":"<p>The environment requires several ports to be free. Use either command below to check if any are in use. No output means they're available:</p> <pre><code>netstat -tulnp | grep -E ':80|:3000|:4000|:5432|:5678|:8000|:8001|:8080'\nss -tulnp|grep -E ':80|:3000|:4000|:5432|:5678|:8000|:8001|:8080'\n</code></pre> <p>Start the containers:</p> <pre><code>docker compose up -d\n</code></pre> <p>and you should be able to view the applications here:</p> <p>If you are using your laptop:</p> <ol> <li>Orchestrator ui: Frontend: http://localhost:3000</li> <li>Orchestrator backend: REST api: http://localhost:8080/api/redoc and Graphql API: http://localbost:8080/api/graphql</li> <li>Netbox (admin|admin): Netbox: http://localhost:8000</li> </ol> <p>If you are using a remote machine:</p> <ol> <li>Orchestrator ui: Frontend: http://&lt;$IP_ADDRESS_OF_THE_MACHINE&gt;:3000</li> <li>Orchestrator backend: REST api: http://&lt;$IP_ADDRESS_OF_THE_MACHINE&gt;:8080/api/redoc    Graphql API: http://&lt;$IP_ADDRESS_OF_THE_MACHINE&gt;:8080/api/graphql</li> <li>Netbox (admin|admin): Netbox: http://&lt;$IP_ADDRESS_OF_THE_MACHINE&gt;:8000</li> </ol> <p>Note</p> <p>Take your time to familiarise with the applications and make sure they are working correctly. You can then continue with the following steps.</p>"},{"location":"workshops/advanced/bootstrap/#optional-step-4-containerlab","title":"Optional: Step 4 Containerlab","text":"<p>Now that we have our orchestrator stack running, we can spin up the containerlab topology:</p> <p><pre><code>cd clab\ncontainerlab deploy\n</code></pre> At the end of this process we can use <code>containerlab inspect</code> to check the status of our topology:</p> <pre><code>~/example-orchestrator/clab# containerlab inspect\nINFO[0000] Parsing &amp; checking topology file: srlinux01.clab.yaml\n+---+-----------------------+--------------+-----------------------+------+---------+----------------+--------------+\n| # |         Name          | Container ID |         Image         | Kind |  State  |  IPv4 Address  | IPv6 Address |\n+---+-----------------------+--------------+-----------------------+------+---------+----------------+--------------+\n| 1 | clab-orch-demo-ams-pe | 46ddee7df745 | ghcr.io/nokia/srlinux | srl  | running | 172.22.0.11/16 | N/A          |\n| 2 | clab-orch-demo-lon-pe | fe3f5d6eb35e | ghcr.io/nokia/srlinux | srl  | running | 172.22.0.10/16 | N/A          |\n| 3 | clab-orch-demo-par-p  | 4831968e075c | ghcr.io/nokia/srlinux | srl  | running | 172.22.0.9/16  | N/A          |\n+---+-----------------------+--------------+-----------------------+------+---------+----------------+--------------+\n</code></pre> <p>And with the command: <pre><code>containerlab graph\n</code></pre> we can have a nice rendering of the topology served on port 50080 (for example https://localhost:50080).</p> <p>The topology we are going to use is something like this one:</p> <p></p> <p>The Example orchestrator used in this workshop already has a number of products pre-configured and ready to be used:</p> <ul> <li>Nodes (including Ansible to deploy example config)</li> <li>Core-links (including Ansible to deploy/delete example config)</li> <li>Ports</li> <li>L2VPN</li> </ul> <p>We can start feeding initial data into the environment and run some workflows!</p>"},{"location":"workshops/advanced/bootstrap/#helpful-items","title":"Helpful Items","text":""},{"location":"workshops/advanced/bootstrap/#resetting-your-environment","title":"Resetting Your Environment","text":"<p>To reset the active state of your environment back to scratch, simply use docker compose to delete volumes, like so:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose down -v\n</code></pre> <p>You can then restart the containers as described above.</p>"},{"location":"workshops/advanced/create-your-own/","title":"Create your own workflow and product","text":"<p>To cap off this workshop we will create a new product and workflows by using the built in tools that the Workflow Orchestrator provides the user. In this scenario you will create a product that is very similar to the provided L2VPN product, but constrained to two interfaces. In other words a L2 Point-to-Point circuit.</p>"},{"location":"workshops/advanced/create-your-own/#l2-point-to-point-model","title":"L2 Point-to-Point model","text":"<p>The Layer 2 point-to-point service is modelled using two product blocks. The l2_point_to_point product block holds the pointers to IMS and the NRM, the speed of the circuit, and whether the speed policer is enabled or not, as well as pointers to the two service attach points. The latter are modelled with the L2_service_attach_point product block and keep track of the port associated with that endpoint and, in the case where 802.1Q has to be enabled, the VLAN range used. The service can either be deployed protected or unprotected in the service provider network. This is administered with the fixed input protection_type.</p> <p></p> <ul> <li>protection_type: this service is either unprotected or protected</li> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>speed: the speed of the point-to-point service in Mbit/s</li> <li>speed_policer: enable the speed policer for this service</li> <li>sap: a constrained list of exactly two Layer2 service attach points</li> <li>vlan_range: range of Layer 2 labels to be used on this endpoint of the service</li> <li>port: link to the Port product block this service endpoint connects to</li> </ul>"},{"location":"workshops/advanced/docker-installation/","title":"Installation Instructions","text":"<p>Here is how you can run the orchestrator-core, orchestrator-ui, and netbox with Docker Compose. We have this all setup in our docker-compose.yml file so that you don't have to think about how to start the applications required for this workshop! If you want to read more about how to manually install the Workflow Orchestrator, please refer to the beginner workshop here. The following Docker images are used in this workshop:</p> <ul> <li>orchestrator-core: The workflow orchestrator step engine.</li> <li>orchestrator-ui: The   GUI for the orchestrator-core.</li> <li>netbox: A free IPAM and SoT system.</li> <li>postgres: The PostgreSQL object-relational database system.</li> <li>redis: An open source, in-memory data store used by netbox</li> <li>Optional: containerlab: A free network topology simulator that uses containerized   network operating systems.</li> </ul> <p>Danger</p> <p>To run the workshop with container lab, the host architecture must be x86_64 with virtualization enabled</p>"},{"location":"workshops/advanced/docker-installation/#step-1-prepare-environment","title":"Step 1 - Prepare environment","text":"<p>Ensure that you have docker and docker compose installed on your system. We won't go into deep details on how to do this as we expect you to have the knowledge to provide a working docker setup for this workshop. To make sure that docker is setup properly, run the following checks:</p> <p>First, let's make sure that docker is installed:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker --version\nDocker version 23.0.1, build a5ee5b1dfc\n</code></pre> <p>In this case, we see that version 23.0.1 is installed, which is plenty new enough for this workshop. Any version of docker later than <code>19.03.0</code> should work for this.</p> <p>Next, let's make sure that we have Docker Compose v2 setup on our machine:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose version\nDocker Compose version v2.17.2\n</code></pre> <p>Tip</p> <p>If this command does not work and produce a similar output, follow the official Docker guide on installing the Docker Compose v2 plugin.</p>"},{"location":"workshops/advanced/docker-installation/#step-2-start-environment","title":"Step 2 - Start environment","text":"<p>Docker compose will take care of all necessary initialization and startup of the database, orchestrator and GUI:</p> <ol> <li>A postgres container, holding the databases for netbox and the orchestrator</li> <li>A redis container used by netbox.</li> <li>A set of containers spun up by netbox.</li> <li>An orchestrator backend container that runs off main.py</li> <li>Finally, a GUI frontend container is started.</li> </ol> <p>To start all of this, simply clone the repo:</p> <pre><code>jlpicard@ncc-1701-d:~$ git clone git@github.com:workfloworchestrator/example-orchestrator.git\n</code></pre> <p>and then start the containers!</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose up -d\n</code></pre>"},{"location":"workshops/advanced/docker-installation/#step-3-open-a-browser","title":"Step 3 - Open a browser","text":"<p>Now point a web browser to <code>http://localhost:3000/</code> and have a look around. This is a functional orchestrator instance and represents an environment where you can perform the exercises that are part of this workshop.</p> <p>Tip</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive. This workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/advanced/docker-installation/#helpful-items","title":"Helpful Items","text":""},{"location":"workshops/advanced/docker-installation/#resetting-your-environment","title":"Resetting Your Environment","text":"<p>To reset the active state of your environment back to scratch, simply use docker compose to delete volumes, like so:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose down -v\n</code></pre> <p>You can then restart the containers as described above.</p>"},{"location":"workshops/advanced/docker-installation/#accessing-netbox","title":"Accessing Netbox","text":"<p>Netbox can be accessed for troubleshooting and verifying that everything you have done in the workflow is working properly by pointing your web browser to <code>http://localhost:8000</code>. From there, you can login with <code>admin/admin</code>.</p>"},{"location":"workshops/advanced/domain-models/","title":"Domain models","text":""},{"location":"workshops/advanced/domain-models/#introduction","title":"Introduction","text":"<p>First read the Architecture; TL;DR section of the orchestrator core documentation to get an overview of the concepts that will be covered.</p>"},{"location":"workshops/advanced/domain-models/#products","title":"Products","text":"<p>The Orchestrator uses the concept of a Product to describe what can be built to the end user. When a user runs a workflow to create a Product, this results in a unique instance of that product called a Subscription. A Subscription is always tied to a certain lifecycle state (eg. Initial, Provisioning, Active, Terminated, etc) and is unique per customer. In other words a Subscription contains all the information needed to uniquely identify a certain resource owned by a user/customer that conforms to a certain definition, namely a Product.</p>"},{"location":"workshops/advanced/domain-models/#product-description-in-python","title":"Product description in Python","text":"<p>Products are described in Python classes called Domain Models. These classes are designed to help the developer manage complex subscription models and interact with the objects in a developer-friendly way. Domain models use Pydantic[^6] with some additional functionality to dynamically cast variables from the database, where they are stored as a string, to their correct type in Python at runtime. Pydantic uses Python type hints to validate that the correct type is assigned. The use of typing, when used together with type checkers, already helps to make the code more robust, furthermore the use of Pydantic makes it possible to check variables at runtime which greatly improves reliability.</p>"},{"location":"workshops/advanced/domain-models/#example-of-runtime-typecastingsafety","title":"Example of \"Runtime typecasting/safety\"","text":"<p>In the example below we attempt to access a resource that has been stored in an instance of a product (subscription instance). It shows how it can be done directly through the ORM and it shows the added value of Domain Models on top of the ORM.</p> <p>Serialisation direct from the database</p> <pre><code>&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; instance_value_from_db\n\"False\"\n&gt;&gt;&gt; if instance_value_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"True\"\n</code></pre> <p>Serialisation using domain models</p> <pre><code>&gt;&gt;&gt; class ProductBlock(ProductBlockModel):\n...     instance_from_db: bool\n...\n&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; type(instance_value_from_db)\n&lt;class str&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model = SubscriptionModel.from_subscription(\"ID\")\n&gt;&gt;&gt; type(subscription_model.product_block.instance_from_db)\n&lt;class bool&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model.product_block.instance_from_db\nFalse\n&gt;&gt;&gt;\n&gt;&gt;&gt; if subscription_model.product_block.instance_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"False\"\n</code></pre> <p>As you can see in the example above, interacting with the data stored in the database rows, helps with some of the heavy lifting, and makes sure the database remains generic and it's schema remains stable.</p>"},{"location":"workshops/advanced/domain-models/#product-structure","title":"Product Structure","text":"<p>A Product definition has two parts in its structure. The Higher order product type that contains information describing the product in a more general sense, and multiple layers of product blocks that logically describe the set of resources that make up the product definition. The product type describes the fixed inputs and the top-level product blocks. The fixed inputs are used to differentiate between variants of the same product, for example the speed of a network port. There is always at least one top level product block that contains the resource types to administer the customer facing input. Beside resource types, the product blocks usually contain links to other product blocks as well. If a fixed input needs a custom type, then it is defined here together with fixed input definition.</p>"},{"location":"workshops/advanced/domain-models/#terminology","title":"Terminology","text":"<ul> <li>Product: A definition of what can be instantiated through a Subscription.</li> <li>Product Type: The higher order definition of a Product. Many different Products can exist within a Product Type.</li> <li>Fixed Input: Product attributes that discriminate the different Products that adhere to the same Product Type definition.</li> <li>Product Block: A (logical) construct that contain references to other Product Blocks or Resource Types. It gives   structure to the product definition and defines what resources are related to other resources</li> <li>Resource Types: Customer facing attributes that are the result of choices made by the user whilst filling an   input form. This can be a value the user chose, or an identifier towards a different system.</li> </ul>"},{"location":"workshops/advanced/domain-models/#product-types","title":"Product types","text":"<p>The product types in the code are upper camel cased. Per default, the product type is declared for the inactive, provisioning and active lifecycle states, and the product type name is suffixed with the state if the lifecycle is not active. Usually, the lifecycle state starts with inactive, and then transitions through provisioning to active, and finally to terminated. During its life, the subscription, an instantiation of a product for a particular customer, can transition from active to provisioning and back again many times, before it ends up terminated. The terminated state does not have its own type definition, but will default to initial unless otherwise defined.</p>"},{"location":"workshops/advanced/domain-models/#domain-model-aka-product-type-definition","title":"Domain Model a.k.a Product Type Definition","text":"<pre><code>class PortInactive(SubscriptionModel, is_base=True):\n    speed: PortSpeed\n    port: PortBlockInactive\n\nclass PortProvisioning(PortInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    speed: PortSpeed\n    port: PortBlockProvisioning\n\nclass Port(PortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    speed: PortSpeed\n    port: PortBlock\n</code></pre> <p>As can be seen in the above example, the inactive product type definition is subclassed from SubScriptionModel, and the following definitions are subclassed from the previous one. This product has one fixed input called speed and one port product block (see below about naming). Notice that the port product block matches the lifecycle of the product, for example, the PortInactive product has a PortBlockInactive product block, but it is totally fine to use product blocks from different lifecycle states if that suits your use case.</p>"},{"location":"workshops/advanced/domain-models/#fixed-input","title":"Fixed Input","text":"<p>Because a port is only available in a limited number of speeds, a separate type is declared with the allowed values, see below.</p> <pre><code>from enum import IntEnum\n\nclass PortSpeed(IntEnum):\n    _1000 = 1000\n    _10000 = 10000\n    _40000 = 40000\n    _100000 = 100000\n    _400000 = 400000\n</code></pre> <p>This type is not only used to ensure that the speed fixed input can only take these values, but is also used in user input forms to limit the choices, and in the database migration to register the speed variant of this product.</p>"},{"location":"workshops/advanced/domain-models/#wiring-it-up-in-the-orchestrator","title":"Wiring it up in the Orchestrator","text":"This section contains advanced information about how to configure the Orchestrator. It is also possible to use a more user friendly tool available here. This tool uses a configuration file to generate the boilerplate, migrations and configuration necessary to make use of the product straight away.   Products need to be registered in two places. All product variants have to be added to the `SUBSCRIPTION_MODEL_REGISTRY`, in `products/__init__.py`, as shown below.  <pre><code>from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\nfrom products.product_types.core_link import CoreLink\n\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"core link 10G\": CoreLink,\n        \"core link 100G\": CoreLink,\n    }\n)\n</code></pre>  And all variants also have to entered into the database using a migration. The migration uses the create helper function from `orchestrator.migrations.helpers` that takes the following dictionary as an argument, see below. Notice that the name of the product and the product type need to match with the subscription model registry.  <pre><code>from orchestrator.migrations.helpers import create\n\nnew_products = {\n    \"products\": {\n        \"core link 10G\": {\n            \"product_id\": uuid4(),\n            \"product_type\": \"CoreLink\",\n            \"description\": \"Core link\",\n            \"tag\": \"CORE_LINK\",\n            \"status\": \"active\",\n            \"product_blocks\": [\n                \"CoreLink\",\n                \"CorePort\",\n            ],\n            \"fixed_inputs\": {\n                \"speed\": CoreLinkSpeed._10000.value,\n            },\n        },\n}\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    create(conn, new_products)\n</code></pre>"},{"location":"workshops/advanced/domain-models/#product-blocks","title":"Product blocks","text":"<p>Like product types, the product blocks are declared for the inactive, provisioning and active lifecycle states. The name of the product block is suffixed with the word Block, to clearly distinguish them from the product types, and again suffixed by the state if the lifecycle is not active.</p> <p>Every time a subscription is transitioned from one lifecycle to another, an automatic check is performed to ensure that resource types that are not optional are in fact present on that instantiation of the product block. This safeguards for incomplete administration for that lifecycle state.</p>"},{"location":"workshops/advanced/domain-models/#resource-type-lifecycle-when-to-use-none","title":"Resource Type lifecycle. When to use <code>None</code>","text":"<p>The resource types on an inactive product block are usually all optional to allow the creation of an empty product block instance. All resource types that are used to hold the user input for the subscription is stored using resource types that are not optional anymore in the provisioning lifecycle state. All resource types used to store information that is generated while provisioning the subscription is stored using resource types that are optional while provisioning but are not optional anymore for the active lifecycle state. Resource types that are still optional in the active state are used to store non-mandatory information.</p>"},{"location":"workshops/advanced/domain-models/#example","title":"Example","text":"<pre><code>class NodeBlockInactive(ProductBlockModel, product_block_name=\"Node\"):\n    type_id: int | None = None\n    node_name: str | None = None\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    node_description: str | None = None\n\nclass NodeBlockProvisioning(NodeBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    type_id: int\n    node_name: str\n    ims_id: int | None = None\n    nrm_id: int | None = None\n    node_description: str | None = None\n\nclass NodeBlock(NodeBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    type_id: int\n    node_name: str\n    ims_id: int\n    nrm_id: int\n    node_description: str | None = None\n</code></pre> <p>In the simplified node product block shown above, the type and the name of the node are supplied by the user and stored on the <code>NodeBlockInactive</code>. Then, the subscription transitions to Provisioning and a check is performed to ensure that both pieces of information are present on the product block. During the provisioning phase the node is administered in IMS and the handle to that information is stored on the <code>NodeBlockProvisioning</code>. Next, the node is provisioned in the NRM and the handle is also stored. If both of these two actions were successful, the subscription is transitioned to Active and it is checked that the type and node name, and the IMS and NRM ID, are present on the product block. The description of the node remains optional, even in the active state. These checks ensure that information that is necessary for a particular state is present so that the actions that are performed in that state do not fail.</p>"},{"location":"workshops/advanced/domain-models/#product-block-customisation","title":"Product Block customisation","text":"<p>Sometimes there are resource types that depend on information stored on other product blocks, even on linked product blocks that do not belong to the same subscription. This kind of types need to be calculated at run time so that they include the most recent information. Consider the following example of a, stripped down version, of a port and node product block, and a title for the port block that is generated dynamically.</p> <pre><code>class NodeBlock(NodeBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    node_name: str\n\nclass PortBlock(PortBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    port_name: str\n    node: NodeBlock\n\n    @serializable_property\n    def title(self) -&gt; str:\n        return f\"{self.port_name} on {self.node.node_name}\"\n\nclass Port(PortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    port: PortBlock\n</code></pre> <p>A <code>@serializable_property</code> has been added that will dynamically render the title of the port product block. Even after a modify workflow was run to change the node name on the node subscription, the title of the port block will always be up to date. The title can be referenced as any other resource type using subscription.port.title. This is not a random example, the title of a product block is used by the orchestrator GUI while displaying detailed subscription information.</p>"},{"location":"workshops/advanced/execute-workflows/","title":"Seeding data","text":"<p>The topology in the previous section will be used in the workshop as an example of what a network could look like. Obviously it is possible to create any \"physical\" topology you like and build the \"logical\" topology that matches using th Workflow Orchestrator.</p>"},{"location":"workshops/advanced/execute-workflows/#putting-initial-data-in-place","title":"Putting initial data in place","text":"<p>The first thing we are going to do is populate Netbox with some initial data such as Manifacturers and Device types as well as some networks allocated for:</p> <ul> <li>Loopback addressing</li> <li>Core links addressing</li> </ul> <p>This is done using the task \"Netbox Bootstrap\" under the tasks submenu.</p> <p>Once the workflow has successfully ran, we can login into netbox (admin/admin) and check the situation: we should see some vendors and some network device models. In the IPAM section we are going to reserve the first address of the loopback newtworks since certain network devices dont like \"network addresses\" to be used as loopback addresses.</p> <ul> <li>from the IPv4 prefix 10.0.127.0/24 we allocate the address 10.0.127.0 marking it with a description \"RESERVED\"</li> <li>from the IPv6 prefix fc00:0:0:127::/64 we allocate the address fc00:0:0:127:: marking it with a description \"RESERVED\"</li> </ul> <p>We do this from IPAM &gt;&gt; Prefixes.</p>"},{"location":"workshops/advanced/execute-workflows/#deploying-the-nodes","title":"Deploying the nodes","text":"<p>Now we should be able to deploy our routers using the <code>create node</code> workflow. This is going to be a new subscription of the product node - specifically a nokia node -  and we will have to fill an initial form.</p> <p>Note</p> <p>Make sure that the node name is the same as the node name in containerlab (clab-orch-demo-ams-pe/clab-orch-demo-lon-pe/clab-orch-demo-par-p)</p> <p>Once the workflow has successfully ran, we can login into the node just configured and take a look at the config: <pre><code>ssh clab-orch-demo-ams-pe -l admin ##PWD: NokiaSrl1!\n</code></pre> We can do the same in Netbox, and we will notice that these nodes have no interfaces, to create them in netbox, we can use a workflow. Specifically the \"Update Node Interfaces\" workflow that will seed the necessary data into Netbox, so we can re-use it later.</p> <p>We can practice this deployin all the 3 nodes in the topology.</p>"},{"location":"workshops/advanced/execute-workflows/#deploying-core-links","title":"Deploying core links","text":"<p>Once we have 2 nodes configured, we should be able to deploy a core link between them using the \"create core link 10G\" workflow.</p> <p>You can login into the router and check the status of ISIS using:</p> <pre><code>show network-instance default protocols isis adjacency\nshow network-instance default protocols isis interface\n</code></pre>"},{"location":"workshops/advanced/generator/","title":"Product and Workflow Generator","text":"<p>To create a new product configuration and wire up the python, database and workflows correctly you need to create a lot of boilerplate configuration and code. To speed up this process and make the experience as user friendly as possible, initial configuration of what a product looks like can be created with a yaml file.</p> <p>At the end of these steps the developer will have all the necessary configuration and boilerplate completed to run the workflow and start developing on implementing the business logic.</p>"},{"location":"workshops/advanced/generator/#step-1-create-the-product-configuration-file","title":"Step 1 - Create the product configuration file","text":"<p>Open the Example Orchestrator directory and list the templates directory. It should look similar to this:</p> <pre><code>~/Documents/SURF/projects/example-orchestrator\n\u276f ls -l templates\ntotal 32\n-rw-r--r--  1 boers001  staff  2687 Mar  7 11:28 core_link.yaml\n-rw-r--r--  1 boers001  staff  2052 Mar  7 11:28 l2vpn.yaml\n-rw-r--r--  1 boers001  staff  2575 Mar  7 11:28 node.yaml\n-rw-r--r--  1 boers001  staff  2444 Mar  7 11:28 port.yaml\n~/Documents/SURF/projects/example-orchestrator\n</code></pre> <p>This directory houses all the configuration of the initial products that the example orchestrator provides. It is a starting point for developing new products. In this workshop we will create a new file and generate the L2-Point-to-Point model and workflows by configuring it with this yaml file.</p>"},{"location":"workshops/advanced/generator/#step-2-configure-the-yaml","title":"Step 2 - Configure the YAML","text":"<p>Create a new file in the template directory called <code>l2-p2p.yaml</code> <pre><code>touch templates/l2-p2p.yaml\n</code></pre> This file will contain the Initial product type configuration. Please create a yaml configuration reflecting the product model as described on the previous page. The goal is to configure the generator to reuse as many of the product blocks already existing in the orchestrator as possible.</p> <p>Inspiration</p> <p>Take a look at the <code>l2vpn.yaml</code> model for inspiration. As you can see this file has been configured in a certain way to reflect the configuration of the product. For more in depth documentation take a look at the reference doc.</p> <p>What can I do when I encounter errors?</p> <p>If you get stuck just remove all generated files, edit the yaml and try again.</p> Answer - l2-p2p.yaml <p>When creating the YAML you should notice that you do not have to create the SAP product blocks again. You just have reference the SAPS in the Virtual Circuit configuration. In this way you start reusing existing building blocks that already exist in the orchestrator. We cannot reuse the existing Virtual Circuit with the generator due to the different limits on the amount of SAPS that can be connected to the Virtual Circuit of the L2 P2P product.</p>"},{"location":"workshops/advanced/generator/#yaml-file","title":"Yaml file","text":"<pre><code>config:\n  summary_forms: true\nname: l2_p2p\ntype: L2P2P\ntag: L2P2P\ndescription: \"L2 Point-to-Point\"\nfixed_inputs:\n  - name: protection_type\n    type: enum\n    enum_type: str\n    values:\n      - protected\n      - unprotected\n    description: \"Level of network redundancany\"\nproduct_blocks:\n  - name: l2_p2p_virtual_circuit\n    type: L2P2PVirtualCircuit\n    tag: VC\n    description: \"virtual circuit product block\"\n    fields:\n      - name: saps\n        type: list\n        description: \"Virtual circuit service access points\"\n        list_type: SAP\n        min_items: 2\n        max_items: 2\n        required: provisioning\n      - name: speed\n        description: \"speed of the L2VPN im Mbit/s\"\n        type: int\n        required: provisioning\n        modifiable:\n      - name: speed_policer\n        description: \"speed policer active?\"\n        type: bool\n        required: provisioning\n        modifiable:\n      - name: ims_id\n        description: \"ID of the L2VPN in the inventory management system\"\n        type: int\n        required: active\n      - name: nrm_id\n        type: int\n        description: \"ID of the L2VPN in the network resource manager\"\n        required: active\n</code></pre>"},{"location":"workshops/advanced/generator/#step-3-run-the-generator-functions","title":"Step 3 - Run the generator functions","text":"<p>To help generate the correct file exec into the running container and run the generator:</p> <pre><code>docker exec -it example-orchestrator-orchestrator-1 /bin/bash\n</code></pre>"},{"location":"workshops/advanced/generator/#product-blocks","title":"Product Blocks","text":"<p>Run the command to generate the domain models product blocks:</p> <p><pre><code>python main.py generate product-blocks -cf templates/l2-p2p.yaml --no-dryrun\n</code></pre> The <code>--no-dryrun</code> option will immediately write the files to the <code>products/product_blocks</code> folder and create: <code>l2_p2p_sap.py</code> and <code>l2_p2p_virtual_circuit.py</code>. This file contains the product block configuration for the l2-p2p product and defines the strict hierarchy of virtual circuit and saps.</p>"},{"location":"workshops/advanced/generator/#product","title":"Product","text":"<p>Now create the product.</p> <pre><code>python main.py generate product -cf templates/l2-p2p.yaml --no-dryrun\n</code></pre> <p>This will create the file <code>products/product_types/l2_p2p.py</code>. When looking at this file you can see it created the domain model, fixed inputs and imported the correct product blocks to be used in this subscription.</p>"},{"location":"workshops/advanced/generator/#workflows","title":"Workflows","text":"<p>Now generate the workflows. This command will always create 4 sets of workflows <code>create</code>, <code>modify</code>, <code>terminate</code> and <code>validate</code>. These can be implemeted as the users sees fit.</p> <p>Run the command: <pre><code>python main.py generate workflows -cf templates/l2-p2p.yaml --no-dryrun --force\n</code></pre></p> <p>As you can see this file needs to be run with the --force flag as it needs to overwrite a number of configuration files. Furthermore it will populate the files in <code>workflows.l2_p2p</code>. Feel free to take a look and see what it already has done.</p>"},{"location":"workshops/advanced/generator/#database-migrations","title":"Database migrations","text":"<p>As a final step the user must generate and run the migrations to wire up the database. This is done as follows.</p> <pre><code>python main.py generate migration -cf templates/l2-p2p.yaml\npython main.py db upgrade heads\n</code></pre>"},{"location":"workshops/advanced/generator/#step-4-profit","title":"Step 4 -  Profit","text":"<p>If this has been executed without errors, you should be able to create a new subscription for the l2-p2p product by running the create workflow through the UI. All it does is create the domain model and fill it in with some rudimentary values from the input form, but it's a starting point. Users can now go into the workflow source code and start implementing steps to provision the resource that is being created by the create workflow. Take some time in the orchestrator UI to see what has been configured.</p> <ul> <li>Metadata pages</li> <li>Action menu</li> <li>Available workflows</li> </ul>"},{"location":"workshops/advanced/generator/#step-5-bonus","title":"Step 5 - Bonus","text":"<p>Implement a new step in the create workflow that manipulates the subscription in a certain way. An example could be to change the subscription description. Or any other value you can think of that exists in the subscription</p> Answer <pre><code>@step(\"Update Subscription Description\")\ndef update(subscription: L2p2pProvisioning) -&gt; State:\n    subscription.descrtiption = \"My Awesome L2P2P\"\n    return state | {\"subscription\": subscription}\n</code></pre>"},{"location":"workshops/advanced/node-create/","title":"Create Workflow","text":"<p>A create workflow needs an initial input form generator and defines the steps to create a subscription on a product. The <code>@create_workflow</code> decorator adds some additional steps to the workflow that are always part of a create workflow. The steps of a create workflow in general follow the same pattern, as described below using the create node workflow as an example.</p> <pre><code>@create_workflow(\"Create node\", initial_input_form=initial_input_form_generator)\ndef create_node() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; construct_node_model\n        &gt;&gt; store_process_subscription(Target.CREATE)\n        &gt;&gt; create_node_in_ims\n        &gt;&gt; reserve_loopback_addresses\n        &gt;&gt; provision_node_in_nrm\n    )\n</code></pre> <ol> <li>Collect input from user (<code>initial_input_form</code>)</li> <li>Instantiate subscription (<code>construct_node_model</code>):</li> <li>Create inactive subscription model</li> <li>assign user input to subscription</li> <li>transition to subscription to provisioning</li> <li>Register create process for this subscription (<code>store_process_subscription</code>)</li> <li>Interact with OSS and/or BSS, in this example</li> <li>Administer subscription in IMS (<code>create_node_in ims</code>)</li> <li>Reserve IP addresses in IPAM (<code>reserve_loopback_addresses</code>)</li> <li>Provision subscription in the network (<code>provision_node_in_nrm</code>)</li> <li>Transition subscription to active and \u2018in sync\u2019 (<code>@create_workflow</code>)</li> </ol> <p>As long as every step remains as idempotent as possible, the work can be divided over fewer or more steps as desired.</p>"},{"location":"workshops/advanced/node-create/#input-form","title":"Input Form","text":"<p>The input form is created by subclassing the <code>FormPage</code> and add the input fields together with the type and indication if they are optional or not. Additional form settings can be changed via the Config class, like for example the title of the form page.</p> <pre><code>class CreateNodeForm(FormPage):\n    model_config = ConfigDict(title=product_name)\n\n    role_id: NodeRoleChoice\n    node_name: str\n    node_description: str | None = None\n</code></pre> <p>By default, Pydantic validates the input against the specified type and will signal incorrect input and/or missing but required input fields. Type annotations can be used to describe additional constraints, for example a check on the validity of the entered VLAN ID can be specified as shown below, the type <code>Vlan</code> can then be used instead of <code>int</code>.</p> <pre><code>Vlan = Annotated[int, Ge(2), Le(4094), doc(\"Allowed VLAN ID range.\")]\n</code></pre> <p>The node role is defined as type Choice and will be rendered as a dropdown that is filled with a mapping between the role IDs and names as defined in NetBox.</p> <pre><code>def node_role_selector() -&gt; Choice:\n    roles = {str(role.id): role.name for role in netbox.get_device_roles()}\n    return Choice(\"RolesEnum\", zip(roles.keys(), roles.items()))\n\nNodeRoleChoice: TypeAlias = cast(type[Choice], node_role_selector())\n</code></pre> <p>When more than one item needs to be selected, a <code>choice_list</code> can be used to specify the constraints, for example to select two ports for a point-to-point service:</p> <pre><code>def ports_selector(number_of_ports: int) -&gt; type[list[Choice]]:\n    subscriptions = subscriptions_by_product_type(\"Port\", [SubscriptionLifecycle.ACTIVE])\n    ports = {str(subscription.subscription_id): subscription.description for subscription in subscriptions)}\n    return choice_list(\n        Choice(\"PortsEnum\", zip(ports.keys(), ports.items())),\n        min_items=number_of_ports,\n        max_items=number_of_ports,\n        unique_items=True,\n    )\n\nPortsChoiceList: TypeAlias = cast(type[Choice], ports_selector(2))\n</code></pre>"},{"location":"workshops/advanced/node-create/#extra-validation-between-dependant-fields","title":"Extra Validation between dependant fields","text":"<p>Validations between multiple fields is also possible by making use of the Pydantic <code>@model_validator</code> decorator that gives access to all fields. To check if the A and B side of a point-to-point service are not on the same network node one could use:</p> <pre><code>@model_validator(mode=\"after\")\ndef separate_nodes(self) -&gt; \"SelectNodes\":\n    if self.node_subscription_id_b == self.node_subscription_id_a:\n        raise ValueError(\"node B cannot be the same as node A\")\n    return self\n</code></pre> <p>For more information on validation, see the Pydantic Validators documentation</p> <p>Finally, a summary form is shown with the user supplied values. When a value appears to be incorrect, the user can go back to the previous form to correct the mistake, otherwise, when the form is submitted, the workflow is kicked off.</p> <pre><code>summary_fields = [\"role_id\", \"node_name\", \"node_description\"]\nyield from create_summary_form(user_input_dict, product_name, summary_fields)\n</code></pre>"},{"location":"workshops/advanced/node-modify/","title":"Modify Workflow","text":"<p>A modify workflow also follows a general pattern, like described below. The <code>@modify_workflow</code> decorator adds some additional steps to the workflow that are always needed.</p> <pre><code>@modify_workflow(\"Modify node\", initial_input_form=initial_input_form_generator)\ndef modify_node() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; set_status(SubscriptionLifecycle.PROVISIONING)\n        &gt;&gt; update_subscription\n        &gt;&gt; update_node_in_ims\n        &gt;&gt; update_node_in_nrm\n        &gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n    )\n</code></pre> <ol> <li>Collect input from user (<code>initial_input_form</code>)</li> <li>Necessary subscription administration (<code>@modify_workflow</code>):</li> <li>Register modify process for this subscription</li> <li>Set subscription \u2018out of sync\u2019 to prevent the start of other processes</li> <li>Transition subscription to Provisioning (<code>set_status</code>)</li> <li>Update subscription with the user input</li> <li>Interact with OSS and/or BSS, in this example</li> <li>Update subscription in IMS (<code>update_node_in ims</code>)</li> <li>Update subscription in NRM (<code>update_node_in nrm</code>)</li> <li>Transition subscription to active (<code>set_status</code>)</li> <li>Set subscription \u2018in sync\u2019 (<code>@modify_workflow</code>)</li> </ol> <p>Like a create workflow, the modify workflow also uses an initial input form but this time to only collect the values from the user that need to be changed. Usually, only a subset of the values may be changed. To assist the user, additional values can be shown in the input form using <code>ReadOnlyField</code>. In the example below, the name of the node is shown but cannot be changed, the node status can be changed and the dropdown is set to the current node status, and the node description is still optional.</p> <pre><code>class ModifyNodeForm(FormPage):\n    node_name: ReadOnlyField(port.node.node_name)\n    node_status: NodeStatusChoice = node.node_status\n    node_description: str | None = node.node_description\n</code></pre> <p>After a summary form has been shown that lists the current and the new values, the modify workflow is started.</p> <pre><code>summary_fields = [\"node_status\", \"node_name\", \"node_description\"]\nyield from modify_summary_form(user_input_dict, subscription.node, summary_fields)\n</code></pre>"},{"location":"workshops/advanced/node-terminate/","title":"Terminate Workflow","text":"<p>At the end of the subscription lifecycle, the terminate workflow updates all OSS and BSS accordingly, and the <code>@terminate_workflow</code> decorator takes care of most of the necessary subscription administration.</p> <pre><code>@terminate_workflow(\"Terminate node\",\ninitial_input_form=initial_input_form_generator)\ndef terminate_node() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; load_initial_state\n        &gt;&gt; delete_node_from_ims\n        &gt;&gt; deprovision_node_in_nrm\n    )\n</code></pre> <ol> <li>Show subscription details and ask user to confirm termination (<code>initial_input_form</code>)</li> <li>Necessary subscription administration (<code>@terminate_workflow</code>):</li> <li>Register terminate process for this subscription</li> <li>Set subscription \u2018out of sync\u2019 to prevent the start of other processes</li> <li>Get subscription and add information for following steps to the State (<code>load_initial_state</code>)</li> <li>Interact with OSS and/or BSS, in this example</li> <li>Delete node in IMS (<code>delete_node_in ims</code>)</li> <li>Deprovision node in NRM (<code>deprovision_node_in_nrm</code>)</li> <li>Necessary subscription administration (<code>@terminate_workflow</code>)</li> <li>Transition subscription to terminated</li> <li>Set subscription \u2018in sync\u2019</li> </ol> <p>The initial input form for the terminate workflow is very simple, it only has to show the details of the subscription:</p> <pre><code>class TerminateForm(FormPage):\n    subscription_id: DisplaySubscription = subscription_id\n</code></pre>"},{"location":"workshops/advanced/node-validate/","title":"Validate Workflow","text":"<p>And finally, the validate workflow, used to check if the information in all OSS and BSS is still the same with the information in the subscription. One way to do this is to reconstruct the payload sent to the external system using information queried from that system, and compare this with the payload that would have been sent by generating a payload based on the current state of the subscription. The <code>@validate_workflow</code> decorator takes care of necessary subscription administration. There is no initial input form for this type of workflow.</p> <pre><code>@validate_workflow(\"Validate l2vpn\")\ndef validate_l2vpn() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; validate_l2vpn_in_ims\n        &gt;&gt; validate_l2vpn_terminations_in_ims\n        &gt;&gt; validate_vlans_on_ports_in_ims\n   )\n</code></pre> <ol> <li>Necessary subscription administration (<code>@validate_workflow</code>):</li> <li>Register validate process for this subscription</li> <li>Set subscription \u2018out of sync\u2019, even when subscription is already out of sync</li> <li>One or more steps to validate the subscription against all OSS and BSS:</li> <li>Validate subscription against IMS:<ol> <li><code>validate_l2vpn_in_ims</code></li> <li><code>validate_l2vpn_terminations_in_ims</code></li> <li><code>validate_vlans_on_ports_in_ims</code></li> </ol> </li> <li>Set subscription \u2018in sync\u2019 again (<code>@validate_workflow</code>)</li> </ol> <p>When one of the validation steps fail, the subscription will stay \u2018out of sync\u2019, prohibiting other workflows to be started for this subscription. The failed validation step can be retried as many times as needed until it succeeds, which finally will set the subscription \u2018in sync\u2019 and allow other workflows to be started again. This safeguards workflows to be started for subscription with mismatching information in OSS and BSS which would make these workflows likely to fail.</p> <p>It is better to limit the number of validations done in each step. This will make it easier to see in a glance what discrepancy was found and will make a retry of the failed step much faster. A commonly used strategy is to use separate steps for each OSS and BSS, and separate steps per external system for each payload that was sent. This can be done by comparing a payload created for a product block in the orchestrator with a payload that is generated by querying the external system.</p> <p>Not only validations per subscription can be done, is also possible to validate other requirements. For example, to make sure that there are no L2VPNs administered in IMS that do not have a matching subscription in the orchestrator, a task (a workflow with <code>Target.SYSTEM</code>) can be written that will retrieve a list of all L2VPNs from IMS and compare it against a list of all L2VPN subscription from the orchestrator.</p>"},{"location":"workshops/advanced/overview/","title":"Example Orchestrator Workshop Overview","text":""},{"location":"workshops/advanced/overview/#intended-audience","title":"Intended audience","text":"<p>This workshop is intended for those who are interested in using the Workflow Orchestrator as network orchestrator, but is also accessible to those who are new to the Workflow Orchestrator and would like to use it as a generic orchestrator. The main goal of this workshop is to introduce you to how to write orchestrator workflows that talk to external systems, as well as teaching you how to relate products to other products, using the dependency model of the Workflow Orchestrator.</p> <p>Tip</p> <p>Knowledge of the Python programming language, Docker, and the Unix command line interface are prerequisites for this workshop.</p>"},{"location":"workshops/advanced/overview/#topics","title":"Topics","text":"<ul> <li>Installation   Detailed instructions are given on how to prepare your environment and install the orchestrator and GUI using docker compose.</li> <li>Start applications   Outline how to start the Workflow Orchestrator backend and GUI using docker compose.</li> <li>Bootstrapping the applications and familiarisation   Through a simple network node and network circuit scenario, a set of products is created showing how domain models are defined.<ul> <li>Domain models Explains the benefits of the use of domain models and shows how the hierarchy of products, product blocks, fixed inputs and resource types are used to create product subscriptions for customers.</li> </ul> </li> <li>L2 Point-to-Point product modelling and workflow   For the L2 Point-to-Point product, we will make the CREATE workflow by using the product generator. The use of input   forms is explained as part of defining the create workflow. By using this method you should be able to quickly get   up to speed and start coding quickly</li> </ul>"},{"location":"workshops/advanced/overview/#workshop-folder-layout","title":"Workshop folder layout","text":"<p>This workshop uses the following folder layout:</p> <pre><code>\u251c\u2500\u2500 migrations\n\u2502   \u2514\u2500\u2500 versions\n\u2502   \u2514\u2500\u2500 schema\n\u251c\u2500\u2500 products\n\u2502   \u251c\u2500\u2500 product_blocks\n\u2502   \u251c\u2500\u2500 product_types\n\u2502   \u2514\u2500\u2500 services\n\u2502   \u2514\u2500\u2500 &lt;service&gt;\n\u251c\u2500\u2500 services\n\u2502 \u2514\u2500\u2500 &lt;service&gt;\n\u251c\u2500\u2500 templates\n\u251c\u2500\u2500 translations\n\u251c\u2500\u2500 utils\n\u2514\u2500\u2500 workflows\n\u251c\u2500\u2500 &lt;product&gt;\n\u2514\u2500\u2500 tasks\n37 directories, 99 files\n</code></pre>"},{"location":"workshops/advanced/overview/#workshop-software-architecture","title":"Workshop software architecture","text":"<p>The workshop combines a number of opensource software components that can provision a simulated network running in containerlab. The following diagram shows the logical components of the application and how the data flows. In reality there are a number of extra services like Postgres and Redis that store the application data of the Orchestrator, Netbox and LSO.</p>"},{"location":"workshops/advanced/overview/#software-used-in-the-workshop","title":"Software used in the workshop","text":"<ul> <li>The orchestrator: This includes the UI and python backend that will run all workflows. All data is persisted in a Postgres database. Redis is used for caching and syncronisation purposes.</li> <li>Netbox: Netbox is the source of truth for this network topology. It contains all resources that are known   in the topology: Interfaces, Nodes, IP addresses etc. The Orchestrator will configure Netbox but also retrieve resource from it.</li> <li>LSO: The Network Resource Manager (NRM) of this topology. This software is an API abstraction on top of   ansible that integrates well with the orchestrator. It is responsible for running ansible jobs to provision the topology.</li> <li>Container Lab: This software will manage the (virtual) network topology running the Network Operating System   of the workshop. Below the network topology is explained.</li> </ul>"},{"location":"workshops/advanced/overview/#workshop-topology","title":"Workshop topology","text":"<p>Assuming you have installed the example orchestartor with containerlab integration enabled. We need to build the workshop topology that can be used to actually see packets flow. The workflows that you will run in the following steps will do the following:</p> <ul> <li>Seed Netbox</li> <li>Provision two PE nodes</li> <li>Create an IS-IS cloud to signal MPLS LSP's with backbone links</li> <li>Provision customer Ports that can be used in network services</li> </ul> <p>The topology will be as follows:</p> <p></p> <p>Host 1 and Host 2 will be pre-provisioned, but the routers will need to be bootstrapped from scratch to work. This process will be explained step by step in the last section of this workshop.</p>"},{"location":"workshops/advanced/scenario/","title":"Scenario","text":"<p>During this workshop a set of products will be used together with the needed workflows to manage enrolling network nodes into the Workflow Orchestrator and creating circuits between nodes. The products will be just complex enough to show the basic capabilities of products, product blocks, fixed inputs, resource types and workflows in the workflow orchestrator. We will cover nesting product blocks and products together.</p>"},{"location":"workshops/advanced/scenario/#product-hiearchy-example","title":"Product hiearchy example","text":"<p>In the diagram below you can see how all products and product blocks relate to each other. The example orchestrator has implemented the following example products and corresponding workflows that can be used to build a basic network topology and customer facing services:</p> <p>In the <code>product.product_types</code> module the following products are defined:</p> <ul> <li>Node</li> <li>CoreLink</li> <li>Port</li> <li>L2vpn</li> </ul> <p>And in the <code>product.product_blocks</code> module the following product blocks are defined:</p> <ul> <li>NodeBlock</li> <li>CoreLinkBlock</li> <li>CorePortBlock</li> <li>PortBlock</li> <li>SAPBlock</li> <li>VirtualCircuitBlock</li> </ul> <p>Usually, the top-level product block if a product is named after the product, but this is not true for the top-level product block of the L2VPN product. The more generic name <code>VirtualCIrcuitBlock</code> allows the reuse of this product block by other services like Internet Access and L3VPN.</p> <p>The Service Access Point (SAP) product block <code>SAPBlock</code> is used to encapsulate transport specific service endpoint information, in our case Ethernet 802.1Q is used and the SAP holds the VLAN used on the indicated port.</p> <p>When this example orchestrator is deployed, it can create a growing graph of product blocks as is shown below.</p> <p></p> <p>Hint</p> <p>Take some time to explore the module described in above. It shows how the product modelling is done in Python. Once you are familiar with the code. Continue with the workshop</p>"},{"location":"workshops/advanced/workflow-basics/","title":"Workflow Basics","text":"<p>Workflows are used to orchestrate the lifecycle of a Product Subscription and process the user or systems intent and apply that to the service. As mentioned above a Subscription is created, then modified <code>N</code> number of times, after which it is terminated. During it's life a Subscription may also be validated on a regular basis to check whether there is any drift between the state captured in the Orchestrator and actual state on the system. This workflow is slightly different compared to the workflows that process intent and apply that to a system, as it does not modify the system.</p> <p>Four types of workflows are defined, three lifecycle related ones to create, modify and terminate subscriptions, and a fourth one to validate subscriptions against the OSS and BSS. The decorators <code>@create_workflow</code>, <code>@modify_workflow</code>, <code>@terminate_workflow</code>, and <code>@validate_workflow</code> are used to define the different types of workflow, and the <code>@step</code> decorator is used to define workflow steps that can be used in any type of workflow.</p>"},{"location":"workshops/advanced/workflow-basics/#workflow-architecture-passing-information-from-one-step-to-the-next","title":"Workflow Architecture - Passing information from one step to the next","text":"<p>Information between workflow steps is passed using <code>State</code>, which is nothing more than a collection of key/value pairs, in Python represented by a <code>Dict</code>, with string keys and arbitrary values. Between steps the <code>State</code> is serialized to JSON and stored in the database. The step decorator is used to turn a function into a workflow step, all arguments to the step function will automatically be initialised with the value from the matching key in the <code>State</code>. In turn the step function will return a <code>Dict</code> of new and/or modified key/value pairs that will be merged into the <code>State</code> to be consumed by the next step. The serialization and deserialization between JSON and the indicated Python types is done automatically. That is why it is important to correctly type the step function parameters.</p>"},{"location":"workshops/advanced/workflow-basics/#example","title":"Example","text":"<p>Given this function, when a user correctly makes use of the step decorator it is very easy to extract variables and make a calculation. It creates readable code, that is easy to understand and reason about. Furthermore the variables become available in the step in their correct type according to the domain model. Logic errors due wrong type interpretation are much less prone to happen.</p> <p>Bad use of the step decorator</p> <pre><code>@step(\"A Bad example of using input params\")\ndef my_ugly_step(state: State) -&gt; State:\n    variable_1 = int(state[\"variable_1\"])\n    variable_2 = str(state[\"variable_2\"])\n    subscription = SubscriptionModel.from_subscription_id(state[\"subscription_id\"])\n\n    if variable_1 &gt; 42:\n        subscription.product_block_model.variable_1 = -1\n        subscription.product_block_model.variable_2 = \"Infinity\"\n    else:\n        subscription.product_block_model.variable_1 = variable_1\n        subscription.product_block_model.variable_2 = variable_2\n\n    state[\"subscription\"] = subscription\n    return state\n</code></pre> <p>In the above example you see we do a simple calculation based on <code>variable_1</code>. When computing with even more variables, you van imagine how unreadable the function will be. Now consider the next example.</p> <p>Good use of the step decorator</p> <pre><code>@step(\"Good use of the input params functionality\")\ndef my_beautiful_step(variable_1: int, variable_2: str, subscription: SubscriptionModel) -&gt; State:\n    if variable_1 &gt; 42:\n        subscription.product_block_model.variable_1 = -1\n        subscription.product_block_model.variable_2 = \"Infinity\"\n    else:\n        subscription.product_block_model.variable_1 = variable_1\n        subscription.product_block_model.variable_2 = variable_2\n\n    return state | {\"subscription\": subscription}\n</code></pre> <p>As you can see the Orchestrator the orchestrator helps you a lot to condense the logic in your function. The <code>@step</code> decorator does the following:</p> <ul> <li>Loads the previous steps state from the database.</li> <li>Inspects the step functions signature</li> <li>Finds the arguments in the state and injects them as function arguments to the step function</li> <li>It casts them to the correct type by using the type hints of the step function.</li> <li>Finally it updates the state of the workflow and persists all model changes to the database upon reaching the   <code>return</code> of the step function.</li> </ul>"},{"location":"workshops/advanced/workflow-basics/#forms","title":"Forms","text":"<p>The input form is where a user can enter the details for a subscription on a certain product at the start of the workflow, or can enter additional information during the workflow. The input forms are dynamically generated in the backend and use Pydantic to define the type of the input fields. This also allows for the definition of input validations. Input forms are (optionally) used by all types of workflows to gather and validate user input. It is possible to have more than one input form, with the ability to navigate back and forth between the forms, until the last input form is submitted, and the first (or next) step of the workflow is started. This allows for on-the-fly generation of input forms, where the content of the following form(s) depend on the input of the previous form(s). For example, when creating a core link between two nodes, a first input form could ask to choose two nodes from a list of active nodes, and the second form will present two lists with ports on these two nodes to choose from.</p> Best Practices for writing workflows While developing a new product, the workflows can be written in any order. For those that use a test-driven development style probably will start with the validate workflow. But in general people start with the create workflow as it helps to discuss the product model (the information involved) and the workflows (the procedures involved) with the stakeholders to get the requirements clear. Once the minimal viable create workflow is implemented, the validate workflow can be written to ensure that all information is administered correctly in all touched OSS and BSS and is not changed again by hand because human workflows were not correctly adapted yet. Then after the terminate workflow is written, the complete lifecycle of the product can be tested. Even when the modify is not implemented, a change to a subscription can be carried out by terminating the subscription and creating it again with the modified input. Finally, the modify workflow is implemented to allow changes to a subscription with minimal or no impact to the customer."},{"location":"workshops/advanced/workflow-basics/#form-magic","title":"Form Magic","text":"<p>As mentioned before, forms are dynamically created from the backend. This means, little to no frontend coding is needed to make complex wizard like input forms available to the user. When selecting an action in the UI. The first thing the frontend does is make an api call to load a form from the backend. The resulting <code>JSONschema</code> is parsed and the correct widgets are loaded in the frontend. Upon submit this is posted to the backend that does all validation and signals to the user if there are any errors. The following forms are supported:</p> <ul> <li>Multiselect</li> <li>Drop-down</li> <li>Text field (restricted)</li> <li>Number (float and dec)</li> <li>Radio</li> </ul>"},{"location":"workshops/advanced/workflow-introduction/","title":"Introduction","text":"<p>This document assumes you are already familiar with the key safeguards and potential pitfalls outlined in the Architecture of workflows</p>"},{"location":"workshops/advanced/workflow-introduction/#continuing-the-workshop","title":"Continuing the Workshop","text":"<p>The next sections introduce the workflow concept and its relationship to the product model. Code examples referenced throughout can be found in the example orchestrator under the <code>.workflows.node</code> directory.</p> <p>Please read at least the following pages to grasp the functionality of how workflows work and how the user/frontend will interact with the Orchestrator API:</p> <ul> <li>Workflow Basics</li> <li>Create Workflow</li> </ul>"},{"location":"workshops/beginner/create-user-group/","title":"Create UserGroup workflow","text":""},{"location":"workshops/beginner/create-user-group/#exercise-1-create-usergroup-workflow","title":"Exercise 1: create UserGroup workflow","text":"<p>The create workflow will produce a subscription for a specific customer on the UserGroup product. This is done by executing the following workflow steps:</p> <pre><code>init\n&gt;&gt; create_subscription\n&gt;&gt; store_process_subscription()\n&gt;&gt; initialize_subscription\n&gt;&gt; provision_user_group\n&gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>The builtin steps <code>init</code> and <code>done</code> are always part of a workflow and mark the begin and end of the workflow. Three other builtin steps are being used here that are almost always part of a create workflow:</p> <ul> <li> <p>store_process_subscription</p> <p>The orchestrator executes a workflow in a process, and keeps track of all workflows that have been run to create, modify or terminate a subscription, so there is an administrative trail of what happened to each subscription and when.  This step is used to enter this information into the database. The argument <code>Target.CREATE</code> indicates that this is a create workflow. The reason that this step is not the first step directly after <code>init</code> is because we need to know the subscription ID that is not yet known at that point in the workflow.</p> </li> <li> <p>set_status</p> <p>At the end of the workflow, after the subscription has been created, and the interaction with all OSS and BSS was successfully finished, the subscription state is set to <code>SubscriptionLifecycle.ACTIVE</code>. From this moment on the modify and terminate workflows can be started on this subscription.</p> </li> <li> <p>resync</p> <p>Every subscription has a notion of being in sync or not with all OSS and BSS. There are several moments that a subscription is deemed to be out of sync, one is during the time a workflow is active for a subscription, another is when a validation workflow (will be explained in the advanced workshop) has detected a discrepancy between the subscription and any of the OSS or BSS. One important side effect of a subscription being out of sync is that no new workflows can be started for that subscription, this is to protect for actions being taken on information that is possibly not accurate. The <code>resync</code> step sets the subscription in sync.</p> </li> </ul> <p>The three remaining steps are custom to this workflow:</p> <ul> <li> <p>create_subscription</p> <p>This step will create a new virgin subscription on a product. Every product has the standard method <code>from_product_id()</code> that takes two mandatory arguments: <code>product_id</code> and <code>customer_id</code>. Use this method on the <code>UserGroupInactive</code> product to create a subscription in state <code>SubscriptionLifecycle.INITIAL</code>. Because there is no CRM used during this beginner workshop the customer UUID can be faked.</p> <p>Make sure that this step returns a <code>Dict</code> with at least the keys <code>'subscription'</code> and <code>'subscription_id'</code> to make the orchestrator merge these keys into the workflow <code>State</code>. When leaving a step the orchestrator will also automatically commit the <code>'subscription'</code> to the database. The <code>'subscription_id'</code> key is needed by the <code>store_process_subscription</code> step.</p> </li> <li> <p>initialize_subscription</p> <p>This step will initialize the resource types based on the input from the user. In this case only the name of the group needs to be assigned. Also set the subscription description to something meaningful at this stage. After this, the subscription can be transitioned to <code>SubscriptionLifecycle.PROVISIONING</code>, this will trigger checks to make sure that all mandatory resource types are present for that lifecycle state. Every product has the standard method <code>from_other_lifecycle()</code> to accomplish this, this method takes the original subscription and the targe lifecycle state as arguments.</p> <p>Make sure that this step returns a <code>Dict</code> with also at least the key <code>'subscription'</code> to merge the modified subscription into the workflow state and have the orchestrator commit the subscription changes to the database.</p> </li> <li> <p>provision_user_group</p> <p>Now the user group can be provisioned in all OSS and BSS as necessary.  As there is no actual user group provisioning system during this workshop, this interaction is being faked. The returned (fake) group ID is assigned to the intended resource type.</p> <p>Yet again make sure that this step returns a <code>Dict</code> with at least the key <code>'subscription'</code>.</p> </li> </ul> <p>The only thing left that is needed is an initial input form generator function with one string input field to ask the user for the name of the user group.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/create_user_group.py</code>:</p> <pre><code>from uuid import uuid4\n\nfrom orchestrator.forms import FormPage\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription\nfrom orchestrator.workflows.utils import wrap_create_initial_input_form\n\nfrom products.product_types.user_group import UserGroupInactive, UserGroupProvisioning\n\n# initial input form generator\n...\n\n# create subscription step\n...\n\n# initialize subscription step\n...\n\n# provision user group step\n...\n\n# create user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group create workflow </p>"},{"location":"workshops/beginner/create-user/","title":"Create User workflow","text":""},{"location":"workshops/beginner/create-user/#exercise-1-create-user-workflow","title":"Exercise 1: create User workflow","text":"<p>The create <code>User</code> workflow is very similar to the create <code>UserGroup</code> workflow, the major difference is the increased number of user inputs needed to initialize the subscription. This workflow uses the following steps:</p> <pre><code>init\n&gt;&gt; create_subscription\n&gt;&gt; store_process_subscription()\n&gt;&gt; initialize_subscription\n&gt;&gt; provision_user\n&gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>There is one important difference though, one of the user inputs on the input form is special: the selection of the user group the user belongs to. It is not just an integer or a string, but the user must be able to select a user group out of a list of already provisioned user groups. For this the database will be queried to obtain a list of active user group subscriptions, and a special input field type is used to display a dropdown input field on the input form.</p> <p>In the orchestrator, all access to the database is implemented using SQLAlchemy, and queries can be formulated using the classes from <code>orchestrator.db.models</code> that map to the tables in the database. The following query is all that is needed to get a list of <code>active</code> <code>UserGroup</code> subscriptions:</p> <pre><code>from orchestrator.db import db\nfrom orchestrator.db.models import ProductTable, SubscriptionTable\nfrom sqlalchemy import select\n\n...\n\nstmt = (\n    select(SubscriptionTable)\n    .join(ProductTable)\n    .filter(ProductTable.product_type == \"UserGroup\", SubscriptionTable.status == \"active\")\n    .with_only_columns(SubscriptionTable.subscription_id, SubscriptionTable.description)\n)\n\nsubscriptions = db.session.scalars(stmt)\n\n...\n</code></pre> <p>The <code>orchestrator.forms.validators</code> package provides a standard input component called <code>choice_list</code> that will create the indicated enumeration and expects an iterator that returns tuples containing a label and a value. The iterator is created making use of the standard Python <code>zip</code> function. This input component will show a dropdown with all labels and returns a list of associated chosen keys.  The amount of entries that may be chosen is controlled by the <code>min_items</code> and <code>max_items</code> arguments.</p> <p>Putting everything together, the user group selector looks like this:</p> <pre><code>from orchestrator.db import db\nfrom orchestrator.db.models import ProductTable, SubscriptionTable\nfrom sqlalchemy import select\n\ndef user_group_selector() -&gt; list:\n    user_group_subscriptions = {}\n    stmt = (\n        select(SubscriptionTable).join(ProductTable)\n        .filter(ProductTable.product_type == \"Port\", SubscriptionTable.status == \"active\")\n        .with_only_columns(SubscriptionTable.subscription_id, SubscriptionTable.description)\n    )\n\n    for user_group_id, user_group_description in db.session.execute(stmt).all():\n        user_group_subscriptions[str(user_group_id)] = user_group_description\n\n    return choice_list(\n        Choice(\"UserGroupEnum\", zip(user_group_subscriptions.keys(), user_group_subscriptions.items())),\n        min_items=1,\n        max_items=1,\n    )\n</code></pre> <p>And can now be used in the input form as follows:</p> <pre><code>user_group_ids: user_group_selector()\n</code></pre> <p>In the subscription initialization step the <code>group</code> resource type of the <code>UserBlock</code> product block is assigned with the the <code>UserGroupBlock</code> from the <code>UserGroup</code> subscription:</p> <pre><code>subscription.user.group = UserGroup.from_subscription(user_group_ids[0]).user_group\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/create_user.py</code>:</p> <pre><code>from typing import List, Optional\nfrom uuid import uuid4\n\nfrom orchestrator.db.models import ProductTable, SubscriptionTable\nfrom orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Choice, choice_list\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription\nfrom orchestrator.workflows.utils import wrap_create_initial_input_form\n\nfrom products.product_types.user import UserInactive, UserProvisioning\nfrom products.product_types.user_group import UserGroup\n\n# user group selector\n...\n\n# initial input form generator\n...\n\n# create subscription step\n...\n\n# initialize subscription step\n...\n\n# provision user step\n...\n\n# create user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user create workflow </p>"},{"location":"workshops/beginner/database-migration/","title":"Database migration","text":""},{"location":"workshops/beginner/database-migration/#introduction","title":"Introduction","text":"<p>The orchestrator uses SQLAlchemy, the Python SQL toolkit and Object Relational Mapper, as interface to the database. Alembic, which is part of SQLAlchemy, is used for the creation, management, and invocation of database change management scripts.</p> <p>Now that the product and product block domain models have been created, it is time to create an Alembic database migration to insert this information into the database. All the SQL statements needed for this migration can be written by hand, but knowledge about the database tables and how they are used is required to write correct statements. Luckily, the orchestrator comes with helper functions, located at <code>orchestrator/migrations/helpers</code>, that produce the needed SQL statements.  These helper functions make use of a set of simple data structures that describe the domain models and workflows that need to be added to the database. Recently a third option was added, the orchestrator is now able to detect differences between the database and the registered product domain models and create all needed SQL statements for you.</p> <p>Below we will make use of the ability of the orchestrator to create database migrations for us.</p>"},{"location":"workshops/beginner/database-migration/#exercise-1-add-products-to-registry","title":"Exercise 1: add products to registry","text":"<p>In order to use the products that were defined earlier, the orchestrator needs to know about their existence. This is done by adding the products with a description to the <code>SUBSCRIPTION_MODEL_REGISTRY</code>.</p> <p>The products can be added to the registry in <code>main.py</code>, but for this exercise the registry will be updated by the <code>products</code> module, this keeps the registration code close to the definition of the products and nicely separated from the rest of the code.</p> <p>Create the file <code>products/__init__.py</code> and add the following code:</p> <pre><code>from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\n\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"User Group\": UserGroup,\n        \"User internal\": User,\n        \"User external\": User,\n    }\n)\n</code></pre> <p>To make Python execute this code, add the following import statement to <code>main.py</code>:</p> <pre><code>import products\n</code></pre>"},{"location":"workshops/beginner/database-migration/#exercise-2-create-database-migration","title":"Exercise 2: create database migration","text":"<p>To manually create a database migration a Python environment is needed, as created with the manual installation steps, to run the orchestrator from the command line. When using Docker compose an example migration is being used.</p>"},{"location":"workshops/beginner/database-migration/#docker-compose","title":"Docker compose","text":"<p>Copy the example product and product block migration:</p> <pre><code>(\n  cd migrations/versions/schema\n  curl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/2022-11-11_45984f4b8010_add_user_and_usergroup_products.py\n)\n</code></pre>"},{"location":"workshops/beginner/database-migration/#manual","title":"Manual","text":"<p>The orchestrator command line interface offers the <code>db migrate-domain-models</code> command to create a database migration based on the differences between the database and the registered products. In most cases this command will be able to detect all changes, but in more complex situations it will ask the user for additional input to create the correct migration. For new products it will also ask for user-friendly descriptions for the products, product blocks, resource types and fixed inputs, as well as information that is not defined in the domain models like product and product block tags, and values for the fixed inputs to differentiate the products of the same product type.</p> <p>Create the migration with the following command, have a look at the overview below when in doubt of the correct answer to the questions, and make sure that the product type entered exactly matches the product types defined in the domain models, including upper/lowercase:</p> <pre><code>PYTHONPATH=. python main.py db migrate-domain-models \"Add User and UserGroup products\"\n</code></pre> <p>When finished have a look at the migration created in the folder <code>migrations/versions/schema</code>.</p> <p>Note</p> <p>While creating the migration, the order of the questions/answers may be different from the order in the overview below. Therefore do not blindly copy/paste the answers.</p> <p>Example</p> <p>Create new products Product: UserGroup User Group Supply the product description: user group administration Supply the product tag: GROUP Product: User User internal Supply the product description: user administration - internal Supply the product tag: USER_INT Product: User User external Supply the product description: user administration - external Supply the product tag: USER_EXT</p> <p>Create fixed inputs Supply fixed input value for product User internal and fixed input affiliation: internal Supply fixed input value for product User external and fixed input affiliation: external</p> <p>Create product blocks Product block: UserGroupBlock Supply the product block description: user group block Supply the product block tag: UGB Product block: UserBlock Supply the product block description: user block Supply the product block tag: UB</p> <p>Create resource types Supply description for new resource type group_name: name of the user group Supply description for new resource type group_id: id of the user group Supply description for new resource type username: name of the user Supply description for new resource type age: age of the user Supply description for new resource type user_id: id of the user</p>"},{"location":"workshops/beginner/database-migration/#exercise-3-perform-database-migration","title":"Exercise 3: perform database migration","text":"<p>To create a representation of the products in the database that matches the domain models, the database migration created above is executed.</p>"},{"location":"workshops/beginner/database-migration/#docker-compose_1","title":"Docker compose","text":"<p>The Docker compose environment contains a initialization container that will always upgrade the database to the latest heads. To trigger this you only have to restart the environment.</p>"},{"location":"workshops/beginner/database-migration/#manual_1","title":"Manual","text":"<p>One way to manually migrate to the latest schemas is to explicitly upgrade the database to the revision that was just created  with <code>db upgrade &lt;revision&gt;</code>. Another way is to upgrade to the latest heads again, as was done during the initialisation of the database.</p> <pre><code>PYTHONPATH=. python main.py db upgrade heads\n</code></pre> <p>Look at what the migration added to the database by either querying the database directly:</p> <pre><code>psql orchestrator-core\n</code></pre> <p>or by using the orchestrator API:</p> <pre><code>curl http://127.0.0.1:8080/api/products/ | jq\n</code></pre> <p>or by browsing through the orchestrator meta data through the GUI at:</p> <pre><code>http://localhost:3000/metadata/products\n</code></pre> <p>or all of the above.</p> <p>The metadata/products page should look as following:</p> <p></p> <p>Example</p> <p>if the database migration is incorrect, use this example Add User and UserGroup products  migration</p>"},{"location":"workshops/beginner/debian/","title":"Debian 11 (bullseye) installation instructions","text":"<p>How to manually install the orchestrator-core and orchestrator-core-gui on Debian 11 (Bullseye) is described in the following steps.</p>"},{"location":"workshops/beginner/debian/#step-1-install-dependencies","title":"Step 1 - Install dependencies","text":"<p>First make sure the debian install is up to date. Then install the following software dependencies:</p> <ul> <li>PostgreSQL (version &gt;=11)</li> <li>Git</li> <li>virtualenvwrapper</li> <li>Node.js (version 14)</li> </ul> <pre><code>sudo apt update\nsudo apt upgrade --yes\ncurl -sL https://deb.nodesource.com/setup_14.x | sudo bash -\nsudo apt-get install --yes postgresql git virtualenvwrapper nodejs\n</code></pre>"},{"location":"workshops/beginner/debian/#step-2-database-setup","title":"Step 2 - Database setup","text":"<p>In step 1 the database server is already started as part of the apt installation procedure. If the database server was previously installed and stopped then start it again. Create the database with the following commands, use <code>nwa</code> as password:</p> <pre><code>sudo -u postgres createuser -sP nwa\nsudo -u postgres createdb orchestrator-core -O nwa\n</code></pre> <p>For debug purposes, interact directly with the database by starting the PostgresSQL interactive terminal:</p> <pre><code>sudo -u postgres psql orchestrator-core\n</code></pre>"},{"location":"workshops/beginner/debian/#step-3-install-orchestrator","title":"Step 3 - Install orchestrator","text":"<p>The minimal version of Python is 3.11. Before the orchestrator core and all its dependencies are installed, a Python virtual environment is created:</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nmkvirtualenv --python python3.11 example-orchestrator\n</code></pre> <p>Make sure that the just created Python virtual environment is active before installing the orchestrator-core:</p> <pre><code>pip install orchestrator-core\n</code></pre> <p>A next time in a new shell, be sure to activate the Python virtual environment again:</p> <pre><code>source /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nworkon example-orchestrator\n</code></pre>"},{"location":"workshops/beginner/debian/#step-4-init-orchestrator","title":"Step 4 - Init orchestrator:","text":"<p>Create a <code>main.py</code> file with the following content:</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre> <p>Commit the just created main.py to git:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add main.py\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git must contain at least one commit because otherwise the <code>db init</code> below will fail.</p> <p>Initialize the database and run all the database migrations:</p> <pre><code>PYTHONPATH=. python main.py db init\nPYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/debian/#step-5-install-orchestrator-frontend","title":"Step 5 - Install orchestrator frontend","text":"<p>Install the orchestrator client in the parent directory of the example-orchestrator:</p> <pre><code>cd ..\ngit clone https://github.com/workfloworchestrator/example-orchestrator-ui.git\n</code></pre> <p>Install the npm packages <code>npm i</code></p> <p>Set the environment variables. The defaults from .env.example will work out of the box with the example orchestrator backend. <code>cp .env.example .env</code> If you are working without authentication, be sure to set <code>OAUTH2_ACTIVE=true</code>.</p>"},{"location":"workshops/beginner/debian/#step-6-init-orchestrator-client","title":"Step 6 - Init orchestrator client:","text":"<p>Use the supplied environment variable defaults:</p> <pre><code>cp .env.local.example .env.local\n</code></pre> <p>And make the following changes to <code>.env.local</code>:</p> <pre><code># change the existing REACT_APP_BACKEND_URL variable value into:\nREACT_APP_BACKEND_URL=http://your_ip_address_here:3000\n# and add the following:\nDANGEROUSLY_DISABLE_HOST_CHECK=true\n</code></pre> <p>The <code>custom-example</code> folder contains some SURF specific modules that can be used as an example. It must be linked to the folder <code>custom</code> in order for the app to start:</p> <pre><code>(cd src &amp;&amp; ln -s custom-example custom)\n</code></pre>"},{"location":"workshops/beginner/docker/","title":"Docker compose installation instructions","text":"<p>How to run the orchestrator-core and orchestrator-core-gui with Docker Compose is described in the steps below. The following Docker images are used:</p> <ul> <li>orchestrator-core:   The workflow orchestrator step engine.</li> <li>orchestrator-core-gui:   The GUI for the orchestrator-core.</li> <li>postgres:   The PostgreSQL object-relational database system.</li> <li>busybox:   The swiss army knife of embedded linux.</li> </ul>"},{"location":"workshops/beginner/docker/#step-1-prepare-environment","title":"Step 1 - Prepare environment","text":"<p>First create the folder to hold the example orchestrator that is being build during this workshop. Then copy the <code>docker-compose.yml</code> and <code>orchestrator-core-gui.env</code> to control and configure the environment, and copy a <code>main.py</code> that contains a rudimentary orchestrator application.</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/docker-compose.yml\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/orchestrator-core-gui.env\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/main.py\n</code></pre> <p>Commit the copied files to a local git repository:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git repository must contain at least one commit because otherwise the database initializations step below will fail.</p>"},{"location":"workshops/beginner/docker/#step-2-start-environment","title":"Step 2 - Start environment","text":"<p>Docker compose will take care of all necessary initialization and startup of the database, orchestrator and GUI:</p> <ol> <li>the busybox container creates the folder <code>db_data</code>, if not yet present</li> <li>then a postgres container creates a database, if it does not exist    already, after which the database server is started</li> <li>an orchestrator container is used to initialize the database, if    not already initialized, and creates an <code>alembic.ini</code> file and a    <code>migrations</code> folder for the database migrations</li> <li>a second run of the orchestrator container will upgrade the database to    the latest heads, and will do so everytime the environment is started</li> <li>then a third run of the orchestrator container will use <code>main.py</code> to    run the orchestrator</li> <li>finally, the GUI is started in the orchestrator-core-gui container</li> </ol> <pre><code>docker compose up\n</code></pre>"},{"location":"workshops/beginner/docker/#step-3-open-a-browser","title":"Step 3 - Open a browser","text":"<p>Now point a browser to:</p> <p><pre><code>http://localhost:3000/\n</code></pre> and have a look around.</p> <p>Note</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive, this workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/beginner/domain-models/","title":"Domain models","text":""},{"location":"workshops/beginner/domain-models/#introduction","title":"Introduction","text":"<p>First read the Architecture TL;DR section of the orchestrator core documentation to get an overview of the concepts that will be covered.</p> <p>To put a part of the terminology in context, products are modeled using a set of product blocks. The product attributes are modeled by resource types.  By default all resource types are mutable and can be changed over the lifetime of a subscription. Fixed inputs are used to model immutable attributes.</p> <p>An example of an immutable attribute is for example the speed of a network interface, which is a physical property of the interface, and cannot be changed without a field engineer swapping the interface with one with a different speed. Another example is an attribute that is linked to the price of a product, for example the greater the capacity of a product, the higher the price. A customer is not allowed to increase the capacity himself, he has to pay extra first.</p> <p>The products and product blocks for this workshop will be modeled as follows:</p> <ul> <li>product UserGroup<ul> <li>product block reference user_group (UserGroupBlock)</li> </ul> </li> <li>product User<ul> <li>fixed input affiliation</li> <li>product block reference user (UserBlock)</li> </ul> </li> <li>product block UserGroupBlock<ul> <li>resource type group_name</li> <li>resource type group_id</li> </ul> </li> <li>product block UserBlock<ul> <li>resource type username</li> <li>resource type age</li> <li>resource type user_id</li> <li>product block reference group (UserGroupBlock)</li> </ul> </li> </ul> <p>A product can be seen as a container for fixed inputs and (at least one) references to a product block, and a product block as a container for resources types and (optional) references to other product blocks. Product block references may be nested as deep as needed.</p>"},{"location":"workshops/beginner/domain-models/#exercise-1-create-usergroup-product-block","title":"Exercise 1: create UserGroup product block","text":"<p>Read the Domain models section of the orchestrator core documentation to learn more about domain models and how they are defined. For now, skip the code examples Product Model a.k.a SubscriptionModel and Advanced Use Cases.</p> <p>Use the following skeleton to create the file <code>user_group.py</code> in the <code>products/product_blocks</code> folder and define the <code>UserGroupBlockInactive</code>, <code>UserGroupBlockProvisioning</code> and <code>UserGroupBlock</code> domain models describing the user group product block in the lifecycle states <code>INITIAL</code>, <code>PROVISIONING</code> and <code>ACTIVE</code>:</p> <pre><code>from typing import Optional\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\n# UserGroupBlockInactive with all resource types optional\n...\n\n# UserGroupBlockProvisioning with only resource type group_id optional\n...\n\n# UserGroupBlock with all resource types mandatory\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user group product block </p>"},{"location":"workshops/beginner/domain-models/#exercise-2-create-usergroup-product","title":"Exercise 2: create UserGroup product","text":"<p>Return to the Domain models section of the orchestrator core documentation and look at the code example Product Model a.k.a SubscriptionModel.</p> <p>Use the following skeleton to create the file <code>user_group.py</code> in the <code>products/product_types</code> folder and define the <code>UserGroupInactive</code>, <code>UserGroupProvisioning</code> and <code>UserGroup</code> domain models describing the user group product in its different lifecycle states:</p> <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n# UserGroupInactive\n...\n\n# UserGroupProvisioning\n...\n\n# UserGroup\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user group product </p>"},{"location":"workshops/beginner/domain-models/#exercise-3-create-user-product-block","title":"Exercise 3: create User product block","text":"<p>Use the following skeleton to create the file <code>user.py</code> in the <code>products/product_blocks</code> folder and define the <code>UserBlockInactive</code>, <code>UserBlockProvisioning</code> and <code>UserBlock</code> domain models describing the user group product block in its different lifecycle states:</p> <pre><code>from typing import Optional\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n# UserBlockInactive with only product block reference group mandatory\n...\n\n# UserBlockProvisioning with only resource type user_id and age optional\n...\n\n# UserBlock with only resource type age optional\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user product block </p>"},{"location":"workshops/beginner/domain-models/#exercise-4-create-user-product","title":"Exercise 4: create User product","text":"<p>Use the following skeleton to create the file <code>user.py</code> in the <code>products/product_types</code> folder and define the <code>UserInactive</code>, <code>UserProvisioning</code> and <code>User</code> domain models describing the user product in its different lifecycle states.</p> <p>Note that the <code>strEnum</code> type from the orchestrator is used, which uses the standard python module <code>enum</code> to define an enumeration of strings, to create a type to be used for the fixed input <code>affiliation</code>.</p> <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle, strEnum\n\nfrom products.product_blocks.user import (\n    UserBlock,\n    UserBlockInactive,\n    UserBlockProvisioning,\n)\n\n\nclass Affiliation(strEnum):\n    internal = \"internal\"\n    external = \"external\"\n\n\n# UserInactive(SubscriptionModel\n...\n\n# UserProvisioning\n...\n\n# User\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user product </p>"},{"location":"workshops/beginner/explore/","title":"Explore the GUI and API","text":""},{"location":"workshops/beginner/explore/#is-the-example-orchestrator-working","title":"Is the example orchestrator working?","text":"<p>If the orchestrator setup including the products that were created as part of this workshop do not work for some reason or the other, it is possible to quickly setup a working example orchestrator by  following the steps below (on Debian add <code>sudo</code> where needed):</p> <pre><code>git clone https://github.com/workfloworchestrator/example-orchestrator-beginner.git\ncd example-orchestrator-beginner\n# only drop database when you are really sure!\necho 'drop database \"orchestrator-core\";' | psql postgres\ncreatedb orchestrator-core -O nwa\nsource virtualenvwrapper.sh\nworkon example-orchestrator || mkvirtualenv --python python3.11 example-orchestrator\npip install orchestrator-core\nPYTHONPATH=. python main.py db init\ncp -av examples/*add_user_and_usergroup* migrations/versions/schema\nPYTHONPATH=. python main.py db upgrade heads\nENABLE_WEBSOCKETS=True uvicorn --host 127.0.0.1 --port 8080 wsgi:app\n</code></pre>"},{"location":"workshops/beginner/explore/#explore","title":"Explore","text":"<p>It is now time to explore the GUI and API. With the set of products created during this workshop users and groups can be created, modified and deleted. The easiest way is by using the GUI at:</p> <pre><code>http://localhost:3000/\n</code></pre> <p>But also check out the API at:</p> <pre><code>http://127.0.0.1:8080/api/docs\n</code></pre> <p>And look at the products:</p> <pre><code>curl http://127.0.0.1:8080/api/products/ | jq\n</code></pre> <p>Or get a list of subscriptions:</p> <pre><code>curl http://127.0.0.1:8080/api/subscriptions/all | jq\n</code></pre> <p>Or inspect the instantiated domain model for a subscription ID:</p> <pre><code>curl http://127.0.0.1:8080/api/subscriptions/domain-model/&lt;subscription_id&gt; | jq\n</code></pre> <p>And for the adventurers, create a subscription from the command line:</p> <pre><code>curl -X POST \\\n     -H \"Content-Type: application/json\" \\\n     http://127.0.0.1:8080/api/processes/create_user_group \\\n    --data '[\n  {\n    \"product\": \"a03eb19a-8a83-4964-85ea-98371f1d87f8\"\n  },\n  {\n    \"group_name\": \"Test Group\"\n  }\n]'\n</code></pre>"},{"location":"workshops/beginner/input-forms/","title":"Input forms","text":""},{"location":"workshops/beginner/input-forms/#introduction","title":"Introduction","text":"<p>The orchestrator GUI is a ReactJS application that runs in front of the orchestrator. It will consume the orchestrator API and enable the user to interact with the products, subscriptions and processes that are built and run in the orchestrator.</p> <p>The GUI uses Elastic-UI as framework for standard components and Uniforms to parse JSON-Schema produced by the forms endpoints in the core and render the correct components and widgets.</p>"},{"location":"workshops/beginner/input-forms/#input-form-generator","title":"Input form generator","text":"<p>An input form generator function is needed to define the fields and the type of the fields to be shown in the GUI to allow the user to input information needed to instantiate a subscription based on a product. A simple input form generator function looks as follows:</p> <pre><code>def initial_input_form_generator(product_name: str) -&gt; FormGenerator:\n    class CreateProductForm(FormPage):\n        class Config:\n            title = product_name\n\n        user_input: str\n\n    form_input = yield CreateProductForm\n\n    return form_input.dict()\n</code></pre> <p>All forms use <code>FormPage</code> as base and can be extended with the form input fields needed. In this case a string input field will be shown, the text entered will be assigned to <code>user_input</code>. All inputs from all input fields are then returned as a <code>Dict</code> and will be merged into the <code>State</code>. The <code>product_name</code> argument comes from the initial <code>State</code>.</p> <p>The optional <code>Config</code> class can be used to pass configuration information to Uniforms. In this case Uniforms is asked to show a input form page with the name of the product as title.</p> <p>The helper functions <code>wrap_create_initial_input_form</code>, for create workflows, and <code>wrap_modify_initial_input_form</code>, for modify and terminate workflows, are used to integrate the input form into the workflow and perform all needed <code>State</code> management. A common pattern used is:</p> <pre><code>@workflow(\n    \"Create product subscription\",\n    initial_input_form=wrap_create_initial_input_form(initial_input_form_generator),\n    target=Target.CREATE,\n)\ndef create_product_subscription():\n    return init &gt;&gt; create_subscription &gt;&gt; done\n</code></pre> <p>The <code>wrap_*</code> helper functions pre-populates the <code>State</code> with information needed by the initial input form. For the create workflow the <code>product</code> ID and <code>product_name</code> are added to the <code>State</code>:</p> <pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      workflow start       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\nproduct_name\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       input form(s)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\nproduct_name\nuser_input\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      create workflow      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>And for the modify and terminate workflows the <code>product</code> ID, <code>customer_id</code> ID and <code>subscription_id</code> are added to the <code>State</code></p> <pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      workflow start       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\ncustomer_id\nsubscription_id\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       input form(s)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\ncustomer_id\nsubscription_id\nuser_input\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 modify/terminate workflow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>"},{"location":"workshops/beginner/macos/","title":"MacOS version 12 (Monterey) installation instructions","text":"<p>How to manually install the orchestrator-core and orchestrator-core-gui on MacOS version 12 (Monterey) is described in the following steps.</p>"},{"location":"workshops/beginner/macos/#step-1-install-dependencies","title":"Step 1 - Install dependencies","text":"<p>This installation instruction assumes the use of Homebrew. The following software dependencies need to be installed:</p> <ul> <li>Python 3.11</li> <li>Postgres &gt;= 15</li> <li>virtualenvwrapper (or use any other tool to create virtual Python   environments)</li> <li>Node.js (version 14)</li> <li>yarn</li> </ul> <pre><code>brew install python@3.11 postgresql@15 virtualenvwrapper node@14 yarn\n</code></pre>"},{"location":"workshops/beginner/macos/#step-2-database-setup","title":"Step 2 - Database setup","text":"<p>Start the database server and create the database with the following commands, use <code>nwa</code> as password:</p> <pre><code>brew services start postgresql@15\ncreateuser -sP nwa\ncreatedb orchestrator-core -O nwa\n</code></pre> <p>For debug purposes, interact directly with the database by starting the PostgresSQL interactive terminal:</p> <pre><code>psql orchestrator-core\n</code></pre>"},{"location":"workshops/beginner/macos/#step-3-install-orchestrator","title":"Step 3 - Install orchestrator","text":"<p>The minimal version of Python is 3.11. Before the orchestrator core and all its dependencies are installed, a Python virtual environment is created:</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\nsource virtualenvwrapper.sh\nmkvirtualenv --python python3.11 example-orchestrator\n</code></pre> <p>Make sure that the just created Python virtual environment is active before installing the orchestrator-core:</p> <pre><code>pip install orchestrator-core\n</code></pre> <p>A next time in a new shell, be sure to activate the Python virtual environment again:</p> <pre><code>source virtualenvwrapper.sh\nworkon example-orchestrator\n</code></pre>"},{"location":"workshops/beginner/macos/#step-4-init-orchestrator","title":"Step 4 - Init orchestrator:","text":"<p>Create a <code>main.py</code> file with the following content:</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre> <p>Commit the just created main.py to git:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add main.py\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git must contain at least one commit, otherwise the <code>db init</code> below will fail.</p> <p>Initialize the database and run all the database migrations:</p> <pre><code>PYTHONPATH=. python main.py db init\nPYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/macos/#step-5-install-orchestrator-client","title":"Step 5 - Install orchestrator client","text":"<p>Install the orchestrator client in the parent directory of the example-orchestrator:</p> <pre><code>cd ..\ngit clone https://github.com/workfloworchestrator/example-orchestrator-ui.git\n</code></pre> <p>When multiple version of Node.js are installed, make sure node@14 is being used, this can be achieved by explicitly prepending it to the shell PATH.  Use the Yarn package manager to install the orchestrator client dependencies:</p> <pre><code>cd example-orchestrator-ui/\nexport PATH=\"/usr/local/opt/node@14/bin:$PATH\"\nyarn install\n</code></pre>"},{"location":"workshops/beginner/macos/#step-6-init-orchestrator-client","title":"Step 6 - Init orchestrator client:","text":"<p>Use the supplied environment variable defaults:</p> <pre><code>cp .env.local.example .env.local\n</code></pre> <p>And make the following changes to <code>.env.local</code>:</p> <pre><code># change the existing REACT_APP_BACKEND_URL variable value into:\nREACT_APP_BACKEND_URL=http://127.0.0.1:3000\n# and add the following:\nDANGEROUSLY_DISABLE_HOST_CHECK=true\n</code></pre> <p>The <code>custom-example</code> folder contains some SURF specific modules that can be used as an example. It must be linked to the folder <code>custom</code> in order for the app to start:</p> <pre><code>(cd src &amp;&amp; ln -s custom-example custom)\n</code></pre>"},{"location":"workshops/beginner/modify-user-group/","title":"Modify UserGroup workflow","text":""},{"location":"workshops/beginner/modify-user-group/#exercise-1-modify-usergroup-workflow","title":"Exercise 1: modify UserGroup workflow","text":"<p>The modify workflow can be used to change some or all of the resource types of an existing subscription. In this case the following workflow steps will be used:</p> <pre><code>init\n&gt;&gt; store_process_subscription()\n&gt;&gt; unsync\n&gt;&gt; modify_user_group_subscription\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>Besides the subscription administration that needs to be done, which probably already starts to look familiar, there is only one custom step that needs to be implemented. Most of the builtin steps were already discussed, but the <code>unsync</code> step is new. As can be guessed, this step has the opposite effect as the <code>resync</code> step, it sets an subscription out of sync for the duration of the modify which prohibits other workflows being started for this subscription.</p> <p>The <code>modify_user_group_subscription</code> step has three simple tasks. It will store the changed name of the group in the resource type <code>user_group</code> of the subscription. Secondly, it will change the subscription description to reflect the changed user group name. And last but not least, the user group name is also updated in the imaginary external user group provisioning system. Do not forget to return a <code>Dict</code> with a key <code>'subscription'</code> to merge the updated subscription into the workflow <code>State</code>, otherwise updates to the subscription will also not be saved to the database.</p> <p>The only thing remaining now is to create an initial input form generator that will show an input form with a string input field that shows the existing user group name, and allows for changes to be made. This is established by assigning the existing value to the input field used to enter the user group name. And as an extra, a second read-only field will be shown with the user group ID. For the latter the forms helper function <code>ReadOnlyField</code> can be used in the following way:</p> <pre><code>group_id: int = ReadOnlyField(subscription.user_group.group_id)\n</code></pre> <p>But where does the instantiated subscription come from in the initial input form generator? Remember that the workflow <code>State</code> available to the input form does not include the <code>subscription</code>, it only has the <code>subscription_id</code> at that stage. Luckily every product has a standard <code>from_subscription()</code> method that takes an subscription ID as argument that will fetch the subscription from the database and returns a fully instantiated domain model.  Remember to use the <code>wrap_modify_initial_input_form</code> wrapper for this modify workflow to make the subscription ID available to the input form.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/modify_user_group.py</code>:</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products.product_types.user_group import UserGroup\n\n# initial input form generator\n...\n\n# modify user group step\n...\n\n# modify user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group modfiy workflow </p>"},{"location":"workshops/beginner/modify-user/","title":"Modify User workflow","text":""},{"location":"workshops/beginner/modify-user/#exercise-1-modify-user-workflow","title":"Exercise 1: modify User workflow","text":"<p>The modify <code>User</code> workflow is also very similar to the modify <code>UserGroup</code> workflow, except for the different set of resource types that can be changed. This workflow uses the following steps:</p> <pre><code>init\n&gt;&gt; store_process_subscription()\n&gt;&gt; unsync\n&gt;&gt; modify_user_subscription\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>To show the current user group in the dropdown on the input form, the subscription ID of that user group is needed. But the <code>User</code> subscription only contains a reference to the <code>UserGroupBlock</code>, not the <code>UserGroup</code> subscription that is needed. Luckily, every instantiated product block has an attribute <code>owner_subscription_id</code> that contains the subscription ID of the subscription owning this product block instance.</p> <p>The <code>choice_list</code> input both returns a list as result and expects a list of values that it uses to display the currently selected item(s). The following will display a dropdown showing the currently selected user group:</p> <pre><code>user_group_id: user_group_selector() = [str(subscription.user.group.owner_subscription_id)]\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/modify_user.py</code>, and note that the <code>user_group_selector</code> from the create workflow is being reused:</p> <pre><code>from typing import List, Optional\n\nfrom orchestrator.forms import FormPage\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\nfrom workflows.user.create_user import user_group_selector\n\n# initial input form generator\n...\n\n# modify user step\n...\n\n# modify user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user modfiy workflow </p>"},{"location":"workshops/beginner/register-workflows/","title":"Register workflows","text":"<p>The orchestrator needs to know which workflows are available for which products. This is a two stage registration process. The workflows need to be registered as a workflow function in the code and a mapping between workflow and product_type needs to be added to the database through migration script. First we will add the workflow functions. For creating the migration script, we can either let the <code>cli</code> create an empty one and fill it manually or use the <code>db migrate-workflows</code> command to generate one based on the diffs between the registered workflows in the code and the database.</p>"},{"location":"workshops/beginner/register-workflows/#step-1-map-workflow-function-to-package","title":"Step 1: Map workflow function to package","text":"<p>Registering workflow functions in the code is done by creating appropriate <code>LazyWorkflowInstance</code> instances that maps a workflow function to the Python package where it is defined.</p> <p>For example, the <code>LazyWorkflowInstance</code> for the <code>UserGroup</code> create workflow looks like this:</p> <pre><code>from orchestrator.workflows import LazyWorkflowInstance\n\nLazyWorkflowInstance(\"workflows.user_group.create_user_group\", \"create_user_group\")\n</code></pre> <p>Add the <code>LazyWorkflowInstance</code> calls for all six workflows to <code>workflows/__init__. py</code>, and add <code>import workflows</code> to <code>main.py</code> so the instances are created as part of the workflow package.</p> <p>Example</p> <p>for inspiration look at an example implementation of the lazy workflow instances </p>"},{"location":"workshops/beginner/register-workflows/#step-2-register-workflow-in-database","title":"Step 2: Register workflow in database","text":"<p>There are several ways to complete this step:</p> <ul> <li>Copy the example workflows migration file from the example repository</li> <li>Use the <code>db migrate-workflows</code> generator script</li> <li>Create an empty migration file and edit it</li> </ul>"},{"location":"workshops/beginner/register-workflows/#copy-the-example-workflows-migration","title":"Copy the example workflows migration","text":"<pre><code>(\n  cd migrations/versions/schema\n  curl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/2022-11-12_8040c515d356_add_user_and_usergroup_workflows.py\n)\n</code></pre> <p>And restart the Docker compose environment.</p>"},{"location":"workshops/beginner/register-workflows/#migrate-workflows-generator-script","title":"Migrate workflows generator script","text":"<p>Similar to <code>db migrate-domain-models</code>, the orchestrator command line interface offers the <code>db migrate-workflows</code> command that walks you through a menu to create a database migration file based on the difference between the registered workflows in the code and the database.</p> <p>Start with the following command:</p> <pre><code>python main.py db migrate-workflows \"add User and UserGroup workflows\"\n</code></pre> <p>Navigate through the menu to add the six workflows to the corresponding <code>User</code> or <code>UserGroup</code> product type. After confirming a migration file will be added to <code>migrations/versions/schema</code> The migration can be run with:</p> <pre><code>python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/register-workflows/#manual","title":"Manual","text":"<p>Create a new empty database migration with the following command:</p> <pre><code>PYTHONPATH=. python main.py db revision --head data --message \"add User and UserGroup workflows\"\n</code></pre> <p>This will create an empty database migration in the folder <code>migrations/versions/schema</code>. For the migration we will make use of the migration helper functions <code>create_workflow</code> and <code>delete_workflow</code> that both expect a <code>Dict</code> that describes the workflow registration to be added or deleted from the database.</p> <p>To add all User and UserGroup workflows in bulk a list of <code>Dict</code> is created, for only the UserGroup create workflow the list looks like this:</p> <pre><code>from orchestrator.targets import Target\n\nnew_workflows = [\n    {\n        \"name\": \"create_user_group\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create user group\",\n        \"product_type\": \"UserGroup\",\n    },\n]\n</code></pre> <p>This registers the workflow function <code>create_user_group</code> as a create workflow for the <code>UserGroup</code> product.</p> <p>Add a list of <code>Dict</code>s describing the create, modify and terminate workflows for both the <code>UserGroup</code> and <code>User</code> products to the migration that was created above.</p> <p>The migration <code>upgrade</code> and <code>downgrade</code> functions will just loop through the list:</p> <pre><code>from orchestrator.migrations.helpers import create_workflow, delete_workflow\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n</code></pre> <p>Run the migration with the following command:</p> <pre><code>PYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/start-applications/","title":"Start orchestrator and client","text":""},{"location":"workshops/beginner/start-applications/#manual","title":"Manual","text":""},{"location":"workshops/beginner/start-applications/#start-orchestrator","title":"Start orchestrator","text":"<p>From the <code>example-orchestrator</code> folder, use Uvicorn to start the orchestrator:</p> <pre><code>uvicorn --host 127.0.0.1 --port 8080 wsgi:app\n</code></pre> <p>If you are running without authentication set up, you can set the environment variable to false from the command line: <pre><code>OAUTH2_ACTIVE=false uvicorn --host localhost --port 8080 wsgi:app\n</code></pre></p> <p>Visit the app to view the API documentation.</p>"},{"location":"workshops/beginner/start-applications/#start-client","title":"Start client","text":"<p>From the <code>example-orchestrator-ui</code> folder, run the following command to start the front end. <code>npm run dev</code></p>"},{"location":"workshops/beginner/start-applications/#docker-compose","title":"Docker compose","text":"<p>Using Docker compose the only thing needed to start all application is to run:</p> <pre><code>docker compose up\n</code></pre> <p>And point a browser to <code>http://localhost:3000/</code>.</p> <p>Note</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive, this workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/beginner/terminate-user-group/","title":"Terminate UserGroup workflow","text":""},{"location":"workshops/beginner/terminate-user-group/#exercise-1-terminate-usergroup-workflow","title":"Exercise 1: terminate UserGroup workflow","text":"<p>The terminate workflow is intended to end an subscription on a product for a customer, releasing all provisioned resources.  The terminate workflow for the <code>UserGroup</code> product uses the following steps:</p> <pre><code>init\n&gt;&gt; store_process_subscription()\n&gt;&gt; unsync\n&gt;&gt; deprovision_user_group\n&gt;&gt; set_status(SubscriptionLifecycle.TERMINATED)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>All builtin steps used here were already discussed and follow the same pattern, record the workflow process that was started for this subscription with <code>store_process_subscription</code>, and protect the steps that modify the subscription and/or OSS and BSS with <code>unsync</code> and <code>resync</code>. The only extra builtin step used in the terminate worfklow is <code>set_status</code> to set the subscription lifecycle state to<code>TERMINATED</code>.</p> <p>The only task for the custom step <code>deprovision_user_group</code> is to deprovision the user group from the imaginary group provisioning system.</p> <p>Also no new or changed input is needed for this workflow, instead the input form is used to ask the user if he/she really wants to terminate the subscription. This is done by using a <code>Label</code> to show a question on the input form:</p> <pre><code>from orchestrator.forms.validators import Label\n\n...\n    class TerminateForm(FormPage):\n        are_you_sure: Label = f\"Are you sure you want to remove {subscription.description}?\"\n...\n</code></pre> <p>Remember that you can use the standard product method <code>from_subscription()</code> to fetch the subscription from the database.</p> <p>Besides the question if the user really wants to terminate the subscription, only the Cancel and Submit button are shown on the input form. If the user clicks the Cancel button then the terminate workflow is not started, so nothing really happens. If the user clicks the Submit button then the terminate workflow is started and will execute all steps that in the end will result in a terminated subscription. Note that none of the workflow steps is using user input from the <code>State</code> because there was no user input given.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/terminate_user_group.py</code>:</p> <pre><code>from orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Label\nfrom orchestrator.targets import Target\nfrom orchestrator.types import InputForm, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products import UserGroup\n\n# initial input form generator\n...\n\n# deprovision user group step\n...\n\n# terminate user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group terminate workflow </p>"},{"location":"workshops/beginner/terminate-user/","title":"Terminate User workflow","text":""},{"location":"workshops/beginner/terminate-user/#exercise-1-terminate-user-workflow","title":"Exercise 1: terminate User workflow","text":"<p>Nothing more needs to be explained at this stage, the terminate user workflow is almost identical to the terminate user group workflow. As a reminder, and for completeness, the terminate workflow for the <code>User</code> product uses  following steps:</p> <pre><code>init\n&gt;&gt; store_process_subscription()\n&gt;&gt; unsync\n&gt;&gt; deprovision_user\n&gt;&gt; set_status(SubscriptionLifecycle.TERMINATED)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/terminate_user.py</code>:</p> <pre><code>from orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Label\nfrom orchestrator.targets import Target\nfrom orchestrator.types import InputForm, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products import User\n\n# initial input form generator\n...\n\n# deprovision user step\n...\n\n# terminate user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user terminate workflow </p>"},{"location":"workshops/beginner/workflow-introduction/","title":"Introduction","text":"<p>A workflow is the combination of an initial input form, used to acquire input from the user, and a list of workflow steps. For more details, see workflow architecture.</p> <p>The <code>workflow</code> decorator takes a description, initial input form, and a target as input and turns a function into a workflow that returns a step list to be executed by the workflow engine in a workflow process. A minimal workflow looks like this:</p> <pre><code>@workflow(\n    \"Create product subscription\",\n    initial_input_form=initial_input_form_generator,\n    target=Target.CREATE,\n)\ndef create_product_subscription():\n    return init &gt;&gt; create_subscription &gt;&gt; done\n</code></pre> <p>Information between workflow steps is passed using <code>State</code>, which is nothing more than a collection of key/value pairs, in Python represented by a <code>Dict</code>, with string keys and arbitrary values. Between steps the <code>State</code> is serialized to JSON and stored in the database. The <code>step</code> decorator is used to turn a function into a workflow step, all arguments to the step function will automatically be initialised with the value from the matching key in the <code>State</code>. In turn the step function will return a <code>Dict</code> of new and/or modified key/value pairs that will be merged into the <code>State</code> to be consumed by the next step. The serialization and deserialization between JSON and the indicated Python types is done automatically. A minimal workflow step looks as follows:</p> <pre><code>@step(\"Create subscription\")\ndef create_subscription(\n    product: UUIDstr,\n    user_input: str,\n) -&gt; State:\n    subscription = build_subscription(product, user_input)\n    return {\"subscription\": subscription}\n</code></pre> <p>The <code>product</code> and <code>user_input</code> arguments are filled from the corresponding key/value pairs in the <code>State</code>, and the new <code>subscription</code> key/value is added to the state to be used by one of the following steps.</p> <p>Every workflow starts with the builtin step <code>init</code> and ends with the builtin step <code>done</code>, with an arbitrary list of other builtin steps or custom steps in between.</p>"},{"location":"workshops/beginner/workshop-overview/","title":"Beginner Workshop Overview","text":""},{"location":"workshops/beginner/workshop-overview/#intended-audience","title":"Intended audience","text":"<p>This workshop is intended for everybody who is new to the workflow orchestrator and wants to learn how to install and run the applications, and create a first working set of products and associated workflows.</p> <p>Knowledge of the Python programming language and the Unix command line interface are prerequisites to do this workshop.</p>"},{"location":"workshops/beginner/workshop-overview/#topics","title":"Topics","text":"<ul> <li>Installation   Detailed instructions are given on how to prepare your environment and   install the orchestrator and GUI. Instructions for both Debian and MacOS are   included.</li> <li>Start applications   Shows a simple way of starting the orchestrator and GUI.</li> <li>Create User and User Group products   Through a simple user and group management scenario a set of products is   created showing how domain models are defined.<ul> <li>Domain models    Explains the benefits of the use of domain models and shows how the    hierarchy of products, product blocks, fixed inputs and resource    types is used to create product subscriptions for customers.</li> <li>Database migration   Use the orchestrator to create an Alembic database migration based on the   domain models that describe the created products and product blocks.</li> </ul> </li> <li>Create User and User Group workflows   For both the User And User Group products a set of create, modify and   terminate workflows will be created. The use of input forms is explained   as part of defining the create workflow. This will show how a simple   product block hierarchy is created.</li> </ul> <p>During this workshop a set of products will be created together with the needed workflows to administer users and user groups. The products will be just complex enough to show the basic capabilities of products, product blocks, fixed inputs, resource types and workflows in the workflow orchestrator.</p> <p>The following user and user group attributes will be stored:</p> <ul> <li>UserGroup<ul> <li>group_name: name of the user group</li> <li>group_id: ID of the group in an imaginary group management system</li> </ul> </li> <li>User<ul> <li>affiliation: the user's affiliation (internal or external)</li> <li>username: name of the user</li> <li>age: age of the user</li> <li>user_id: ID of the user in an imaginary user management system</li> <li>group: group the user belongs to</li> </ul> </li> </ul>"},{"location":"workshops/beginner/workshop-overview/#workshop-folder-layout","title":"Workshop folder layout","text":"<p>This workshop uses the following folder layout:</p> <pre><code>beginner-workshop\n\u2502\n\u251c\u2500\u2500 example-orchestrator\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 migrations\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 products\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 product_blocks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 user.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 user_group.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 product_types\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 user.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 user_group.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 user\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_user.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 modify_user.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 terminate_user.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 user_group\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 create_user_group.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 modify_user_group.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 terminate_user_group.py\n\u2502\n\u2514\u2500\u2500 orchestrator-core-gui\n \u00a0\u00a0 \u2514\u2500\u2500 ...\n</code></pre> <p>The <code>example-orchestrator-ui</code> folder will be cloned from GitHub. The <code>example-orchestrator</code> folder will be used for the orchestrator that is created during this workshop.  Although any layout of the latter folder will work, it is encouraged to use the suggested folder layout and filenames during this workshop.</p>"}]}